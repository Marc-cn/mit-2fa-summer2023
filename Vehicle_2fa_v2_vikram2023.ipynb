{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection & Confirmation System\n",
    "## Version 2\n",
    "Summer 2023 \n",
    "\n",
    "Created by Vikram Anantha \\\n",
    "Continued from Ben Dwyer's code \\\n",
    "July 27 2023\n",
    "\n",
    "## Summary\n",
    "\n",
    "(Same as V1) \\\n",
    "This code is meant to be implemented into Road-Side Systems (RSSs), like traffic cameras, such that it can communicate with vehicles, especially Autonomous Vehicles (AVs).\n",
    "\n",
    "### Background\n",
    "(Same as V1) \\\n",
    "One problem that might arise when AVs communicate with RSSs is that a hacker with malicious intent can join the same channel and communicate with the RSSs as if they were the vehicle. To combat this, the RSS can command the vehicle to confirm its identity by performing a specific task. Examples of this include:\n",
    " + Displaying a specific pattern on a screen, like a QR code\n",
    " + Flashing headlights in a specific pattern\n",
    " + Making a sound in a specific pattern\n",
    "\n",
    "Once the vehicle performs the task, the RSS can confirm this has been done visually, thus confirming the identiy of the vehicle. This visual confirmation, a form of Two Factor Authentication (2FA) is the premise of this code.\n",
    "\n",
    "\n",
    "\n",
    "### Changes from V1\n",
    "\n",
    "These were the shortcomings from Version 1:\n",
    " + When using live feed, there is about a second of delay per frame to detect if a vehicle is present, and to recognize there is no bounding box. It is much more time to recognize the array in the bounding box, which needs to be done for each vehicle.\n",
    " + The code runs a for loop to go through each vehicle and detect the pattern, meaning it verifies each vehicle one by one. Because each vehicle takes a long time to be verified, when multiple vehicles are present, the system will take a _very_ long time.\n",
    " + When using a webcam, or a dedicated camera for the computer (as in not a smartphone camera, which uses HDR to make screens appear normal with everything else), the Arduino display is super bright, and cannot be recognized.\n",
    " + The last point also doesn't really matter as much, since in the real environment, having a screen on the windshield won't be implemented. Instead, it would most likely leverage the headlights, having them flash a pattern across time, rather than display a pattern across space.\n",
    "\n",
    "Version 2 solves these problems in these ways:\n",
    " + (nothing yet)\n",
    "\n",
    "Todo (Jul 27 2023):\n",
    " + implement Zed camera based object detection\n",
    " + run a model to use headlights as the pattern recognition\n",
    "    + first get that to run on a given video\n",
    "    + then write arduino code to make the display act like headlights\n",
    "    + then get it to work in real time\n",
    "    + then see if the pattern recognition is needed with ML or just if statements\n",
    " + try to get multiple vehicles to work fast (idk how yet)\n",
    "    + zed api?\n",
    " + Priority 1: documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meeting with Daijang Suo [Jul 24 2023]\n",
    "\n",
    "\n",
    "ZED obj detection should be faster\n",
    "\n",
    "https://www.stereolabs.com/docs/object-detection/\n",
    "\n",
    "\n",
    "Detect vehicle + detect headlights + detect headlight pattern\n",
    "\n",
    "^^ simultaneously\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1906.03683.pdf\n",
    "\n",
    "Action-State Joint Learning-Based Vehicle Taillight\n",
    "\n",
    "Recognition in Diverse Actual Traffic Scenes\n",
    "\n",
    "\n",
    "\n",
    "Document all these steps rigorouly\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/TechToker/CarLightSignalsDetection\n",
    "\n",
    "^^ sliding window of frames?\n",
    "\n",
    "\n",
    "Api provided by zed camera to do categorization\n",
    "\n",
    "\n",
    "Look through https://www.stereolabs.com/docs/object-detection/custom-od/\n",
    "\n",
    "To get obj detection and custom detection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Todo\n",
    "\n",
    "Make the headlights work over time on the display\n",
    "\n",
    "Get multiple vehicle detection to work well\n",
    "\n",
    "And also get multiple vehicles to work fast (with zed api)\n",
    "\n",
    "Documentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "avg = lambda x: sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zed Camera Code: Object Detection Birds Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# Copyright (c) 2022, STEREOLABS.\n",
    "#\n",
    "# All rights reserved.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
    "# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
    "# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
    "# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
    "# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
    "# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
    "# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
    "# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
    "# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "    This sample demonstrates how to capture 3D point cloud and detected objects\n",
    "    with the ZED SDK and display the result in an OpenGL window.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "# import helpercode.ogl_viewer.viewer as gl\n",
    "import helpercode.cv_viewer.tracking_viewer as cv_viewer\n",
    "from helpercode.batch_system_handler import *\n",
    "\n",
    "\n",
    "##\n",
    "# Variable to enable/disable the batch option in Object Detection module\n",
    "# Batching system allows to reconstruct trajectories from the object detection module by adding Re-Identification / Appareance matching.\n",
    "# For example, if an object is not seen during some time, it can be re-ID to a previous ID if the matching score is high enough\n",
    "# Use with caution if image retention is activated (See batch_system_handler.py) :\n",
    "#   --> Images will only appear if an object is detected since the batching system is based on OD detection.\n",
    "USE_BATCHING = False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "if True:\n",
    "    print(\"Running object detection ... Press 'Esc' to quit\")\n",
    "    zed = sl.Camera()\n",
    "    \n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters()\n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP  \n",
    "    init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "    init_params.depth_maximum_distance = 20\n",
    "    is_playback = False                             # Defines if an SVO is used\n",
    "        \n",
    "    # If applicable, use the SVO given as parameter\n",
    "    # Otherwise use ZED live stream\n",
    "    if len(sys.argv) == 2:\n",
    "        filepath = sys.argv[1]\n",
    "        print(\"Using SVO file: {0}\".format(filepath))\n",
    "        init_params.svo_real_time_mode = True\n",
    "        init_params.set_from_svo_file(filepath)\n",
    "        is_playback = True\n",
    "\n",
    "    status = zed.open(init_params)\n",
    "    if status != sl.ERROR_CODE.SUCCESS:\n",
    "        print(repr(status))\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # Enable positional tracking module\n",
    "    positional_tracking_parameters = sl.PositionalTrackingParameters()\n",
    "    # If the camera is static in space, enabling this setting below provides better depth quality and faster computation\n",
    "    positional_tracking_parameters.set_as_static = True\n",
    "    zed.enable_positional_tracking(positional_tracking_parameters)\n",
    "\n",
    "    # Enable object detection module\n",
    "    batch_parameters = sl.BatchParameters()\n",
    "    if USE_BATCHING:\n",
    "        batch_parameters.enable = True\n",
    "        batch_parameters.latency = 2.0\n",
    "        batch_handler = BatchSystemHandler(batch_parameters.latency*2)\n",
    "    else:\n",
    "        batch_parameters.enable = False\n",
    "    obj_param = sl.ObjectDetectionParameters(batch_trajectories_parameters=batch_parameters)\n",
    "        \n",
    "    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.MULTI_CLASS_BOX_FAST\n",
    "    # Defines if the object detection will track objects across images flow.\n",
    "    obj_param.enable_tracking = True\n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    camera_infos = zed.get_camera_information()\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    point_cloud_res = sl.Resolution(min(camera_infos.camera_configuration.resolution.width, 720), min(camera_infos.camera_configuration.resolution.height, 404)) \n",
    "    point_cloud_render = sl.Mat()\n",
    "    # viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)\n",
    "    \n",
    "    # Configure object detection runtime parameters\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "    detection_confidence = 60\n",
    "    obj_runtime_param.detection_confidence_threshold = detection_confidence\n",
    "    # To select a set of specific object classes\n",
    "    obj_runtime_param.object_class_filter = [sl.OBJECT_CLASS.PERSON]\n",
    "    # To set a specific threshold\n",
    "    obj_runtime_param.object_class_detection_confidence_threshold = {sl.OBJECT_CLASS.PERSON: detection_confidence} \n",
    "\n",
    "    # Runtime parameters\n",
    "    runtime_params = sl.RuntimeParameters()\n",
    "    runtime_params.confidence_threshold = 50\n",
    "\n",
    "    # Create objects that will store SDK outputs\n",
    "    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "    objects = sl.Objects()\n",
    "    image_left = sl.Mat()\n",
    "\n",
    "    # Utilities for 2D display\n",
    "    display_resolution = sl.Resolution(min(camera_infos.camera_configuration.resolution.width, 1280), min(camera_infos.camera_configuration.resolution.height, 720))\n",
    "    image_scale = [display_resolution.width / camera_infos.camera_configuration.resolution.width\n",
    "                 , display_resolution.height / camera_infos.camera_configuration.resolution.height]\n",
    "    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239,255], np.uint8)\n",
    "\n",
    "    # Utilities for tracks view\n",
    "    camera_config = zed.get_camera_information().camera_configuration\n",
    "    tracks_resolution = sl.Resolution(400, display_resolution.height)\n",
    "    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance)\n",
    "    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)\n",
    "    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)\n",
    "\n",
    "    # Will store the 2D image and tracklet views \n",
    "    global_image = np.full((display_resolution.height, display_resolution.width+tracks_resolution.width, 4), [245, 239, 239,255], np.uint8)\n",
    "\n",
    "    # Camera pose\n",
    "    cam_w_pose = sl.Pose()\n",
    "    cam_c_pose = sl.Pose()\n",
    "\n",
    "    quit_app = False\n",
    "\n",
    "    # while(viewer.is_available() and (quit_app == False)):\n",
    "    while((quit_app == False)):\n",
    "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
    "            # Retrieve objects\n",
    "            returned_state = zed.retrieve_objects(objects, obj_runtime_param)\n",
    "            \n",
    "            if (returned_state == sl.ERROR_CODE.SUCCESS and objects.is_new):\n",
    "                # Retrieve point cloud\n",
    "                zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA,sl.MEM.CPU, point_cloud_res)\n",
    "                point_cloud.copy_to(point_cloud_render)\n",
    "                # Retrieve image\n",
    "                zed.retrieve_image(image_left, sl.VIEW.LEFT, sl.MEM.CPU, display_resolution)\n",
    "                image_render_left = image_left.get_data()\n",
    "                # Get camera pose\n",
    "                zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)\n",
    "\n",
    "                update_render_view = True\n",
    "                # update_3d_view = True\n",
    "                update_tracking_view = True\n",
    "\n",
    "                if USE_BATCHING:\n",
    "                    zed.get_position(cam_c_pose, sl.REFERENCE_FRAME.CAMERA)\n",
    "                    objects_batch = []\n",
    "                    zed.get_objects_batch(objects_batch)\n",
    "                    batch_handler.push(cam_c_pose,cam_w_pose,image_left,point_cloud,objects_batch)\n",
    "                    cam_c_pose, cam_w_pose, image_left, point_cloud_render, objects = batch_handler.pop(cam_c_pose,cam_w_pose,image_left,point_cloud,objects)\n",
    "                    \n",
    "                    image_render_left = image_left.get_data()\n",
    "                    \n",
    "                    update_tracking_view = objects.is_new\n",
    "\n",
    "                    if WITH_IMAGE_RETENTION:\n",
    "                        update_render_view = objects.is_new\n",
    "                        # update_3d_view = objects.is_new\n",
    "                    else:\n",
    "                        update_render_view = True\n",
    "                        # update_3d_view = True\n",
    "\n",
    "                # 3D rendering\n",
    "                # if update_3d_view:\n",
    "                    # viewer.updateData(point_cloud_render, objects)\n",
    "\n",
    "                # 2D rendering\n",
    "                if update_render_view:\n",
    "                    np.copyto(image_left_ocv,image_render_left)\n",
    "                    cv_viewer.render_2D(image_left_ocv,image_scale,objects, obj_param.enable_tracking)\n",
    "                    global_image = cv2.hconcat([image_left_ocv,image_track_ocv])\n",
    "\n",
    "                # Tracking view\n",
    "                if update_tracking_view:\n",
    "                    track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)\n",
    "                    \n",
    "            cv2.imshow(\"ZED | 2D View and Birds View\",global_image)\n",
    "            cv2.waitKey(10)\n",
    "\n",
    "        if (is_playback and (zed.get_svo_position() == zed.get_svo_number_of_frames()-1)):\n",
    "            print(\"End of SVO\")\n",
    "            quit_app = True\n",
    "\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    # viewer.exit()\n",
    "    image_left.free(sl.MEM.CPU)\n",
    "    point_cloud.free(sl.MEM.CPU)\n",
    "    point_cloud_render.free(sl.MEM.CPU)\n",
    "\n",
    "    if USE_BATCHING:\n",
    "        batch_handler.clear()\n",
    "\n",
    "    # Disable modules and close camera\n",
    "    zed.disable_object_detection()\n",
    "    zed.disable_positional_tracking()\n",
    "\n",
    "    zed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About RoboFlow and Yolo\n",
    "\n",
    "[Roboflow](https://roboflow.com/) is a platform which houses a vast variety of models, mainly for computer vision. It is often used because it is easy to train the models, and prediction happens over the cloud. For computer vision, Roboflow uses [YOLOv8](https://blog.roboflow.com/whats-new-in-yolov8/) (You Only Look Once), the fastest and most accurate architecture.\n",
    "\n",
    "Roboflow has a very intuitive process to upload training images and annotate them. For models that detect (rather than just classify), the model not only must be given the image, but also the coordinates on the image of where the object in question is, and what the object is called. Roboflow allows you to create the training data fairly easily, and can then be downloaded to be trained on device.\n",
    "\n",
    "In Version 1, the training data and the models was stored on RoboFlow, meaning anytime the code required a prediction from the model, it would make a call through the internet. This is ideal for most cases, (i.e. if the user is using a Windows laptop from 1997), however this code is meant to be run on an Edge computer, meaning it should have the capabilities to run the model on device. In this version, the model is trained and run on device.\n",
    "\n",
    "To do this, the dataset created in RoboFlow is downloaded onto the device, and a YoloV8 model is trained with this dataset.\n",
    "\n",
    "When the dataset is downloaded, it is stored in a folder (for example, `RC-Vehicle-Detection_v2-1`), which has a couple files\n",
    " - `test`\n",
    "    - `images`\n",
    "        - folder of the unedited images that were uploaded\n",
    "    - `labels`\n",
    "        - txt files with the coords of where the object is and its classification\n",
    " - `train`\n",
    "    - same subfolders as `test`\n",
    " - `valid`\n",
    "    - same subfolders as `test`\n",
    " - `data.yaml`\n",
    "    - the main datafile, has info about where the images are, what the clssifications are, etc. This file is the only file inputted when training the model \n",
    " - `README`s\n",
    "\n",
    "The model is then trained with this data, and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "\n",
    "rf = Roboflow(api_key=\"2Vs9PCO5LGDCSkI0huRq\")\n",
    "project = rf.workspace(\"meng-thesis-5fidi\").project(\"rc-vehicle-detection_v2\")\n",
    "dataset = project.version(1).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a custom detector\n",
    "\n",
    "# based on this tutorial: https://learnopencv.com/train-yolov8-on-custom-dataset/\n",
    "\n",
    "\n",
    " \n",
    "# Load the model.\n",
    "model = YOLO('yolov8m.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8m_rc_v2'\n",
    ")\n",
    "\n",
    "# For nano model, epochs 100, imgsize 640, it took 769 mins to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8s.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8s_rc_v3'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')\n",
    "# metrics = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.box.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(glob.glob('RC-Vehicle-Detection_v2-1/test/images/*.jpg'),\n",
    "                save=True,\n",
    "                save_txt=True,\n",
    "                save_conf=True,\n",
    "                save_crop=True,\n",
    "                )\n",
    "# Nano: Takes 3.6 seconds to predict on 35 images, so about 0.1s per image.\n",
    "# Medium: 56s to predict 35 images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for the Model\n",
    "realdata_path = 'RC-Vehicle-Detection_v2-1/test/labels/*.txt'\n",
    "fakedata_path = 'runs/detect/predict2/labels/*.txt'\n",
    "\n",
    "realdata_files = glob.glob(realdata_path)\n",
    "fakedata_files = glob.glob(fakedata_path)\n",
    "if (len(realdata_files) != len(fakedata_files)):\n",
    "    raise \"WrongFilePathException\"\n",
    "confs = []\n",
    "bounds_diffs = []\n",
    "for i in range(len(realdata_files)):\n",
    "    with open(realdata_files[i], 'r') as rfile:\n",
    "        robj, rxl, rxr, ryl, ryr = map(float, rfile.readline().split(\" \"))\n",
    "    with open(fakedata_files[i], 'r') as ffile:\n",
    "        fobj, fxl, fxr, fyl, fyr, fconf = map(float, ffile.readline().split(\" \"))\n",
    "        \n",
    "    confs.append(fconf)\n",
    "    bounds_diffs.append(abs(rxl-fxl)*100)\n",
    "    bounds_diffs.append(abs(rxr-fxr)*100)\n",
    "    bounds_diffs.append(abs(ryl-fyl)*100)\n",
    "    bounds_diffs.append(abs(ryr-fyr)*100)\n",
    "    \n",
    "print(\"Average Confidence:             \", avg(confs))\n",
    "print(\"Average Bounding Box Difference:\", avg(bounds_diffs), \"% of the image\")\n",
    "\n",
    "# Average Confidence:              0.8520260857142857\n",
    "# Average Bounding Box Difference: 0.6273896666666672% of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Videos/drive_05/drive_05_vikram2023.mp4\",\n",
    "      save=True,\n",
    "      save_txt=True,\n",
    "      save_conf=True,\n",
    "      save_crop=True\n",
    "      )\n",
    "\n",
    "# 15.5s video, with save,save_txt,save_conf,save_crop:\n",
    "# 160 seconds to predict\n",
    "# 1/3 seconds to predict per frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_livefeed_rfdevice():\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    foldername = 'runs/detect/predict10/'\n",
    "    filename = foldername + 'image0.jpg'\n",
    "    latencies = []\n",
    "    count = 0\n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        startbig = time.time()\n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        model(frame, save=True, verbose=False)\n",
    "        # opencv_boxed_image = np.array(opencv_boxed_image)[:, :, ::-1].copy() # turns the PIL image to a CV2-ready image\n",
    "        \n",
    "        # display frame\n",
    "        boxedframe = cv2.imread(filename)\n",
    "        endbig = time.time()\n",
    "        cv2.imshow('frame', boxedframe)\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "        cv2.imwrite(foldername + 'video_frames/frame_%s.jpg' % current_time, frame)\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    \n",
    "cv2_livefeed_rfdevice()\n",
    "# ~0.52 seconds per loop\n",
    "# ~0.42 seconds of latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_video(input_val, output_val, fps=25):\n",
    "    # stich images together into video\n",
    "    # os.system(f\"ffmpeg -r {fps} -s 3840x2160 -pattern_type glob -i {input_val} -vcodec libx264 -crf {fps}  -pix_fmt yuv420p {output_val}\")\n",
    "    # !ffmpeg -r 25 -s 1920x1080 -i '/Users/bendwyer/Documents/Academic/2022-2023/Thesis/Vehicle Detection/%04d.png' -vcodec libx264 -crf 25  -pix_fmt yuv420p '/Users/bendwyer/Documents/Academic/2022-2023/Thesis/Vehicle Detection/test.mp4'\n",
    "    os.system(f\"ffmpeg -y -framerate {fps} -pattern_type glob -i '{input_val}' -c:v libx264 -r 30 -pix_fmt yuv420p {output_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_video(\n",
    "    input_val='runs/detect/predict5/video_frames/frame_*.jpg',\n",
    "    fps=4,\n",
    "    output_val='runs/detect/predict5/in_the_sun.mp4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from threading import Lock, Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "avg = lambda x: sum(x)/len(x)\n",
    "\n",
    "# import helpercode.custom.ogl_viewer.viewer as gl\n",
    "import helpercode.cv_viewer.tracking_viewer as cv_viewer\n",
    "# from helpercode.batch_system_handler import *\n",
    "\n",
    "lock = Lock()\n",
    "run_signal = False\n",
    "exit_signal = False\n",
    "\n",
    "\n",
    "def xywh2abcd(xywh, im_shape):\n",
    "    output = np.zeros((4, 2))\n",
    "\n",
    "    # Center / Width / Height -> BBox corners coordinates\n",
    "    x_min = (xywh[0] - 0.5*xywh[2]) #* im_shape[1]\n",
    "    x_max = (xywh[0] + 0.5*xywh[2]) #* im_shape[1]\n",
    "    y_min = (xywh[1] - 0.5*xywh[3]) #* im_shape[0]\n",
    "    y_max = (xywh[1] + 0.5*xywh[3]) #* im_shape[0]\n",
    "\n",
    "    # A ------ B\n",
    "    # | Object |\n",
    "    # D ------ C\n",
    "\n",
    "    output[0][0] = x_min\n",
    "    output[0][1] = y_min\n",
    "\n",
    "    output[1][0] = x_max\n",
    "    output[1][1] = y_min\n",
    "\n",
    "    output[2][0] = x_min\n",
    "    output[2][1] = y_max\n",
    "\n",
    "    output[3][0] = x_max\n",
    "    output[3][1] = y_max\n",
    "    return output\n",
    "\n",
    "def detections_to_custom_box(detections, im0):\n",
    "    output = []\n",
    "    for i, det in enumerate(detections):\n",
    "        xywh = det.xywh[0]\n",
    "\n",
    "        # Creating ingestable objects for the ZED SDK\n",
    "        obj = sl.CustomBoxObjectData()\n",
    "        obj.bounding_box_2d = xywh2abcd(xywh, im0.shape)\n",
    "        obj.label = det.cls\n",
    "        obj.probability = det.conf\n",
    "        obj.is_grounded = False\n",
    "        output.append(obj)\n",
    "    return output\n",
    "\n",
    "\n",
    "def torch_thread(weights, img_size, conf_thres=0.2, iou_thres=0.45):\n",
    "    global image_net, exit_signal, run_signal, detections\n",
    "\n",
    "    print(\"Intializing Network...\")\n",
    "\n",
    "    model = YOLO(weights)\n",
    "\n",
    "    while not exit_signal:\n",
    "        if run_signal:\n",
    "            lock.acquire()\n",
    "\n",
    "            img = cv2.cvtColor(image_net, cv2.COLOR_BGRA2RGB)\n",
    "            # https://docs.ultralytics.com/modes/predict/#video-suffixes\n",
    "            det = model.predict(img, save=False, imgsz=img_size, conf=conf_thres, iou=iou_thres)[0].cpu().numpy().boxes\n",
    "\n",
    "            # ZED CustomBox format (with inverse letterboxing tf applied)\n",
    "            detections = detections_to_custom_box(det, image_net)\n",
    "            lock.release()\n",
    "            run_signal = False\n",
    "        sleep(0.01)\n",
    "\n",
    "\n",
    "def main():\n",
    "    global image_net, exit_signal, run_signal, detections\n",
    "    weights = \"runs/detect/yolov8n_rc_v13/weights/best.pt\"\n",
    "    img_size = 416\n",
    "    conf_thres=0.4\n",
    "    svo = None\n",
    "    capture_thread = Thread(target=torch_thread, kwargs={\n",
    "        'weights': weights, \n",
    "        'img_size': img_size, \n",
    "        \"conf_thres\": conf_thres\n",
    "    })\n",
    "    capture_thread.start()\n",
    "    \n",
    "    latencies = []\n",
    "\n",
    "    print(\"Initializing Camera...\")\n",
    "\n",
    "    zed = sl.Camera()\n",
    "\n",
    "    input_type = sl.InputType()\n",
    "    if svo is not None:\n",
    "        input_type.set_from_svo_file(svo)\n",
    "\n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters(input_t=input_type, svo_real_time_mode=True)\n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # QUALITY\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP\n",
    "    init_params.depth_maximum_distance = 50\n",
    "\n",
    "    runtime_params = sl.RuntimeParameters()\n",
    "    status = zed.open(init_params)\n",
    "\n",
    "    if status != sl.ERROR_CODE.SUCCESS:\n",
    "        print(repr(status))\n",
    "        exit()\n",
    "\n",
    "    image_left_tmp = sl.Mat()\n",
    "\n",
    "    print(\"Initialized Camera\")\n",
    "\n",
    "    positional_tracking_parameters = sl.PositionalTrackingParameters()\n",
    "    # If the camera is static, uncomment the following line to have better performances and boxes sticked to the ground.\n",
    "    # positional_tracking_parameters.set_as_static = True\n",
    "    zed.enable_positional_tracking(positional_tracking_parameters)\n",
    "\n",
    "    obj_param = sl.ObjectDetectionParameters()\n",
    "    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.CUSTOM_BOX_OBJECTS\n",
    "    obj_param.enable_tracking = True\n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    objects = sl.Objects()\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "\n",
    "    # Display\n",
    "    camera_infos = zed.get_camera_information()\n",
    "    camera_res = camera_infos.camera_configuration.resolution\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    point_cloud_res = sl.Resolution(min(camera_res.width, 720), min(camera_res.height, 404))\n",
    "    point_cloud_render = sl.Mat()\n",
    "    # viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)\n",
    "    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "    image_left = sl.Mat()\n",
    "    # Utilities for 2D display\n",
    "    display_resolution = sl.Resolution(min(camera_res.width, 1280), min(camera_res.height, 720))\n",
    "    image_scale = [display_resolution.width / camera_res.width, display_resolution.height / camera_res.height]\n",
    "    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239, 255], np.uint8)\n",
    "\n",
    "    # Utilities for tracks view\n",
    "    camera_config = camera_infos.camera_configuration\n",
    "    tracks_resolution = sl.Resolution(400, display_resolution.height)\n",
    "    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance)\n",
    "    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)\n",
    "    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)\n",
    "    # Camera pose\n",
    "    cam_w_pose = sl.Pose()\n",
    "\n",
    "    # while viewer.is_available() and not exit_signal:\n",
    "    while not exit_signal:\n",
    "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
    "            # -- Get the image\n",
    "            \n",
    "            lock.acquire()\n",
    "            zed.retrieve_image(image_left_tmp, sl.VIEW.LEFT)\n",
    "            \n",
    "            image_net = image_left_tmp.get_data()\n",
    "            lock.release()\n",
    "            run_signal = True\n",
    "            startbig = time.time()\n",
    "\n",
    "            # -- Detection running on the other thread\n",
    "            while run_signal:\n",
    "                sleep(0.001)\n",
    "\n",
    "            # Wait for detections\n",
    "            lock.acquire()\n",
    "            # -- Ingest detections\n",
    "            zed.ingest_custom_box_objects(detections)\n",
    "            lock.release()\n",
    "            zed.retrieve_objects(objects, obj_runtime_param)\n",
    "\n",
    "            # -- Display\n",
    "            # Retrieve display data\n",
    "            zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA, sl.MEM.CPU, point_cloud_res)\n",
    "            point_cloud.copy_to(point_cloud_render)\n",
    "            zed.retrieve_image(image_left, sl.VIEW.LEFT, sl.MEM.CPU, display_resolution)\n",
    "            zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)\n",
    "\n",
    "            # 3D rendering\n",
    "            # viewer.updateData(point_cloud_render, objects)\n",
    "            # 2D rendering\n",
    "            np.copyto(image_left_ocv, image_left.get_data())\n",
    "            cv_viewer.render_2D(image_left_ocv, image_scale, objects, obj_param.enable_tracking)\n",
    "            global_image = cv2.hconcat([image_left_ocv, image_track_ocv])\n",
    "            # Tracking view\n",
    "            track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)\n",
    "            endbig = time.time()\n",
    "            cv2.imshow(\"ZED | 2D View and Birds View\", global_image)\n",
    "            key = cv2.waitKey(10)\n",
    "            if key == 27:\n",
    "                exit_signal = True\n",
    "                \n",
    "            \n",
    "            print(round(endbig-startbig, 2), \"seconds\")\n",
    "            latencies.append(endbig-startbig)\n",
    "        else:\n",
    "            exit_signal = True\n",
    "\n",
    "    # viewer.exit()\n",
    "    exit_signal = True\n",
    "    zed.close()\n",
    "    \n",
    "    print(round(avg(latencies), 3), \"seconds per loop\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    main()\n",
    "    \n",
    "# 0.605 seconds per loop\n",
    "# 0.605 seconds of latency\n",
    "\n",
    "\n",
    "# in hallway, max distance = 4.3 meters\n",
    "# 0.68 seconds per loop\n",
    "\n",
    "# in outside, 5.6 max partly cloudy\n",
    "# 80% outside partly cloudy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report after Jul 31 2023\n",
    "\n",
    "I created the YOLOv8 model and have it predicting on device- the performance is pretty good. When I run it on a simple image, it takes about 0.1 seconds to predict. When I run it on a video, it takes about 0.3 seconds per frame. When I run it on live feed with CV2, it takes about 0.43 seconds of processing. However, when I run the custom detector on the ZED Camera code (which processes both cameras, and tracks the object), it takes about 0.65 seconds to process. Is this latency a good amount, or is it too much?\n",
    "\n",
    "I also tried it outside. When the car is coming towards the camera, it has about an 80% accuracy, but can identify the car pretty well, in any scenario. The car can go about 5.6 meters away before the camera stops detecting it. The algorithm was pretty good about recognizing the car in the sunlight, however when I tested it today it was about 5pm and partly cloudy so it might not be the best example- I'll check again tomorrow in more direct sunlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report After Aug 2 2023\n",
    "\n",
    "I got a video yesterday of how the object detection performs in sunny weather when the model is on device using YoloV8 nano model, and it isn't the worst, however it isn't the best. I found that it works best  when the lighting is consistent (everything in the frame is either sunny or shady); when there is a variety, (half shady half sunny) the camera can't decide whether to make the sunny parts too sunny or the shady parts too shady, so it does some combination of both, which doesn't make the frame that good (since these kinds of webcams don't have High Dynamic Range which most of our phone cameras do). [The video (with the confidence levels) is here](https://photos.app.goo.gl/2ZW4CrC4H61wA8sZ6)\n",
    "\n",
    "I think that this accuracy can be fixed simply by making the training data more diverse (since it was largely  just the car in a hallway, a dorm room, and outside but not that sunny), and making sure that the camera taking the photos is also diverse, or is at least the zed camera\n",
    "\n",
    "Something that I thought about while I was on my run this morning is that right now, the code runs like this:\n",
    "while true:\n",
    "   read frame from zed cam\n",
    "   do image recognition on the frame [~0.5 seconds to do]\n",
    "   display the frame\n",
    "\n",
    "This means that the frames are being read about once every half second. The way I had envisioned the blinking would be that within a span of 2 seconds, it would have blinked a considerable number of times in different durations to be able to convey a unique code. My concern with this is that since the camera is reading in each frame infrequently, it wouldn't register most of the blinks.\n",
    "\n",
    "I thought of a couple of fixes for this:\n",
    " - We could have each blink last at least half a second, to ensure the edge computer can read each time the display turns off and on. The only issue with this is that the message might take several seconds to convey, which a car wouldn't have if it is moving on the road\n",
    " - We could also try parallel processing, so that the edge computer would read each frame in separately from the processing, meaning the camera will capture every time the car headlights blink. The problem with this is that the computer has to process each of those frames, which would take about 0.5s per frame, so by the time the identity is confirmed the car is long gone\n",
    " - This could also be a problem with the model itself. I noticed that the default model Roboflow comes with has very little latency, so it isn't impossible to reach very fast prediction times, however I assume that they achieved this with a large, diverse, body of training data, and possibly fine tuning the hyperparameters of the model. It is possible to get the training data, but would take a lot of manual work. \n",
    "\n",
    "I have been trying to train the models with different parameters, but I've only been able to train it over night. I'll do a performance review of each of the models to see which one is the fastest and most accurate.\n",
    "\n",
    "I was also thinking, since I now can edit the code on Ben's display, we could try to display a QR code which links to a url hosted on a server (that could submit a POST request), which the edge computer is connected to. I figured this might be good since there should be some optimized models out there to scan QR codes, so it should help with latency, however this still requires the board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Detecting the Headlights\n",
    "In [this paper from Innopolis University in Russia](https://github.com/TechToker/Vehicle-rear-lights-analyser/blob/master/FinalReport.pdf), researchers intended detect if the car in front of them were braking or not. \n",
    "\n",
    "The researchers use YOLOv4 to detect whether a car is present for each frame, and then cropp the frame to just the car.\n",
    "\n",
    "To detect the presense of brakelights, they convert each image into YCrCb space, then removes all pixels that aren't pure red. They utilize the fact that the taillights are symmetrical in cars to weed away noise from other cars. They then apply a bounding box over the taillights.\n",
    "\n",
    "To determine if the car is braking, it determines the brightness in the bounding boxes. It's important to note that it has to see what the car looks like when braking vs not braking to know which is brighter, and thus be able to distinguish when it's braking.\n",
    "\n",
    "Our algorithm can utilize soemthing similar, but will have to identify the headlights a different way since headlights aren't red. Potentially, a ML model could be used to determine the headlights' position. Determining when its on vs off would be the same as the Innopolis paper.\n",
    "\n",
    "\n",
    "\n",
    "[This article](https://ieeexplore.ieee.org/abstract/document/6761567) talks about how the researchers detected headlights to track vehicles. They first converted the image to a binary image (meaning grayscaled, then converted everything above a certain threshold to 255 and below that threshold to 0), and detected the blobs. The then detected the blobs of white areas, and considered each blob a candidate for one of the pair of headlights. Finally, they used the property that headlights are symmetrical to pair 2 of the blobs and determine if they are symmetrical (using some equations).\n",
    "From this, they applied it to counting and tracking vehicles in night time / low light. It remains to be seen whether this technique would work in the daytime (since converting the image to binary might not work as well / produce more blobs in higher light), but a method like this could work on the prototype car (since it has the red display board)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# Downloading a headlights detection dataset\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"2Vs9PCO5LGDCSkI0huRq\")\n",
    "project = rf.workspace(\"nours-g8unb\").project(\"vehicles_night\")\n",
    "dataset = project.version(3).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a custom detector\n",
    "\n",
    "# based on this tutorial: https://learnopencv.com/train-yolov8-on-custom-dataset/\n",
    "\n",
    "\n",
    " \n",
    "# Load the model.\n",
    "model = YOLO('yolov8n.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8n_headlights_v1'\n",
    ")\n",
    "\n",
    "# Took 1258 mins for imgsz=1280, epochs 100, nano model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_headlights_v1/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(glob.glob('Videos/headlight_test/*.jpg'),\n",
    "                save=True,\n",
    "                # save_txt=True,\n",
    "                save_conf=True,\n",
    "                # save_crop=True,\n",
    "                )\n",
    "# Takes 5.9 seconds to predict on 6 images, so about 1s per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aug 1 2023\n",
    "\n",
    "Steps that need to be done now.\n",
    "\n",
    "I need to find a good way to detect headlights.\n",
    "There is a headlights ML algorithm that I can train it on\n",
    "That needs to be done over night\n",
    "\n",
    "I also would need to make the headlights work on Ben's display\n",
    "I need Ben's arduino code to make that happen\n",
    "\n",
    "I also need to find a way to decode the headlights\n",
    "I need the display to make that work?\n",
    "\n",
    "Overnight I should train the algorithms\n",
    "Then tmr I should test to see if it works in the day\n",
    "Then find the intensity values\n",
    "\n",
    "Also take a picture of actual car headluights, play around with those in different formats (Binary, YCrCb, etc), also needs to be done at home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aug 7 2023\n",
    "\n",
    "To Detect headlights, one approach is to detect the vehicle like normal, and simply detect when the vehicle's lights flash by detecting when the vehicle region becomes bright. This way, you don't have to recognize where a vehicle's headlights are, you can instead act like a human and only notice when the lights flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(['Videos/PXL_20230807_143124752.MP.jpg', 'Videos/PXL_20230807_143126993.MP.jpg'],\n",
    "                save=True,\n",
    "                # save_txt=True,\n",
    "                save_conf=True,\n",
    "                save_crop=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/462, mean: 219.6951171875\n",
      " 2/462, mean: 204.07083333333333\n",
      " 3/462, mean: 180.45677083333334\n",
      " 4/462, mean: 178.7586154513889\n",
      " 5/462, mean: 161.6962673611111\n",
      " 6/462, mean: 149.2085720486111\n",
      " 7/462, mean: 125.78610387731482\n",
      " 8/462, mean: 79.22142650462963\n",
      " 9/462, mean: 114.9827907986111\n",
      " 10/462, mean: 131.90654658564816\n",
      " 11/462, mean: 117.90983072916667\n",
      " 12/462, mean: 130.315625\n",
      " 13/462, mean: 122.4687427662037\n",
      " 14/462, mean: 115.05559172453704\n",
      " 15/462, mean: 113.75673466435185\n",
      " 16/462, mean: 113.71258680555556\n",
      " 17/462, mean: 117.84908130787036\n",
      " 18/462, mean: 118.99704861111111\n",
      " 19/462, mean: 120.36489438657408\n",
      " 20/462, mean: 120.2925853587963\n",
      " 21/462, mean: 116.90353009259259\n",
      " 22/462, mean: 115.37643229166666\n",
      " 23/462, mean: 125.9232204861111\n",
      " 24/462, mean: 120.98640769675926\n",
      " 25/462, mean: 119.6740234375\n",
      " 26/462, mean: 113.93394097222222\n",
      " 27/462, mean: 113.22216435185184\n",
      " 28/462, mean: 110.79341724537036\n",
      " 29/462, mean: 106.70672743055556\n",
      " 30/462, mean: 108.84562355324074\n",
      " 31/462, mean: 115.48391203703704\n",
      " 32/462, mean: 115.6882957175926\n",
      " 33/462, mean: 115.4459129050926\n",
      " 34/462, mean: 115.58278356481482\n",
      " 35/462, mean: 115.59471209490741\n",
      " 36/462, mean: 115.66726707175926\n",
      " 37/462, mean: 117.904296875\n",
      " 38/462, mean: 116.66188512731482\n",
      " 39/462, mean: 118.30211950231481\n",
      " 40/462, mean: 119.87545572916666\n",
      " 41/462, mean: 121.14442997685185\n",
      " 42/462, mean: 119.10366753472222\n",
      " 43/462, mean: 116.2611111111111\n",
      " 44/462, mean: 114.76463396990741\n",
      " 45/462, mean: 114.71126302083333\n",
      " 46/462, mean: 114.5165943287037\n",
      " 47/462, mean: 112.90255353009259\n",
      " 48/462, mean: 112.18462818287037\n",
      " 49/462, mean: 118.60943287037037\n",
      " 50/462, mean: 125.52970196759259\n",
      " 51/462, mean: 122.30653935185185\n",
      " 52/462, mean: 112.72866753472222\n",
      " 53/462, mean: 116.00802951388889\n",
      " 54/462, mean: 120.8560546875\n",
      " 55/462, mean: 125.53585069444445\n",
      " 56/462, mean: 126.82991174768519\n",
      " 57/462, mean: 126.81859809027777\n",
      " 58/462, mean: 126.84073350694445\n",
      " 59/462, mean: 126.85032552083334\n",
      " 60/462, mean: 127.84088541666667\n",
      " 61/462, mean: 127.31344762731482\n",
      " 62/462, mean: 126.88734085648149\n",
      " 63/462, mean: 125.69534866898148\n",
      " 64/462, mean: 126.95067274305555\n",
      " 65/462, mean: 127.40678530092593\n",
      " 66/462, mean: 128.3974681712963\n",
      " 67/462, mean: 128.1431568287037\n",
      " 68/462, mean: 128.4878544560185\n",
      " 69/462, mean: 128.45305266203704\n",
      " 70/462, mean: 126.70128038194444\n",
      " 71/462, mean: 127.0517578125\n",
      " 72/462, mean: 126.5736328125\n",
      " 73/462, mean: 126.70656828703704\n",
      " 74/462, mean: 125.99921875\n",
      " 75/462, mean: 123.17744502314815\n",
      " 76/462, mean: 118.99704861111111\n",
      " 77/462, mean: 111.03825954861111\n",
      " 78/462, mean: 96.03093894675926\n",
      " 79/462, mean: 129.04603587962964\n",
      " 80/462, mean: 129.52219328703703\n",
      " 81/462, mean: 133.96341869212964\n",
      " 82/462, mean: 129.57433449074074\n",
      " 83/462, mean: 128.6939597800926\n",
      " 84/462, mean: 125.7900390625\n",
      " 85/462, mean: 126.40208333333334\n",
      " 86/462, mean: 127.75640190972223\n",
      " 87/462, mean: 131.2070674189815\n",
      " 88/462, mean: 129.5552734375\n",
      " 89/462, mean: 122.8658275462963\n",
      " 90/462, mean: 119.13478009259259\n",
      " 91/462, mean: 123.29193431712963\n",
      " 92/462, mean: 115.18569878472222\n",
      " 93/462, mean: 118.11630497685185\n",
      " 94/462, mean: 107.90708188657408\n",
      " 95/462, mean: 113.32743055555555\n",
      " 96/462, mean: 110.13304398148148\n",
      " 97/462, mean: 111.5548755787037\n",
      " 98/462, mean: 104.2568287037037\n",
      " 99/462, mean: 102.0839916087963\n",
      " 100/462, mean: 102.98367332175926\n",
      " 101/462, mean: 103.40891927083334\n",
      " 102/462, mean: 103.05536747685186\n",
      " 103/462, mean: 97.52962962962962\n",
      " 104/462, mean: 98.0398509837963\n",
      " 105/462, mean: 97.15935329861111\n",
      " 106/462, mean: 97.83620515046296\n",
      " 107/462, mean: 97.85268373842592\n",
      " 108/462, mean: 97.75823929398148\n",
      " 109/462, mean: 97.8585865162037\n",
      " 110/462, mean: 97.2349826388889\n",
      " 111/462, mean: 97.16869936342593\n",
      " 112/462, mean: 97.21665943287037\n",
      " 113/462, mean: 97.40444155092592\n",
      " 114/462, mean: 100.11652199074074\n",
      " 115/462, mean: 101.40185185185184\n",
      " 116/462, mean: 100.30873119212963\n",
      " 117/462, mean: 102.27718460648148\n",
      " 118/462, mean: 102.69369936342592\n",
      " 119/462, mean: 106.2818504050926\n",
      " 120/462, mean: 108.27391493055555\n",
      " 121/462, mean: 107.98357204861111\n",
      " 122/462, mean: 107.95934606481481\n",
      " 123/462, mean: 102.26205873842592\n",
      " 124/462, mean: 99.24881365740741\n",
      " 125/462, mean: 95.48333333333333\n",
      " 126/462, mean: 104.59095052083333\n",
      " 127/462, mean: 102.94284577546296\n",
      " 128/462, mean: 102.81999421296297\n",
      " 129/462, mean: 103.21314380787037\n",
      " 130/462, mean: 111.91260850694445\n",
      " 131/462, mean: 102.84422019675925\n",
      " 132/462, mean: 104.3052806712963\n",
      " 133/462, mean: 103.7843605324074\n",
      " 134/462, mean: 101.98401331018519\n",
      " 135/462, mean: 101.6599754050926\n",
      " 136/462, mean: 101.89338107638889\n",
      " 137/462, mean: 102.18101851851851\n",
      " 138/462, mean: 102.26144386574074\n",
      " 139/462, mean: 103.31324508101852\n",
      " 140/462, mean: 103.04675925925926\n",
      " 141/462, mean: 102.76650028935185\n",
      " 142/462, mean: 102.6165943287037\n",
      " 143/462, mean: 102.02176649305555\n",
      " 144/462, mean: 101.2705150462963\n",
      " 145/462, mean: 99.49144241898148\n",
      " 146/462, mean: 101.79327980324074\n",
      " 147/462, mean: 102.47320601851852\n",
      " 148/462, mean: 102.47738715277778\n",
      " 149/462, mean: 102.28763744212964\n",
      " 150/462, mean: 102.2669777199074\n",
      " 151/462, mean: 102.20475260416667\n",
      " 152/462, mean: 99.5904369212963\n",
      " 153/462, mean: 99.62978877314815\n",
      " 154/462, mean: 101.1234375\n",
      " 155/462, mean: 101.62541956018518\n",
      " 156/462, mean: 102.13232060185184\n",
      " 157/462, mean: 101.95954137731482\n",
      " 158/462, mean: 102.63799189814814\n",
      " 159/462, mean: 102.75985966435185\n",
      " 160/462, mean: 102.4644748263889\n",
      " 161/462, mean: 103.06077835648148\n",
      " 162/462, mean: 102.58043981481481\n",
      " 163/462, mean: 102.63147424768519\n",
      " 164/462, mean: 102.5388744212963\n",
      " 165/462, mean: 102.47000868055555\n",
      " 166/462, mean: 102.40458622685185\n",
      " 167/462, mean: 102.49681712962963\n",
      " 168/462, mean: 102.33362991898149\n",
      " 169/462, mean: 102.32649739583333\n",
      " 170/462, mean: 102.23598813657408\n",
      " 171/462, mean: 102.31911892361111\n",
      " 172/462, mean: 102.43803530092593\n",
      " 173/462, mean: 102.48870081018518\n",
      " 174/462, mean: 102.31788917824075\n",
      " 175/462, mean: 102.11682581018519\n",
      " 176/462, mean: 102.2072120949074\n",
      " 177/462, mean: 101.59381510416667\n",
      " 178/462, mean: 101.65985243055556\n",
      " 179/462, mean: 102.25935329861112\n",
      " 180/462, mean: 100.77664930555555\n",
      " 181/462, mean: 99.3427662037037\n",
      " 182/462, mean: 98.5980324074074\n",
      " 183/462, mean: 102.03972077546297\n",
      " 184/462, mean: 102.30325520833334\n",
      " 185/462, mean: 100.7724681712963\n",
      " 186/462, mean: 98.22234519675926\n",
      " 187/462, mean: 92.45926649305555\n",
      " 188/462, mean: 61.18954716435185\n",
      " 189/462, mean: 113.57510127314815\n",
      " 190/462, mean: 130.00203993055555\n",
      " 191/462, mean: 90.7497974537037\n",
      " 192/462, mean: 85.5797019675926\n",
      " 193/462, mean: 100.28425925925926\n",
      " 194/462, mean: 102.2314380787037\n",
      " 195/462, mean: 82.98420138888889\n",
      " 196/462, mean: 80.44834346064815\n",
      " 197/462, mean: 78.99453848379629\n",
      " 198/462, mean: 76.93508391203704\n",
      " 199/462, mean: 76.4677806712963\n",
      " 200/462, mean: 69.61047453703704\n",
      " 201/462, mean: 77.2361255787037\n",
      " 202/462, mean: 77.87866753472223\n",
      " 203/462, mean: 76.49508101851852\n",
      " 204/462, mean: 76.82686631944445\n",
      " 205/462, mean: 76.08545283564816\n",
      " 206/462, mean: 75.74567418981482\n",
      " 207/462, mean: 75.44389467592593\n",
      " 208/462, mean: 75.37810329861111\n",
      " 209/462, mean: 75.37478298611111\n",
      " 210/462, mean: 75.69796006944445\n",
      " 211/462, mean: 76.64449508101852\n",
      " 212/462, mean: 76.87667100694445\n",
      " 213/462, mean: 76.95439091435185\n",
      " 214/462, mean: 77.50593171296296\n",
      " 215/462, mean: 77.37336516203703\n",
      " 216/462, mean: 76.30028935185184\n",
      " 217/462, mean: 77.5326171875\n",
      " 218/462, mean: 78.04099392361111\n",
      " 219/462, mean: 77.4615379050926\n",
      " 220/462, mean: 77.50187355324074\n",
      " 221/462, mean: 77.49117476851852\n",
      " 222/462, mean: 76.04733072916666\n",
      " 223/462, mean: 76.36780237268519\n",
      " 224/462, mean: 76.37099971064815\n",
      " 225/462, mean: 76.21555989583334\n",
      " 226/462, mean: 76.40432581018518\n",
      " 227/462, mean: 76.68446180555556\n",
      " 228/462, mean: 75.75698784722222\n",
      " 229/462, mean: 74.78671875\n",
      " 230/462, mean: 74.99958767361112\n",
      " 231/462, mean: 79.44278067129629\n",
      " 232/462, mean: 79.9029513888889\n",
      " 233/462, mean: 79.3060329861111\n",
      " 234/462, mean: 80.0869212962963\n",
      " 235/462, mean: 80.05740740740741\n",
      " 236/462, mean: 80.24236111111111\n",
      " 237/462, mean: 80.88822337962964\n",
      " 238/462, mean: 81.04415509259259\n",
      " 239/462, mean: 80.77717737268519\n",
      " 240/462, mean: 82.02487702546296\n",
      " 241/462, mean: 83.68232783564815\n",
      " 242/462, mean: 83.92630931712964\n",
      " 243/462, mean: 85.56297743055555\n",
      " 244/462, mean: 85.15912905092593\n",
      " 245/462, mean: 84.36225405092593\n",
      " 246/462, mean: 84.23177806712962\n",
      " 247/462, mean: 83.54127604166666\n",
      " 248/462, mean: 81.9283420138889\n",
      " 249/462, mean: 82.20737123842592\n",
      " 250/462, mean: 81.92366898148148\n",
      " 251/462, mean: 82.46586371527778\n",
      " 252/462, mean: 81.57503616898148\n",
      " 253/462, mean: 80.6257957175926\n",
      " 254/462, mean: 80.40358072916666\n",
      " 255/462, mean: 82.33821614583333\n",
      " 256/462, mean: 81.8775535300926\n",
      " 257/462, mean: 81.50555555555556\n",
      " 258/462, mean: 81.10539641203704\n",
      " 259/462, mean: 80.344921875\n",
      " 260/462, mean: 79.92447193287038\n",
      " 261/462, mean: 75.93075086805555\n",
      " 262/462, mean: 79.09267216435185\n",
      " 263/462, mean: 80.93507667824075\n",
      " 264/462, mean: 84.94773582175925\n",
      " 265/462, mean: 85.68509114583334\n",
      " 266/462, mean: 87.05834780092593\n",
      " 267/462, mean: 86.9841941550926\n",
      " 268/462, mean: 81.7185474537037\n",
      " 269/462, mean: 82.68598813657407\n",
      " 270/462, mean: 82.99047309027777\n",
      " 271/462, mean: 83.22904369212964\n",
      " 272/462, mean: 78.45381944444445\n",
      " 273/462, mean: 81.42648292824074\n",
      " 274/462, mean: 77.93818721064815\n",
      " 275/462, mean: 81.51158130787037\n",
      " 276/462, mean: 82.10604021990741\n",
      " 277/462, mean: 81.9021484375\n",
      " 278/462, mean: 82.88250144675926\n",
      " 279/462, mean: 82.10001446759259\n",
      " 280/462, mean: 83.66449652777777\n",
      " 281/462, mean: 82.49144241898148\n",
      " 282/462, mean: 83.42789351851852\n",
      " 283/462, mean: 83.88289930555555\n",
      " 284/462, mean: 84.53318865740741\n",
      " 285/462, mean: 83.88757233796296\n",
      " 286/462, mean: 83.21932870370371\n",
      " 287/462, mean: 83.0552806712963\n",
      " 288/462, mean: 83.00375434027778\n",
      " 289/462, mean: 83.10631510416667\n",
      " 290/462, mean: 83.22412471064816\n",
      " 291/462, mean: 82.46979890046296\n",
      " 292/462, mean: 82.66987847222222\n",
      " 293/462, mean: 82.58650173611112\n",
      " 294/462, mean: 82.38420862268518\n",
      " 295/462, mean: 82.07394386574074\n",
      " 296/462, mean: 81.85480324074074\n",
      " 297/462, mean: 81.88480902777778\n",
      " 298/462, mean: 81.5103515625\n",
      " 299/462, mean: 81.45206163194445\n",
      " 300/462, mean: 81.3884837962963\n",
      " 301/462, mean: 81.55966435185185\n",
      " 302/462, mean: 81.29280960648148\n",
      " 303/462, mean: 80.71286168981482\n",
      " 304/462, mean: 81.83746383101852\n",
      " 305/462, mean: 82.04233940972222\n",
      " 306/462, mean: 82.74489293981482\n",
      " 307/462, mean: 81.53457754629629\n",
      " 308/462, mean: 81.67710503472222\n",
      " 309/462, mean: 81.04132667824074\n",
      " 310/462, mean: 81.08153935185184\n",
      " 311/462, mean: 81.07895688657408\n",
      " 312/462, mean: 80.96827980324075\n",
      " 313/462, mean: 81.01095196759259\n",
      " 314/462, mean: 80.61263744212962\n",
      " 315/462, mean: 80.92843605324074\n",
      " 316/462, mean: 81.03271846064816\n",
      " 317/462, mean: 81.6972728587963\n",
      " 318/462, mean: 82.48922887731482\n",
      " 319/462, mean: 82.3095630787037\n",
      " 320/462, mean: 82.74698350694445\n",
      " 321/462, mean: 81.10441261574074\n",
      " 322/462, mean: 82.43057002314815\n",
      " 323/462, mean: 81.11093026620371\n",
      " 324/462, mean: 84.7752025462963\n",
      " 325/462, mean: 85.03209635416667\n",
      " 326/462, mean: 83.8900318287037\n",
      " 327/462, mean: 83.52455150462963\n",
      " 328/462, mean: 83.21674623842593\n",
      " 329/462, mean: 82.93931568287037\n",
      " 330/462, mean: 82.31030092592593\n",
      " 331/462, mean: 80.81161024305555\n",
      " 332/462, mean: 83.4736400462963\n",
      " 333/462, mean: 79.38510561342592\n",
      " 334/462, mean: 82.34227430555555\n",
      " 335/462, mean: 81.7849537037037\n",
      " 336/462, mean: 80.97615017361112\n",
      " 337/462, mean: 80.88650173611111\n",
      " 338/462, mean: 80.66096643518519\n",
      " 339/462, mean: 80.46457609953704\n",
      " 340/462, mean: 80.25379774305556\n",
      " 341/462, mean: 81.42390046296296\n",
      " 342/462, mean: 80.2263744212963\n",
      " 343/462, mean: 79.35018084490741\n",
      " 344/462, mean: 79.47856626157407\n",
      " 345/462, mean: 79.44278067129629\n",
      " 346/462, mean: 79.46331741898148\n",
      " 347/462, mean: 79.71541521990741\n",
      " 348/462, mean: 80.8652271412037\n",
      " 349/462, mean: 80.52212818287038\n",
      " 350/462, mean: 82.4277416087963\n",
      " 351/462, mean: 81.0236183449074\n",
      " 352/462, mean: 81.77474681712962\n",
      " 353/462, mean: 86.71549479166667\n",
      " 354/462, mean: 86.47298900462962\n",
      " 355/462, mean: 87.20985243055556\n",
      " 356/462, mean: 88.27419704861111\n",
      " 357/462, mean: 88.32879774305556\n",
      " 358/462, mean: 90.48589409722223\n",
      " 359/462, mean: 88.85734230324074\n",
      " 360/462, mean: 89.34149305555556\n",
      " 361/462, mean: 88.89915364583334\n",
      " 362/462, mean: 89.28824508101852\n",
      " 363/462, mean: 88.29830005787036\n",
      " 364/462, mean: 87.97586082175926\n",
      " 365/462, mean: 87.84513888888888\n",
      " 366/462, mean: 87.43169849537037\n",
      " 367/462, mean: 87.73802806712963\n",
      " 368/462, mean: 87.47670717592592\n",
      " 369/462, mean: 86.83293547453704\n",
      " 370/462, mean: 87.14934895833333\n",
      " 371/462, mean: 86.36489438657408\n",
      " 372/462, mean: 86.75287905092593\n",
      " 373/462, mean: 86.23171296296296\n",
      " 374/462, mean: 85.55805844907407\n",
      " 375/462, mean: 86.53312355324074\n",
      " 376/462, mean: 85.91566840277778\n",
      " 377/462, mean: 90.01969762731481\n",
      " 378/462, mean: 91.38041087962964\n",
      " 379/462, mean: 92.58433159722222\n",
      " 380/462, mean: 93.67425491898148\n",
      " 381/462, mean: 82.35580150462962\n",
      " 382/462, mean: 113.06106770833334\n",
      " 383/462, mean: 112.40130931712963\n",
      " 384/462, mean: 107.13135850694445\n",
      " 385/462, mean: 79.22511574074075\n",
      " 386/462, mean: 145.40951967592594\n",
      " 387/462, mean: 181.2163845486111\n",
      " 388/462, mean: 143.656640625\n",
      " 389/462, mean: 127.43211805555555\n",
      " 390/462, mean: 125.6671875\n",
      " 391/462, mean: 121.20517939814815\n",
      " 392/462, mean: 121.45420283564815\n",
      " 393/462, mean: 122.06698495370371\n",
      " 394/462, mean: 122.79573206018519\n",
      " 395/462, mean: 123.03676215277778\n",
      " 396/462, mean: 122.3392505787037\n",
      " 397/462, mean: 122.42471788194445\n",
      " 398/462, mean: 120.38297164351852\n",
      " 399/462, mean: 122.21959635416667\n",
      " 400/462, mean: 120.77144820601852\n",
      " 401/462, mean: 122.24320746527778\n",
      " 402/462, mean: 122.76351273148148\n",
      " 403/462, mean: 126.5201388888889\n",
      " 404/462, mean: 128.6199291087963\n",
      " 405/462, mean: 128.06039496527777\n",
      " 406/462, mean: 127.7334056712963\n",
      " 407/462, mean: 128.3384403935185\n",
      " 408/462, mean: 128.08917100694444\n",
      " 409/462, mean: 128.1814019097222\n",
      " 410/462, mean: 128.40103443287038\n",
      " 411/462, mean: 128.66948784722223\n",
      " 412/462, mean: 128.68793402777777\n",
      " 413/462, mean: 128.64956597222223\n",
      " 414/462, mean: 128.6129195601852\n",
      " 415/462, mean: 127.918359375\n",
      " 416/462, mean: 120.40559895833333\n",
      " 417/462, mean: 114.23621238425926\n",
      " 418/462, mean: 114.61448206018518\n",
      " 419/462, mean: 119.50185908564815\n",
      " 420/462, mean: 124.34951533564815\n",
      " 421/462, mean: 126.61802662037037\n",
      " 422/462, mean: 126.84946469907408\n",
      " 423/462, mean: 127.56296296296296\n",
      " 424/462, mean: 127.80325520833334\n",
      " 425/462, mean: 127.70130931712963\n",
      " 426/462, mean: 127.53578559027778\n",
      " 427/462, mean: 127.5673900462963\n",
      " 428/462, mean: 127.55152633101852\n",
      " 429/462, mean: 127.51180555555555\n",
      " 430/462, mean: 127.3751808449074\n",
      " 431/462, mean: 127.46323061342592\n",
      " 432/462, mean: 127.11336805555555\n",
      " 433/462, mean: 127.12086950231482\n",
      " 434/462, mean: 127.07241753472222\n",
      " 435/462, mean: 127.07647569444444\n",
      " 436/462, mean: 126.88303674768518\n",
      " 437/462, mean: 126.8880787037037\n",
      " 438/462, mean: 126.98153935185185\n",
      " 439/462, mean: 126.67213541666666\n",
      " 440/462, mean: 127.35931712962963\n",
      " 441/462, mean: 127.14607928240741\n",
      " 442/462, mean: 126.94157262731481\n",
      " 443/462, mean: 127.04339554398148\n",
      " 444/462, mean: 126.79695457175926\n",
      " 445/462, mean: 126.84503761574074\n",
      " 446/462, mean: 126.85229311342593\n",
      " 447/462, mean: 126.97957175925926\n",
      " 448/462, mean: 125.72055844907408\n",
      " 449/462, mean: 124.053515625\n",
      " 450/462, mean: 121.50068721064815\n",
      " 451/462, mean: 114.63046875\n",
      " 452/462, mean: 110.18985821759259\n",
      " 453/462, mean: 120.90204716435186\n",
      " 454/462, mean: 129.73678385416667\n",
      " 455/462, mean: 129.7255931712963\n",
      " 456/462, mean: 129.51616753472223\n",
      " 457/462, mean: 129.6465205439815\n",
      " 458/462, mean: 129.6359447337963\n",
      " 459/462, mean: 129.63582175925927\n",
      " 460/462, mean: 129.1923755787037\n",
      " 461/462, mean: 122.02455873842592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --arch=arm64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from 'Videos/aug72023vids/diff_frames*.jpg':\n",
      "  Duration: 00:00:28.81, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: mjpeg (Baseline), gray(bt470bg/unknown/unknown), 1920x1080 [SAR 1:1 DAR 16:9], 16 fps, 16 tbr, 16 tbn, 16 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0xaaaac3bfdf40] using SAR=1/1\n",
      "[libx264 @ 0xaaaac3bfdf40] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0xaaaac3bfdf40] profile High, level 4.0\n",
      "[libx264 @ 0xaaaac3bfdf40] 264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'Videos/aug72023vids/frame_differences.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  865 fps= 53 q=-1.0 Lsize=   43357kB time=00:00:28.73 bitrate=12361.2kbits/s dup=404 drop=0 speed=1.76x    \n",
      "video:43347kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.023399%\n",
      "[libx264 @ 0xaaaac3bfdf40] frame I:127   Avg QP:23.04  size:123037\n",
      "[libx264 @ 0xaaaac3bfdf40] frame P:276   Avg QP:28.84  size: 56045\n",
      "[libx264 @ 0xaaaac3bfdf40] frame B:462   Avg QP:30.33  size: 28771\n",
      "[libx264 @ 0xaaaac3bfdf40] consecutive B-frames: 26.7%  4.2%  6.2% 62.9%\n",
      "[libx264 @ 0xaaaac3bfdf40] mb I  I16..4:  9.9% 51.9% 38.2%\n",
      "[libx264 @ 0xaaaac3bfdf40] mb P  I16..4:  1.6%  7.3% 15.2%  P16..4:  6.7%  3.4%  2.1%  0.0%  0.0%    skip:63.7%\n",
      "[libx264 @ 0xaaaac3bfdf40] mb B  I16..4:  1.6%  2.6%  2.9%  B16..8: 22.4%  3.6%  1.5%  direct: 0.7%  skip:64.8%  L0:53.9% L1:45.4% BI: 0.7%\n",
      "[libx264 @ 0xaaaac3bfdf40] 8x8 transform intra:43.3% inter:4.2%\n",
      "[libx264 @ 0xaaaac3bfdf40] coded y,uvDC,uvAC intra: 30.1% 0.0% 0.0% inter: 4.2% 0.0% 0.0%\n",
      "[libx264 @ 0xaaaac3bfdf40] i16 v,h,dc,p: 56% 32% 11%  1%\n",
      "[libx264 @ 0xaaaac3bfdf40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 64%  8% 28%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0xaaaac3bfdf40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 43% 18% 26%  2%  2%  2%  3%  1%  3%\n",
      "[libx264 @ 0xaaaac3bfdf40] i8c dc,h,v,p: 100%  0%  0%  0%\n",
      "[libx264 @ 0xaaaac3bfdf40] Weighted P-Frames: Y:0.7% UV:0.0%\n",
      "[libx264 @ 0xaaaac3bfdf40] ref P L0: 54.0%  3.3% 25.3% 17.3%\n",
      "[libx264 @ 0xaaaac3bfdf40] ref B L0: 70.0% 19.7% 10.4%\n",
      "[libx264 @ 0xaaaac3bfdf40] ref B L1: 95.8%  4.2%\n",
      "[libx264 @ 0xaaaac3bfdf40] kb/s:12315.29\n"
     ]
    }
   ],
   "source": [
    "blinkinglights = sorted(glob.glob(\n",
    "    # \"runs/detect/predict10/crops/RC-Car/*.jpg\"\n",
    "    \"runs/detect/predict5/video_frames/*.jpg\"\n",
    "))\n",
    "\n",
    "for framefileind in range(1, len(blinkinglights)):\n",
    "    framefile = blinkinglights[framefileind]\n",
    "    lastframefile = blinkinglights[framefileind-1]\n",
    "    threshold = 100\n",
    "    # frame = cv2.imread(framefile)\n",
    "    # frame = cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY)\n",
    "    _, frame = cv2.threshold(cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "    # lastframe = cv2.imread(lastframefile)\n",
    "    # lastframe = cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY)\n",
    "    _, lastframe = cv2.threshold(cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "    framediff = frame-lastframe\n",
    "    \n",
    "    cv2.imwrite('Videos/aug72023vids/diff_frames%04d.jpg' % framefileind, framediff)\n",
    "    cv2.imshow('diff frame', framediff)\n",
    "    # cv2.imwrite('Videos/aug72023vids/cropped_vehicle_blinking_normal%04d.jpg' % framefileind, frame)\n",
    "    # cv2.imshow('frame', frame)\n",
    "    print(f' {framefileind}/{len(blinkinglights)}, mean: {frame.mean()}')\n",
    "    # frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # threshold = 70\n",
    "\n",
    "    # # Apply threshold to the masked grayscale image\n",
    "    # _, thresh = cv2.threshold(frame_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # cv2.imshow('frame', thresh)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "export_video(\n",
    "    input_val='Videos/aug72023vids/diff_frames*.jpg',\n",
    "    fps=16,\n",
    "    output_val='Videos/aug72023vids/frame_differences.mp4'\n",
    ")\n",
    "    \n",
    "# for threshold in range(1, 256):\n",
    "#     framefile = blinkinglights[0]\n",
    "#     # lastframefile = blinkinglights[framefileind-1]\n",
    "    \n",
    "#     # frame = cv2.imread(framefile)\n",
    "#     # frame = cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY)\n",
    "#     _, frame = cv2.threshold(cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "#     # lastframe = cv2.imread(lastframefile)\n",
    "#     # lastframe = cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY)\n",
    "#     # _, lastframe = cv2.threshold(cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "#     # framediff = frame-lastframe\n",
    "    \n",
    "#     # cv2.imshow('diff frame', framediff)\n",
    "#     frame = cv2.putText(frame, \"Threshold: %s\" % threshold, (5,15), cv2.FONT_HERSHEY_PLAIN, 0.75, (100,100,100), 1)\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     cv2.imwrite('Videos/aug72023vids/threshold%03d.jpg' % threshold, frame)\n",
    "#     # print(f' {framefileind}/{len(blinkinglights)}')\n",
    "#     # frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     # threshold = 70\n",
    "\n",
    "#     # # Apply threshold to the masked grayscale image\n",
    "#     # _, thresh = cv2.threshold(frame_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#     # cv2.imshow('frame', thresh)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# export_video(\n",
    "#     input_val='Videos/aug72023vids/threshold*.jpg',\n",
    "#     fps=20,\n",
    "#     output_val='Videos/aug72023vids/different_thresholds.mp4'\n",
    "# )\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple ways to detect a blinking light are the following\n",
    " - Take the average brightness of the cropped image, compare them frame by frame, and see when the biggest different is, and that is when it blinks ([explained here](https://stackoverflow.com/questions/38030170/best-way-to-detect-a-light-blinking-from-video-footage-via-python))\n",
    "    - You could do this on the normal cropped image, a greyscaled version, or a thresholded version\n",
    "    - This has problems in that it might be influenced by other factors, like shade, or the vehicle moving\n",
    "    - For my testing, it does work, however I set the threshold to 200. When the headlights blinked, the mean image value was 11, and otherwise it was 0.5\n",
    " - Take the difference of 2 consecutive full frames ([explained here](https://stackoverflow.com/questions/8877228/opencv-detect-blinking-lights)) to detect when the light blinks\n",
    "    - This should only be done with thresholded images (otherwise it causes seizures)\n",
    "    - This won't work because the vehicle moves so it will capture the difference between the vehicle's last positions, not the headlights, [as shown here](https://photos.app.goo.gl/CMKZ16492xYHcSuj6)\n",
    " - Take the difference of 2 consecutive _cropped_ frames to detect when the light blinks\n",
    "    - This won't work because the cropped frames have very slightly different dimensions, and it would be hard to align them as it would get the vehicle change, not the headlights\n",
    "\n",
    "I did the first option because it's the only one that can actually work. I did this by detecting the vehicle, cropping the vehicle to just that, converting the cropped image to only black or white (I call it thresholding the image, [here is a video where I try a bunch of different thresholds](https://photos.app.goo.gl/YyxE9YHTYNSLU2sCA)), and finding the average light value of the cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "Average Latency:  0.392 seconds\n"
     ]
    }
   ],
   "source": [
    "def crop_image(ogimage, cropboxes):\n",
    "    if len(cropboxes) == 0: np.array([])\n",
    "    cropbox = cropboxes[0] \n",
    "    tly, tlx, bry, brx = map(int, list(cropbox))\n",
    "    newimg = ogimage[tlx:brx, tly:bry]\n",
    "    return newimg    \n",
    "    \n",
    "def threshold_image(img, threshold=200):\n",
    "    _, newimg = cv2.threshold(\n",
    "        cv2.cvtColor(\n",
    "            img, \n",
    "            cv2.COLOR_BGR2GRAY\n",
    "        ), \n",
    "        threshold, 255, cv2.THRESH_BINARY\n",
    "    )\n",
    "    \n",
    "    return newimg\n",
    "    \n",
    "    \n",
    "\n",
    "def cv2_livefeed_rfdevice_wheadlights():\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    foldername = 'runs/detect/predict12/'\n",
    "    filename = foldername + 'image0.jpg'\n",
    "    latencies = []\n",
    "    count = 0\n",
    "    headlight_pattern = []\n",
    "    onoff_thres = 8\n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        startbig = time.time()\n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        results = model(\n",
    "            frame, save=True, verbose=False\n",
    "        )\n",
    "        # \n",
    "        # \n",
    "\n",
    "        cropped_image = crop_image(\n",
    "            ogimage = frame, cropboxes=results[0].boxes.xyxy.numpy()\n",
    "        )\n",
    "        \n",
    "        threshimg = threshold_image(\n",
    "            img=cropped_image, threshold=200\n",
    "        )\n",
    "        headlight_pattern.append(int(threshimg.mean() > onoff_thres))\n",
    "        \n",
    "        # cv2.imshow('boxed frame', cv2.imread(filename))\n",
    "        # cv2.imshow('cropped frame', cropped_image)\n",
    "        cv2.imshow('thresholded frame', threshimg)\n",
    "        \n",
    "        \n",
    "        endbig = time.time()\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "        cv2.imwrite(foldername + 'video_frames/frame_%s.jpg' % current_time, frame)\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(headlight_pattern)\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    \n",
    "cv2_livefeed_rfdevice_wheadlights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
