{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection & Confirmation System\n",
    "### Version 2\n",
    "Summer 2023 \n",
    "\n",
    "Created by Vikram Anantha \\\n",
    "Continued from Ben Dwyer's code \\\n",
    "July 27 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "(Same as V1) \\\n",
    "This code is meant to be implemented into Road-Side Systems (RSSs), like traffic cameras, such that it can communicate with vehicles, especially Autonomous Vehicles (AVs) to verify their identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Same as V1) \\\n",
    "One problem that might arise when AVs communicate with RSSs is that a hacker with malicious intent can join the same channel and communicate with the RSSs as if they were the vehicle. To combat this, the RSS can command the vehicle to confirm its identity by performing a specific task. Examples of this include:\n",
    " + Displaying a specific pattern on a screen, similar to a QR code\n",
    " + Flashing headlights in a specific pattern\n",
    " + Making a sound in a specific pattern\n",
    "\n",
    "Once the vehicle performs the task, the RSS can confirm this has been done visually, thus confirming the identiy of the vehicle. This visual confirmation, a form of Two Factor Authentication (2FA) is the premise of this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an overview of the main simple code:\n",
    "~~~\n",
    "while loop forever:\n",
    "   read in the frame from the ZED camera\n",
    "   detect the vehicle in the frame\n",
    "   process the frame:\n",
    "      save a cropped version of the frame of just the vehicle\n",
    "      greyscale the cropped image\n",
    "      convert the image to a binary image (any pixel above a brightness is white, any below is black); I call it \"thresholding\" the image\n",
    "      find the Mean Image Brightness (MIB)\n",
    "      high MIBs means brighter image, so the headlight is on, lower means dimmer means headlights off\n",
    "   add the on/off state to a list\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes from V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were the shortcomings from Version 1:\n",
    " 1. When using live feed, there is about a second of delay per frame to detect if a vehicle is present, and to recognize there is no bounding box. It is much more time to recognize the array in the bounding box, which needs to be done for each vehicle.\n",
    " 2. The code runs a for loop to go through each vehicle and detect the pattern, meaning it verifies each vehicle one by one. Because each vehicle takes a long time to be verified, when multiple vehicles are present, the system will take a _very_ long time.\n",
    " 3. When using a webcam, or a dedicated camera for the computer (as in not a smartphone camera, which uses HDR to make screens appear normal with everything else), the Arduino display is super bright, and cannot be recognized. However, this doesn't really matter as much, since in the real environment, having a screen on the windshield won't be implemented. Instead, it would most likely leverage the headlights, having them flash a pattern across time, rather than display a pattern across space.\n",
    "\n",
    "Version 2 solves these problems in these ways:\n",
    " 1. In this Version, the code only detects if there is a vehicle present, so only using 1 model instead of 2. This should cut the latency by about half. In addition, this code has the model on device, meaning it can utilize the extensive hardware to run the model faster. This cuts the latency by half again.\n",
    " 2. This part isn't really fixed, since it still has to go through every vehicle to detect their pattern verification\n",
    " 3. This version does not need a camera with HDR, in fact it actually leverages the HDR-less camera. When the headlights are shining, the camera overexposes them, meaning they will be much brighter than everything else. This makes it much easier to threshold the image and calculate the average brightness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and Reports\n",
    "In this file, I have the code that I wrote (or borrowed), however I also inclde some mid-way reports, either what I wrote at the end of the day to get my thoughts in order, or what I sent to Dajiang at the end of the day. I kept this in to show my thought process and all the steps taken to make the final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Version 2 reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meeting with Dajiang Suo [Jul 24 2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "ZED obj detection should be faster\n",
    " - https://www.stereolabs.com/docs/object-detection/\n",
    " - Detect vehicle + detect headlights + detect headlight pattern\n",
    " - ^^ simultaneously\n",
    " - Api provided by zed camera to do categorization\n",
    " - Look through https://www.stereolabs.com/docs/object-detection/custom-od/\n",
    " - To get obj detection and custom detection\n",
    "\n",
    "\n",
    "Action-State Joint Learning-Based Vehicle Taillight\n",
    " - https://arxiv.org/pdf/1906.03683.pdf\n",
    " - Recognition in Diverse Actual Traffic Scenes\n",
    "\n",
    "\n",
    "Document all these steps rigorouly\n",
    "\n",
    "\n",
    "Another Paper\n",
    " - https://github.com/TechToker/CarLightSignalsDetection\n",
    " - ^^ sliding window of frames?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Todo (Jul 27 2023):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " + implement Zed camera based object detection\n",
    " + run a model to use headlights as the pattern recognition\n",
    "    + first get that to run on a given video\n",
    "    + then write arduino code to make the display act like headlights\n",
    "    + then get it to work in real time\n",
    "    + then see if the pattern recognition is needed with ML or just if statements\n",
    " + try to get multiple vehicles to work fast (idk how yet)\n",
    "    + zed api?\n",
    " + Priority 1: documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dtaidistance import dtw\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "import random\n",
    "\n",
    "BENS_RF_APIKEY = \"2Vs9PCO5LGDCSkI0huRq\"\n",
    "BENS_RF_WORKSPACE = \"meng-thesis-5fidi\"\n",
    "BENS_RF_PROJECT = \"rc-vehicle-detection_v2\"\n",
    "ONOFF_THRES = 8\n",
    "HL_THRESHOLD = 200\n",
    "\n",
    "avg = lambda x: sum(x)/len(x)\n",
    "def export_video(input_val, output_val, fps=25):\n",
    "    # stich images together into video\n",
    "    os.system(f\"ffmpeg -y -framerate {fps} -pattern_type glob -i '{input_val}' -c:v libx264 -r 30 -pix_fmt yuv420p {output_val}\")\n",
    "def save_frames(image, dir):\n",
    "    current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "    cv2.imwrite(dir % current_time, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zed Camera Code: Object Detection Birds Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is borrowed from the ZED Camera API, which has this and many more codes here\n",
    "# https://github.com/stereolabs/zed-sdk/tree/master/tutorials/tutorial%206%20-%20object%20detection/python\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#\n",
    "# Copyright (c) 2022, STEREOLABS.\n",
    "#\n",
    "# All rights reserved.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
    "# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
    "# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
    "# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
    "# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
    "# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
    "# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
    "# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
    "# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "    This sample demonstrates how to capture 3D point cloud and detected objects\n",
    "    with the ZED SDK and display the result in an OpenGL window.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "# import helpercode.ogl_viewer.viewer as gl\n",
    "import helpercode.cv_viewer.tracking_viewer as cv_viewer\n",
    "from helpercode.batch_system_handler import *\n",
    "# because these files are in some random folder, I copied the important ones into the directory `helpercode`\n",
    "\n",
    "\n",
    "##\n",
    "# Variable to enable/disable the batch option in Object Detection module\n",
    "# Batching system allows to reconstruct trajectories from the object detection module by adding Re-Identification / Appareance matching.\n",
    "# For example, if an object is not seen during some time, it can be re-ID to a previous ID if the matching score is high enough\n",
    "# Use with caution if image retention is activated (See batch_system_handler.py) :\n",
    "#   --> Images will only appear if an object is detected since the batching system is based on OD detection.\n",
    "USE_BATCHING = False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "if True:\n",
    "    print(\"Running object detection ... Press 'Esc' to quit\")\n",
    "    zed = sl.Camera()\n",
    "    \n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters()\n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP  \n",
    "    init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "    init_params.depth_maximum_distance = 20\n",
    "    is_playback = False                             # Defines if an SVO is used\n",
    "        \n",
    "    # If applicable, use the SVO given as parameter\n",
    "    # Otherwise use ZED live stream\n",
    "    if len(sys.argv) == 2:\n",
    "        filepath = sys.argv[1]\n",
    "        print(\"Using SVO file: {0}\".format(filepath))\n",
    "        init_params.svo_real_time_mode = True\n",
    "        init_params.set_from_svo_file(filepath)\n",
    "        is_playback = True\n",
    "\n",
    "    status = zed.open(init_params)\n",
    "    if status != sl.ERROR_CODE.SUCCESS:\n",
    "        print(repr(status))\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # Enable positional tracking module\n",
    "    positional_tracking_parameters = sl.PositionalTrackingParameters()\n",
    "    # If the camera is static in space, enabling this setting below provides better depth quality and faster computation\n",
    "    positional_tracking_parameters.set_as_static = True\n",
    "    zed.enable_positional_tracking(positional_tracking_parameters)\n",
    "\n",
    "    # Enable object detection module\n",
    "    batch_parameters = sl.BatchParameters()\n",
    "    if USE_BATCHING:\n",
    "        batch_parameters.enable = True\n",
    "        batch_parameters.latency = 2.0\n",
    "        batch_handler = BatchSystemHandler(batch_parameters.latency*2)\n",
    "    else:\n",
    "        batch_parameters.enable = False\n",
    "    obj_param = sl.ObjectDetectionParameters(batch_trajectories_parameters=batch_parameters)\n",
    "        \n",
    "    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.MULTI_CLASS_BOX_FAST\n",
    "    # Defines if the object detection will track objects across images flow.\n",
    "    obj_param.enable_tracking = True\n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    camera_infos = zed.get_camera_information()\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    point_cloud_res = sl.Resolution(min(camera_infos.camera_configuration.resolution.width, 720), min(camera_infos.camera_configuration.resolution.height, 404)) \n",
    "    point_cloud_render = sl.Mat()\n",
    "    # viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)\n",
    "    \n",
    "    # Configure object detection runtime parameters\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "    detection_confidence = 60\n",
    "    obj_runtime_param.detection_confidence_threshold = detection_confidence\n",
    "    # To select a set of specific object classes\n",
    "    obj_runtime_param.object_class_filter = [sl.OBJECT_CLASS.PERSON]\n",
    "    # To set a specific threshold\n",
    "    obj_runtime_param.object_class_detection_confidence_threshold = {sl.OBJECT_CLASS.PERSON: detection_confidence} \n",
    "\n",
    "    # Runtime parameters\n",
    "    runtime_params = sl.RuntimeParameters()\n",
    "    runtime_params.confidence_threshold = 50\n",
    "\n",
    "    # Create objects that will store SDK outputs\n",
    "    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "    objects = sl.Objects()\n",
    "    image_left = sl.Mat()\n",
    "\n",
    "    # Utilities for 2D display\n",
    "    display_resolution = sl.Resolution(min(camera_infos.camera_configuration.resolution.width, 1280), min(camera_infos.camera_configuration.resolution.height, 720))\n",
    "    image_scale = [display_resolution.width / camera_infos.camera_configuration.resolution.width\n",
    "                 , display_resolution.height / camera_infos.camera_configuration.resolution.height]\n",
    "    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239,255], np.uint8)\n",
    "\n",
    "    # Utilities for tracks view\n",
    "    camera_config = zed.get_camera_information().camera_configuration\n",
    "    tracks_resolution = sl.Resolution(400, display_resolution.height)\n",
    "    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance)\n",
    "    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)\n",
    "    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)\n",
    "\n",
    "    # Will store the 2D image and tracklet views \n",
    "    global_image = np.full((display_resolution.height, display_resolution.width+tracks_resolution.width, 4), [245, 239, 239,255], np.uint8)\n",
    "\n",
    "    # Camera pose\n",
    "    cam_w_pose = sl.Pose()\n",
    "    cam_c_pose = sl.Pose()\n",
    "\n",
    "    quit_app = False\n",
    "\n",
    "    # while(viewer.is_available() and (quit_app == False)):\n",
    "    while((quit_app == False)):\n",
    "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
    "            # Retrieve objects\n",
    "            returned_state = zed.retrieve_objects(objects, obj_runtime_param)\n",
    "            \n",
    "            if (returned_state == sl.ERROR_CODE.SUCCESS and objects.is_new):\n",
    "                # Retrieve point cloud\n",
    "                zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA,sl.MEM.CPU, point_cloud_res)\n",
    "                point_cloud.copy_to(point_cloud_render)\n",
    "                # Retrieve image\n",
    "                zed.retrieve_image(image_left, sl.VIEW.LEFT, sl.MEM.CPU, display_resolution)\n",
    "                image_render_left = image_left.get_data()\n",
    "                # Get camera pose\n",
    "                zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)\n",
    "\n",
    "                update_render_view = True\n",
    "                # update_3d_view = True\n",
    "                update_tracking_view = True\n",
    "\n",
    "                if USE_BATCHING:\n",
    "                    zed.get_position(cam_c_pose, sl.REFERENCE_FRAME.CAMERA)\n",
    "                    objects_batch = []\n",
    "                    zed.get_objects_batch(objects_batch)\n",
    "                    batch_handler.push(cam_c_pose,cam_w_pose,image_left,point_cloud,objects_batch)\n",
    "                    cam_c_pose, cam_w_pose, image_left, point_cloud_render, objects = batch_handler.pop(cam_c_pose,cam_w_pose,image_left,point_cloud,objects)\n",
    "                    \n",
    "                    image_render_left = image_left.get_data()\n",
    "                    \n",
    "                    update_tracking_view = objects.is_new\n",
    "\n",
    "                    if WITH_IMAGE_RETENTION:\n",
    "                        update_render_view = objects.is_new\n",
    "                        # update_3d_view = objects.is_new\n",
    "                    else:\n",
    "                        update_render_view = True\n",
    "                        # update_3d_view = True\n",
    "\n",
    "                # 3D rendering\n",
    "                # if update_3d_view:\n",
    "                    # viewer.updateData(point_cloud_render, objects)\n",
    "\n",
    "                # 2D rendering\n",
    "                if update_render_view:\n",
    "                    np.copyto(image_left_ocv,image_render_left)\n",
    "                    cv_viewer.render_2D(image_left_ocv,image_scale,objects, obj_param.enable_tracking)\n",
    "                    global_image = cv2.hconcat([image_left_ocv,image_track_ocv])\n",
    "\n",
    "                # Tracking view\n",
    "                if update_tracking_view:\n",
    "                    track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)\n",
    "                    \n",
    "            cv2.imshow(\"ZED | 2D View and Birds View\",global_image)\n",
    "            cv2.waitKey(10)\n",
    "\n",
    "        if (is_playback and (zed.get_svo_position() == zed.get_svo_number_of_frames()-1)):\n",
    "            print(\"End of SVO\")\n",
    "            quit_app = True\n",
    "\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    # viewer.exit()\n",
    "    image_left.free(sl.MEM.CPU)\n",
    "    point_cloud.free(sl.MEM.CPU)\n",
    "    point_cloud_render.free(sl.MEM.CPU)\n",
    "\n",
    "    if USE_BATCHING:\n",
    "        batch_handler.clear()\n",
    "\n",
    "    # Disable modules and close camera\n",
    "    zed.disable_object_detection()\n",
    "    zed.disable_positional_tracking()\n",
    "\n",
    "    zed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting the Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About RoboFlow and Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Roboflow](https://roboflow.com/) is a platform which houses a vast variety of models, mainly for computer vision. It is often used because it is easy to train the models, and prediction happens over the cloud. For computer vision, Roboflow uses [YOLOv8](https://blog.roboflow.com/whats-new-in-yolov8/) (You Only Look Once), the fastest and most accurate architecture.\n",
    "\n",
    "Roboflow has a very intuitive process to upload training images and annotate them. For models that detect (rather than just classify), the model not only must be given the image, but also the coordinates on the image of where the object in question is, and what the object is called. Roboflow allows you to create the training data fairly easily, and can then be downloaded to be trained on device.\n",
    "\n",
    "In Version 1, the training data and the models was stored on RoboFlow, meaning anytime the code required a prediction from the model, it would make a call through the internet. This is ideal for most cases, (i.e. if the user is using a Windows laptop from 1997), however the RSS is meant to be run on an Edge computer, meaning it should have the capabilities to run the model on device. In this version, the model is trained and run on device, making it much faster.\n",
    "\n",
    "To do this, the dataset created in RoboFlow is downloaded onto the device, and a YoloV8 model is trained with this dataset.\n",
    "\n",
    "When the dataset is downloaded, it is stored in a folder (for example, `RC-Vehicle-Detection_v2-1`), which has some stuff\n",
    " - `test/`\n",
    "    - `images/`\n",
    "        - folder of the unedited images that were uploaded\n",
    "    - `labels/`\n",
    "        - txt files with the coords of where the object is and its classification\n",
    " - `train/`\n",
    "    - same subfolders as `test`\n",
    " - `valid/`\n",
    "    - same subfolders as `test`\n",
    " - `data.yaml`\n",
    "    - the main datafile, has info about where the images are, what the clssifications are, etc. This file is the only file inputted when training the model \n",
    " - `README`s\n",
    "\n",
    "The model is then trained with this data, and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Download the Dataset from Ben's RoboFlow project\n",
    "\n",
    "# !pip install roboflow\n",
    "# ^^ Don't run this line if it's already installed\n",
    "\n",
    "rf = Roboflow(api_key=BENS_RF_APIKEY)\n",
    "project = rf.workspace(BENS_RF_WORKSPACE).project(BENS_RF_PROJECT)\n",
    "dataset = project.version(1).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a custom detector\n",
    "\n",
    "# based on this tutorial: https://learnopencv.com/train-yolov8-on-custom-dataset/\n",
    "\n",
    "# Load the model.\n",
    "model = YOLO('yolov8m.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8m_rc_v2'\n",
    ")\n",
    "\n",
    "# For nano model, epochs 100, imgsize 640, it took 769 mins to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8s.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   # data = 'RC-Vehicle-Detection_v2-1/data.yaml',\n",
    "   imgsz=640,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8s_rc_v3'\n",
    ")\n",
    "\n",
    "# If a model ever gets interrupted in training, you can continue it by doing\n",
    "\n",
    "# model = YOLO('runs/detect/yolov8m_rc_v2/weights/last.pt')\n",
    "# model.train(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuming interrupted model\n",
    "# Weekend of Aug 3 it took 6800 mins\n",
    "# Aug 7 night      it took 1232 mins\n",
    "# Aug 10 weekend   it took 1712 mins\n",
    "\n",
    "model = YOLO('runs/detect/yolov8m_rc_v2/weights/last.pt')\n",
    "\n",
    "model.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default YOLOv8 model is the nano model, which is said to be the smallest and is therefore the fastest. However, there are 2 other models that we can use: YOLOv8s (small) and YOLOv8m (medium). These models are bigger, so in theory should be more accurate but might take longer.\n",
    "\n",
    "Here is how each model performed:\n",
    "\n",
    "\n",
    "Training\n",
    "| Model Size | Epochs | Image Size | Training Time | Code Name |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Nano | 100 | 640 | 769 mins (12.8 hrs) | Nano 100x640 |\n",
    "| Medium | 71 | 1280 | 6800 mins (4.7 days) | Med 71x1280 |\n",
    "| Medium | 100 | 1280 | 9744 mins (6.8 days) | Med 100x1280 |\n",
    "\n",
    "Testing\n",
    "| Model Size | Image Source | Time per Image | Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| Nano 100x640 | Photo from Ben's phone   | 0.1s | ~85% |\n",
    "| Nano 100x640 | Video from my phone   | 0.3s | ~85% |\n",
    "| Nano 100x640 | Frame from ZED Camera | 0.5s | ~80% |\n",
    "| Med 100x1280 | Photo from Ben's phone | 0.7s | 87% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')\n",
    "model = YOLO('runs/detect/yolov8s_rc_v3/weights/best.pt')\n",
    "# metrics = model.val()\n",
    "# metrics.box.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying it on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(glob.glob('RC-Vehicle-Detection_v2-1/test/images/*.jpg'),\n",
    "                save=True,\n",
    "                save_txt=True,\n",
    "                save_conf=True,\n",
    "                save_crop=True,\n",
    "                )\n",
    "# Nano: Takes 3.6 seconds to predict on 35 images, so about 0.1s per image.\n",
    "# Small: Takes 9.5 seconds to predict on 35 images, so about 0.27s per image\n",
    "# Medium: 50s to predict 35 images, 1.4s per image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making my own score function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(realdata_path, fakedata_path):\n",
    "    \n",
    "    realdata_files = glob.glob(realdata_path)\n",
    "    fakedata_files = glob.glob(fakedata_path)\n",
    "    if (len(realdata_files) != len(fakedata_files)):\n",
    "        raise \"WrongFilePathException\"\n",
    "    confs = []\n",
    "    bounds_diffs = []\n",
    "    for i in range(len(realdata_files)):\n",
    "        with open(realdata_files[i], 'r') as rfile:\n",
    "            robj, rxl, rxr, ryl, ryr = map(float, rfile.readline().split(\" \"))\n",
    "        with open(fakedata_files[i], 'r') as ffile:\n",
    "            fobj, fxl, fxr, fyl, fyr, fconf = map(float, ffile.readline().split(\" \"))\n",
    "            \n",
    "        confs.append(fconf)\n",
    "        bounds_diffs.append(abs(rxl-fxl)*100)\n",
    "        bounds_diffs.append(abs(rxr-fxr)*100)\n",
    "        bounds_diffs.append(abs(ryl-fyl)*100)\n",
    "        bounds_diffs.append(abs(ryr-fyr)*100)\n",
    "        \n",
    "    print(\"Average Confidence:             \", avg(confs))\n",
    "    print(\"Average Bounding Box Difference:\", avg(bounds_diffs), \"% of the image\")\n",
    "\n",
    "score(\n",
    "    realdata_path='RC-Vehicle-Detection_v2-1/test/labels/*.txt',\n",
    "    fakedata_path = 'runs/detect/predict14/labels/BEN_*.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the model on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Videos/drive_05/drive_05_vikram2023.mp4\",\n",
    "      save=True,\n",
    "      save_txt=True,\n",
    "      save_conf=True,\n",
    "      save_crop=True\n",
    ")\n",
    "\n",
    "# 15.5s video, with save,save_txt,save_conf,save_crop:\n",
    "# \n",
    "# 160 seconds to predict\n",
    "# 1/3 seconds to predict per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CV2 to detect RCC with on device model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_livefeed_rfdevice(foldername = 'runs/detect/predict10/'):\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    \n",
    "    filename = foldername + 'image0.jpg'\n",
    "    # Where it saves the frame\n",
    "    \n",
    "    latencies = [] # For understanding later\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read() # getting the frame\n",
    "        startbig = time.time() # to calculate the latency\n",
    "        \n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        model(\n",
    "            frame, \n",
    "            save=True, \n",
    "            verbose=False\n",
    "        )\n",
    "        # The model can save the image, but does so in some random place\n",
    "        \n",
    "        # display frame\n",
    "        boxedframe = cv2.imread(filename)\n",
    "        endbig = time.time()\n",
    "        cv2.imshow('frame', boxedframe)\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        \n",
    "        \n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        \n",
    "        save_frames(\n",
    "            image=frame,\n",
    "            dir = foldername + 'video_frames/frame_%s.jpg'\n",
    "        )\n",
    "        # To save the frames if you want to make a video\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    \n",
    "    # Turn the frames into a video\n",
    "    export_video(\n",
    "        input_val=foldername + 'video_frames/frame_*.jpg',\n",
    "        fps=4,\n",
    "        output_val=foldername + 'in_the_sun.mp4'\n",
    "    )\n",
    "    \n",
    "cv2_livefeed_rfdevice(\n",
    "    foldername = 'runs/detect/predict10/' \n",
    ")\n",
    "# ~0.52 seconds per loop\n",
    "# ~0.42 seconds of latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ZED camera tracking code with custom detection to recognize the vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses ZED Camera's code\n",
    "# The benefit of this is that it uses the 2 cameras to create a 3D map\n",
    "# This means it can detect the distance of the object, and it can track it across the space\n",
    "# However its a bit hard to modify this code\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from threading import Lock, Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "avg = lambda x: sum(x)/len(x)\n",
    "\n",
    "# import helpercode.custom.ogl_viewer.viewer as gl\n",
    "import helpercode.cv_viewer.tracking_viewer as cv_viewer\n",
    "# from helpercode.batch_system_handler import *\n",
    "\n",
    "lock = Lock()\n",
    "run_signal = False\n",
    "exit_signal = False\n",
    "\n",
    "\n",
    "def xywh2abcd(xywh, im_shape):\n",
    "    output = np.zeros((4, 2))\n",
    "\n",
    "    # Center / Width / Height -> BBox corners coordinates\n",
    "    x_min = (xywh[0] - 0.5*xywh[2]) #* im_shape[1]\n",
    "    x_max = (xywh[0] + 0.5*xywh[2]) #* im_shape[1]\n",
    "    y_min = (xywh[1] - 0.5*xywh[3]) #* im_shape[0]\n",
    "    y_max = (xywh[1] + 0.5*xywh[3]) #* im_shape[0]\n",
    "\n",
    "    # A ------ B\n",
    "    # | Object |\n",
    "    # D ------ C\n",
    "\n",
    "    output[0][0] = x_min\n",
    "    output[0][1] = y_min\n",
    "\n",
    "    output[1][0] = x_max\n",
    "    output[1][1] = y_min\n",
    "\n",
    "    output[2][0] = x_min\n",
    "    output[2][1] = y_max\n",
    "\n",
    "    output[3][0] = x_max\n",
    "    output[3][1] = y_max\n",
    "    return output\n",
    "\n",
    "def detections_to_custom_box(detections, im0):\n",
    "    output = []\n",
    "    for i, det in enumerate(detections):\n",
    "        xywh = det.xywh[0]\n",
    "\n",
    "        # Creating ingestable objects for the ZED SDK\n",
    "        obj = sl.CustomBoxObjectData()\n",
    "        obj.bounding_box_2d = xywh2abcd(xywh, im0.shape)\n",
    "        obj.label = det.cls\n",
    "        obj.probability = det.conf\n",
    "        obj.is_grounded = False\n",
    "        output.append(obj)\n",
    "    return output\n",
    "\n",
    "\n",
    "def torch_thread(weights, img_size, conf_thres=0.2, iou_thres=0.45):\n",
    "    global image_net, exit_signal, run_signal, detections\n",
    "\n",
    "    print(\"Intializing Network...\")\n",
    "\n",
    "    model = YOLO(weights)\n",
    "\n",
    "    while not exit_signal:\n",
    "        if run_signal:\n",
    "            lock.acquire()\n",
    "\n",
    "            img = cv2.cvtColor(image_net, cv2.COLOR_BGRA2RGB)\n",
    "            # https://docs.ultralytics.com/modes/predict/#video-suffixes\n",
    "            det = model.predict(img, save=False, imgsz=img_size, conf=conf_thres, iou=iou_thres)[0].cpu().numpy().boxes\n",
    "\n",
    "            # ZED CustomBox format (with inverse letterboxing tf applied)\n",
    "            detections = detections_to_custom_box(det, image_net)\n",
    "            lock.release()\n",
    "            run_signal = False\n",
    "        sleep(0.01)\n",
    "\n",
    "\n",
    "def main():\n",
    "    global image_net, exit_signal, run_signal, detections\n",
    "    weights = \"runs/detect/yolov8n_rc_v13/weights/best.pt\"\n",
    "    img_size = 416\n",
    "    conf_thres=0.4\n",
    "    svo = None\n",
    "    capture_thread = Thread(target=torch_thread, kwargs={\n",
    "        'weights': weights, \n",
    "        'img_size': img_size, \n",
    "        \"conf_thres\": conf_thres\n",
    "    })\n",
    "    capture_thread.start()\n",
    "    \n",
    "    latencies = []\n",
    "\n",
    "    print(\"Initializing Camera...\")\n",
    "\n",
    "    zed = sl.Camera()\n",
    "\n",
    "    input_type = sl.InputType()\n",
    "    if svo is not None:\n",
    "        input_type.set_from_svo_file(svo)\n",
    "\n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters(input_t=input_type, svo_real_time_mode=True)\n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # QUALITY\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP\n",
    "    init_params.depth_maximum_distance = 50\n",
    "\n",
    "    runtime_params = sl.RuntimeParameters()\n",
    "    status = zed.open(init_params)\n",
    "\n",
    "    if status != sl.ERROR_CODE.SUCCESS:\n",
    "        print(repr(status))\n",
    "        exit()\n",
    "\n",
    "    image_left_tmp = sl.Mat()\n",
    "\n",
    "    print(\"Initialized Camera\")\n",
    "\n",
    "    positional_tracking_parameters = sl.PositionalTrackingParameters()\n",
    "    # If the camera is static, uncomment the following line to have better performances and boxes sticked to the ground.\n",
    "    # positional_tracking_parameters.set_as_static = True\n",
    "    zed.enable_positional_tracking(positional_tracking_parameters)\n",
    "\n",
    "    obj_param = sl.ObjectDetectionParameters()\n",
    "    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.CUSTOM_BOX_OBJECTS\n",
    "    obj_param.enable_tracking = True\n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    objects = sl.Objects()\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "\n",
    "    # Display\n",
    "    camera_infos = zed.get_camera_information()\n",
    "    camera_res = camera_infos.camera_configuration.resolution\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    point_cloud_res = sl.Resolution(min(camera_res.width, 720), min(camera_res.height, 404))\n",
    "    point_cloud_render = sl.Mat()\n",
    "    # viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)\n",
    "    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "    image_left = sl.Mat()\n",
    "    # Utilities for 2D display\n",
    "    display_resolution = sl.Resolution(min(camera_res.width, 1280), min(camera_res.height, 720))\n",
    "    image_scale = [display_resolution.width / camera_res.width, display_resolution.height / camera_res.height]\n",
    "    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239, 255], np.uint8)\n",
    "\n",
    "    # Utilities for tracks view\n",
    "    camera_config = camera_infos.camera_configuration\n",
    "    tracks_resolution = sl.Resolution(400, display_resolution.height)\n",
    "    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance)\n",
    "    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)\n",
    "    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)\n",
    "    # Camera pose\n",
    "    cam_w_pose = sl.Pose()\n",
    "\n",
    "    # while viewer.is_available() and not exit_signal:\n",
    "    while not exit_signal:\n",
    "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
    "            # -- Get the image\n",
    "            \n",
    "            lock.acquire()\n",
    "            zed.retrieve_image(image_left_tmp, sl.VIEW.LEFT)\n",
    "            \n",
    "            image_net = image_left_tmp.get_data()\n",
    "            lock.release()\n",
    "            run_signal = True\n",
    "            startbig = time.time()\n",
    "\n",
    "            # -- Detection running on the other thread\n",
    "            while run_signal:\n",
    "                sleep(0.001)\n",
    "\n",
    "            # Wait for detections\n",
    "            lock.acquire()\n",
    "            # -- Ingest detections\n",
    "            zed.ingest_custom_box_objects(detections)\n",
    "            lock.release()\n",
    "            zed.retrieve_objects(objects, obj_runtime_param)\n",
    "\n",
    "            # -- Display\n",
    "            # Retrieve display data\n",
    "            zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA, sl.MEM.CPU, point_cloud_res)\n",
    "            point_cloud.copy_to(point_cloud_render)\n",
    "            zed.retrieve_image(image_left, sl.VIEW.LEFT, sl.MEM.CPU, display_resolution)\n",
    "            zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)\n",
    "\n",
    "            # 3D rendering\n",
    "            # viewer.updateData(point_cloud_render, objects)\n",
    "            # 2D rendering\n",
    "            np.copyto(image_left_ocv, image_left.get_data())\n",
    "            cv_viewer.render_2D(image_left_ocv, image_scale, objects, obj_param.enable_tracking)\n",
    "            global_image = cv2.hconcat([image_left_ocv, image_track_ocv])\n",
    "            # Tracking view\n",
    "            track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)\n",
    "            endbig = time.time()\n",
    "            cv2.imshow(\"ZED | 2D View and Birds View\", global_image)\n",
    "            key = cv2.waitKey(10)\n",
    "            if key == 27:\n",
    "                exit_signal = True\n",
    "                \n",
    "            \n",
    "            print(round(endbig-startbig, 2), \"seconds\")\n",
    "            latencies.append(endbig-startbig)\n",
    "        else:\n",
    "            exit_signal = True\n",
    "\n",
    "    # viewer.exit()\n",
    "    exit_signal = True\n",
    "    zed.close()\n",
    "    \n",
    "    print(round(avg(latencies), 3), \"seconds per loop\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    main()\n",
    "    \n",
    "# 0.605 seconds per loop\n",
    "# 0.605 seconds of latency\n",
    "\n",
    "\n",
    "# in hallway, max distance = 4.3 meters\n",
    "# 0.68 seconds per loop\n",
    "\n",
    "# in outside, 5.6 max partly cloudy\n",
    "# 80% outside partly cloudy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report after Jul 31 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the YOLOv8 model and have it predicting on device- the performance is pretty good. When I run it on a simple image, it takes about 0.1 seconds to predict. When I run it on a video, it takes about 0.3 seconds per frame. When I run it on live feed with CV2, it takes about 0.43 seconds of processing. However, when I run the custom detector on the ZED Camera code (which processes both cameras, and tracks the object), it takes about 0.65 seconds to process. Is this latency a good amount, or is it too much?\n",
    "\n",
    "I also tried it outside. When the car is coming towards the camera, it has about an 80% accuracy, but can identify the car pretty well, in any scenario. The car can go about 5.6 meters away before the camera stops detecting it. The algorithm was pretty good about recognizing the car in the sunlight, however when I tested it today it was about 5pm and partly cloudy so it might not be the best example- I'll check again tomorrow in more direct sunlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report After Aug 2 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a video yesterday of how the object detection performs in sunny weather when the model is on device using YoloV8 nano model, and it isn't the worst, however it isn't the best. I found that it works best  when the lighting is consistent (everything in the frame is either sunny or shady); when there is a variety, (half shady half sunny) the camera can't decide whether to make the sunny parts too sunny or the shady parts too shady, so it does some combination of both, which doesn't make the frame that good (since these kinds of webcams don't have High Dynamic Range which most of our phone cameras do). [The video (with the confidence levels) is here](https://photos.app.goo.gl/2ZW4CrC4H61wA8sZ6)\n",
    "\n",
    "I think that this accuracy can be fixed simply by making the training data more diverse (since it was largely  just the car in a hallway, a dorm room, and outside but not that sunny), and making sure that the camera taking the photos is also diverse, or is at least the zed camera\n",
    "\n",
    "Something that I thought about while I was on my run this morning is that right now, the code runs like this:\n",
    "while true:\n",
    "   read frame from zed cam\n",
    "   do image recognition on the frame [~0.5 seconds to do]\n",
    "   display the frame\n",
    "\n",
    "This means that the frames are being read about once every half second. The way I had envisioned the blinking would be that within a span of 2 seconds, it would have blinked a considerable number of times in different durations to be able to convey a unique code. My concern with this is that since the camera is reading in each frame infrequently, it wouldn't register most of the blinks.\n",
    "\n",
    "I thought of a couple of fixes for this:\n",
    " - We could have each blink last at least half a second, to ensure the edge computer can read each time the display turns off and on. The only issue with this is that the message might take several seconds to convey, which a car wouldn't have if it is moving on the road\n",
    " - We could also try parallel processing, so that the edge computer would read each frame in separately from the processing, meaning the camera will capture every time the car headlights blink. The problem with this is that the computer has to process each of those frames, which would take about 0.5s per frame, so by the time the identity is confirmed the car is long gone\n",
    " - This could also be a problem with the model itself. I noticed that the default model Roboflow comes with has very little latency, so it isn't impossible to reach very fast prediction times, however I assume that they achieved this with a large, diverse, body of training data, and possibly fine tuning the hyperparameters of the model. It is possible to get the training data, but would take a lot of manual work. \n",
    "\n",
    "I have been trying to train the models with different parameters, but I've only been able to train it over night. I'll do a performance review of each of the models to see which one is the fastest and most accurate.\n",
    "\n",
    "I was also thinking, since I now can edit the code on Ben's display, we could try to display a QR code which links to a url hosted on a server (that could submit a POST request), which the edge computer is connected to. I figured this might be good since there should be some optimized models out there to scan QR codes, so it should help with latency, however this still requires the board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting the headlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background For Detecting the Headlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [this paper from Innopolis University in Russia](https://github.com/TechToker/Vehicle-rear-lights-analyser/blob/master/FinalReport.pdf), researchers intended detect if the car in front of them were braking or not. \n",
    "\n",
    "The researchers use YOLOv4 to detect whether a car is present for each frame, and then cropp the frame to just the car.\n",
    "\n",
    "To detect the presense of brakelights, they convert each image into YCrCb space, then removes all pixels that aren't pure red. They utilize the fact that the taillights are symmetrical in cars to weed away noise from other cars. They then apply a bounding box over the taillights.\n",
    "\n",
    "To determine if the car is braking, it determines the brightness in the bounding boxes. It's important to note that it has to see what the car looks like when braking vs not braking to know which is brighter, and thus be able to distinguish when it's braking.\n",
    "\n",
    "Our algorithm can utilize soemthing similar, but will have to identify the headlights a different way since headlights aren't red. Potentially, a ML model could be used to determine the headlights' position. Determining when its on vs off would be the same as the Innopolis paper.\n",
    "\n",
    "\n",
    "\n",
    "[This article](https://ieeexplore.ieee.org/abstract/document/6761567) talks about how the researchers detected headlights to track vehicles. They first converted the image to a binary image (meaning grayscaled, then converted everything above a certain threshold to 255 and below that threshold to 0), and detected the blobs. The then detected the blobs of white areas, and considered each blob a candidate for one of the pair of headlights. Finally, they used the property that headlights are symmetrical to pair 2 of the blobs and determine if they are symmetrical (using some equations).\n",
    "From this, they applied it to counting and tracking vehicles in night time / low light. It remains to be seen whether this technique would work in the daytime (since converting the image to binary might not work as well / produce more blobs in higher light), but a method like this could work on the prototype car (since it has the red display board)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using a dataset from RoboFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report From Aug 1 2023 (kind of just a flood of random thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps that need to be done now.\n",
    "\n",
    "I need to find a good way to detect headlights.\n",
    "There is a headlights ML algorithm that I can train it on\n",
    "That needs to be done over night\n",
    "\n",
    "I also would need to make the headlights work on Ben's display\n",
    "I need Ben's arduino code to make that happen\n",
    "\n",
    "I also need to find a way to decode the headlights\n",
    "I need the display to make that work?\n",
    "\n",
    "Overnight I should train the algorithms\n",
    "Then tmr I should test to see if it works in the day\n",
    "Then find the intensity values\n",
    "\n",
    "Also take a picture of actual car headluights, play around with those in different formats (Binary, YCrCb, etc), also needs to be done at home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# Downloading a headlights detection dataset\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"2Vs9PCO5LGDCSkI0huRq\")\n",
    "project = rf.workspace(\"nours-g8unb\").project(\"vehicles_night\")\n",
    "dataset = project.version(3).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a custom detector\n",
    "\n",
    "# based on this tutorial: https://learnopencv.com/train-yolov8-on-custom-dataset/\n",
    "\n",
    "\n",
    " \n",
    "# Load the model.\n",
    "model = YOLO('yolov8n.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8n_headlights_v1'\n",
    ")\n",
    "\n",
    "# Took 1258 mins for imgsz=1280, epochs 100, nano model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reopen the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_headlights_v1/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(glob.glob('Videos/headlight_test/*.jpg'),\n",
    "                save=True,\n",
    "                # save_txt=True,\n",
    "                save_conf=True,\n",
    "                # save_crop=True,\n",
    "                )\n",
    "# Takes 5.9 seconds to predict on 6 images, so about 1s per image.\n",
    "\n",
    "# Accuracy really bad\n",
    "# Scrap this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Don't detect headlights, detect flashes of light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Aug 7 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Detect headlights, one approach is to detect the vehicle like normal, and simply detect when the vehicle's lights flash by detecting when the vehicle region becomes bright. This way, you don't have to recognize where a vehicle's headlights are, you can instead act like a human and only notice when the lights flash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple ways to detect a blinking light are the following\n",
    " - Take the average brightness of the cropped image, compare them frame by frame, and see when the biggest different is, and that is when it blinks ([explained here](https://stackoverflow.com/questions/38030170/best-way-to-detect-a-light-blinking-from-video-footage-via-python))\n",
    "    - You could do this on the normal cropped image, a greyscaled version, or a thresholded version\n",
    "    - This has problems in that it might be influenced by other factors, like shade, or the vehicle moving\n",
    "    - For my testing, it does work, however I set the threshold to 200. When the headlights blinked, the mean image value was 11, and otherwise it was 0.5\n",
    " - Take the difference of 2 consecutive full frames ([explained here](https://stackoverflow.com/questions/8877228/opencv-detect-blinking-lights)) to detect when the light blinks\n",
    "    - This should only be done with thresholded images (otherwise it causes seizures)\n",
    "    - This won't work because the vehicle moves so it will capture the difference between the vehicle's last positions, not the headlights, [as shown here](https://photos.app.goo.gl/CMKZ16492xYHcSuj6)\n",
    " - Take the difference of 2 consecutive _cropped_ frames to detect when the light blinks\n",
    "    - This won't work because the cropped frames have very slightly different dimensions, and it would be hard to align them as it would get the vehicle change, not the headlights\n",
    "\n",
    "I did the first option because it's the only one that can actually work. I did this by detecting the vehicle, cropping the vehicle to just that, converting the cropped image to only black or white (I call it thresholding the image, [here is a video where I try a bunch of different thresholds](https://photos.app.goo.gl/YyxE9YHTYNSLU2sCA)), and finding the average light value of the cropped image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open and run the RC detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(['Videos/PXL_20230807_143124752.MP.jpg', 'Videos/PXL_20230807_143126993.MP.jpg'],\n",
    "                save=True,\n",
    "                # save_txt=True,\n",
    "                save_conf=True,\n",
    "                save_crop=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking frames, detecting the RC, Thresholding them to detect blinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blinkinglights = sorted(glob.glob(\n",
    "    # \"runs/detect/predict10/crops/RC-Car/*.jpg\"\n",
    "    \"runs/detect/predict5/video_frames/*.jpg\"\n",
    "))\n",
    "\n",
    "for framefileind in range(1, len(blinkinglights)):\n",
    "    framefile = blinkinglights[framefileind]\n",
    "    lastframefile = blinkinglights[framefileind-1]\n",
    "    threshold = 100\n",
    "    # frame = cv2.imread(framefile)\n",
    "    # frame = cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY)\n",
    "    _, frame = cv2.threshold(cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "    # lastframe = cv2.imread(lastframefile)\n",
    "    # lastframe = cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY)\n",
    "    _, lastframe = cv2.threshold(cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "    framediff = frame-lastframe\n",
    "    \n",
    "    cv2.imwrite('Videos/aug72023vids/diff_frames%04d.jpg' % framefileind, framediff)\n",
    "    cv2.imshow('diff frame', framediff)\n",
    "    # cv2.imwrite('Videos/aug72023vids/cropped_vehicle_blinking_normal%04d.jpg' % framefileind, frame)\n",
    "    # cv2.imshow('frame', frame)\n",
    "    print(f' {framefileind}/{len(blinkinglights)}, mean: {frame.mean()}')\n",
    "    # frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # threshold = 70\n",
    "\n",
    "    # # Apply threshold to the masked grayscale image\n",
    "    # _, thresh = cv2.threshold(frame_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # cv2.imshow('frame', thresh)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "export_video(\n",
    "    input_val='Videos/aug72023vids/diff_frames*.jpg',\n",
    "    fps=16,\n",
    "    output_val='Videos/aug72023vids/frame_differences.mp4'\n",
    ")\n",
    "    \n",
    "# for threshold in range(1, 256):\n",
    "#     framefile = blinkinglights[0]\n",
    "#     # lastframefile = blinkinglights[framefileind-1]\n",
    "    \n",
    "#     # frame = cv2.imread(framefile)\n",
    "#     # frame = cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY)\n",
    "#     _, frame = cv2.threshold(cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "#     # lastframe = cv2.imread(lastframefile)\n",
    "#     # lastframe = cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY)\n",
    "#     # _, lastframe = cv2.threshold(cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "#     # framediff = frame-lastframe\n",
    "    \n",
    "#     # cv2.imshow('diff frame', framediff)\n",
    "#     frame = cv2.putText(frame, \"Threshold: %s\" % threshold, (5,15), cv2.FONT_HERSHEY_PLAIN, 0.75, (100,100,100), 1)\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     cv2.imwrite('Videos/aug72023vids/threshold%03d.jpg' % threshold, frame)\n",
    "#     # print(f' {framefileind}/{len(blinkinglights)}')\n",
    "#     # frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     # threshold = 70\n",
    "\n",
    "#     # # Apply threshold to the masked grayscale image\n",
    "#     # _, thresh = cv2.threshold(frame_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#     # cv2.imshow('frame', thresh)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# export_video(\n",
    "#     input_val='Videos/aug72023vids/threshold*.jpg',\n",
    "#     fps=20,\n",
    "#     output_val='Videos/aug72023vids/different_thresholds.mp4'\n",
    "# )\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(ogimage, cropboxes):\n",
    "    if len(cropboxes) == 0: np.array([])\n",
    "    cropbox = cropboxes[0] \n",
    "    tly, tlx, bry, brx = map(int, list(cropbox))\n",
    "    newimg = ogimage[tlx:brx, tly:bry]\n",
    "    return newimg    \n",
    "    \n",
    "def threshold_image(img, threshold=200):\n",
    "    _, newimg = cv2.threshold(\n",
    "        cv2.cvtColor(\n",
    "            img, \n",
    "            cv2.COLOR_BGR2GRAY\n",
    "        ), \n",
    "        threshold, 255, cv2.THRESH_BINARY\n",
    "    )\n",
    "    \n",
    "    return newimg\n",
    "\n",
    "def detect_onoff_headlights_v1(img):\n",
    "    # greyscale and threshold the image\n",
    "    threshimg = threshold_image(\n",
    "        img=img, threshold=HL_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return int(threshimg.mean() > ONOFF_THRES), threshimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function on live feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_livefeed_rfdevice_wheadlights(model):\n",
    "    \n",
    "    # model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    foldername = 'runs/detect/predict12/'\n",
    "    filename = foldername + 'image0.jpg'\n",
    "    latencies = []\n",
    "    count = 0\n",
    "    headlight_pattern = []\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        startbig = time.time()\n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        results = model(\n",
    "            frame, \n",
    "            save=True, \n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "\n",
    "        # crop the image to just the vehicle\n",
    "        cropped_image = crop_image(\n",
    "            ogimage = frame, cropboxes=results[0].boxes.xyxy.numpy()\n",
    "        )\n",
    "        \n",
    "        hl_onoff, threshimg = detect_onoff_headlights_v1(\n",
    "            img=cropped_image\n",
    "        )\n",
    "        headlight_pattern.append(hl_onoff)\n",
    "\n",
    "        \n",
    "        # cv2.imshow('boxed frame', cv2.imread(filename))\n",
    "        # cv2.imshow('cropped frame', cropped_image)\n",
    "        cv2.imshow('thresholded frame', threshimg)\n",
    "        \n",
    "        \n",
    "        endbig = time.time()\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "        cv2.imwrite(foldername + 'video_frames/frame_%s.jpg' % current_time, frame)\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    print(headlight_pattern)\n",
    "\n",
    "    \n",
    "cv2_livefeed_rfdevice_wheadlights(model)\n",
    "\n",
    "# Average latency: 0.392s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes about the Encryption Architecture (Aug 21 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The encryption should be based on something you know about the vehicle and have a random element (like how credit card numbers have the digits in the prefix to know the company of the card, and a pseudo-random sequence of numbers in the suffix)\n",
    "\n",
    "Marco can create a function, whose inputs are information about the vehicle, and output is a string (or list) of ones and zeros, which will be displayed on the headlights (0 for off, 1 for on, flashed at a certain frequency). \n",
    "\n",
    "The main constraint is that the vehicle will be moving 10mph - 30mph, meaning there will only be 10bits - 30bits of information that can be transmitted. However, the number of bits generated depends on the speed of the car, so a question becomes **how will the camera system know what the speed of the car will be, to make the most optimal encrypted string?**\n",
    "\n",
    "The camera system and the vehicle will both need to know what the correct code is.\n",
    "\n",
    "One approach:\n",
    " - Implement the function to allow each vehicle and infrastructure to have this random function, and both objects will have the same seed to generate the random sequence. One question is then **how will both objects have that seed?**\n",
    "    - Option 1: The seed could be something public about the car, like the license plate but that's not secure\n",
    "    - Option 2: Seed could be sent securely from the camera to the vehicle, so then **what's the best way to send it?**\n",
    "        - Option 1: Asymmetric Encryption, but that is computationally expensive, and not quantum secure (which isn't a big problem now)\n",
    "        - Option 2: Symmetric Encryption, however that requires both the sender and receiver to know a common private key, meaning all vehicles will have to be registerd in some database. **Is this feasible?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirming the headlight pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Approach Version 2)\n",
    "\n",
    "This algorithm takes in the detected headlight sequence, `detected`, compare it to the originally generated headlight sequence, `generated`, and see if the detected sequence can confirm the identity of the vehicle.\n",
    "\n",
    "One example of the generated sequence is `1,0,1,1,0`.\n",
    "\n",
    "One could think that you can directly compare the two lists, and see if they are the same. However, a problem with this is that the camera is reading in frames at twice the frame rate as the screen is flashing. This means, for the example sequence above, the sequences would be \\\n",
    "`1...0...1...1...0..` (generated) \\\n",
    "`1,1,0,0,1,1,1,1,0,0` (detected)\n",
    "\n",
    "However it isn't as simple as doubling each element in the sequence because sometimes, the camera will read in frames faster or slower than what is predicted (it depends on rounding, loop-by-loop latencies, etc). This means the sequences might look more like this: \\\n",
    "`1...0.....1...1.0..` (generated) \\\n",
    "`1,1,0,0,0,1,1,1,0,0` (detected)\n",
    "\n",
    "In addition, the camera might not be completely accurate in detecting the headlights, for example something might obstruct the view of the headlights (a bird might fly in front of the camera, for example): \\\n",
    "`1...0...1...1...0..` (generated) \\\n",
    "`1,1,0,0,1,0,1,1,0,0` (detected)\n",
    "\n",
    "The algorithm needs to be able to handle small problems: the camera and screen aren't completely synced; an object might obstruct the headlights. \n",
    "\n",
    "My algorithm uses a [Dynamic Time Warp (DTW)](https://dtaidistance.readthedocs.io/en/latest/usage/dtw.html) to handle the second. A DTW can compare two temporal sequences, which may slightly differ. The DTW is very good at quantifying how many slight differences there are, by giving back a distance between the two sequences. For example, here are two sequences, where DTW gives where the sequences match up, and guives a distance of sqrt(2) because there are 2 places where they differ:\n",
    "\n",
    "![image](figs/inplace_diff.png) \\\n",
    "Distance: sqrt(2)\n",
    "\n",
    "Where the DTW fails is when the two sequences are on two different time scales, it doesn't try to preserve the length of each leg (a consecutive stretch of 1s or 0s) of the pattern. This image shows two time scales, which shouldn't be deemed the same in our context, but DTW gives a distance of 0:\n",
    "\n",
    "![image](figs/timescale_diff.png) \\\n",
    "Distance: 0\n",
    "\n",
    "This means in addition to the DTW, I need my own algorithm to detect how in-sync the time scales are.\n",
    "\n",
    "My algorithm takes the connected points (called a path) that it determines, and checks the ratio of lengths of each leg between the detected sequence and the generated sequence. If they are roughly equal to the ratio between the headlight-flashing interval and the camera-frame-read interval, then it is good. For example if the length of each leg is: \\\n",
    "6, 4, 8, 7, 2 for the detected sequence, and \\\n",
    "3, 2, 4, 3, 1 for the generated sequence, that that is correct (the 7 vs 3 is considered a time-scale error, but is still deemed correct since the headlights might have meant to flash for 3 seconds and the camera read 3.5).\n",
    "\n",
    "However if the lengths were \\\n",
    "6, 4, 8, 8, 2 for the detected sequence, and \\\n",
    "3, 2, 4, 3, 1 for the generated sequence, then that is an error because the headlights must have been flashing for 4 seconds, not 3.\n",
    "\n",
    "This approach considers both types of errors, and accounts for them while still reporting if the sequences do not verify the vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance import dtw\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "import random\n",
    "s1 = [1, 0, 0,0,0,0,0,0, 1, 0, 1,1, 0, 1, 1, 1]\n",
    "\n",
    "s2 = [1,0,1,0,1,0,1]\n",
    "\n",
    "# s1 = [0,0,1,1,0,0,0,0,1,1,0,0,1,1,1,1,1,1]\n",
    "# s2 = [0,  1,  0,  0,  1,  0,  1,  1,  1]\n",
    "path = dtw.warping_path(\n",
    "    s1, s2\n",
    ")\n",
    "print(path)\n",
    "dtwvis.plot_warping(s1,s2, path, filename=\"warp.png\")\n",
    "print(dtw.distance(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance import dtw\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "\n",
    "s2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "s1 = [0,1,1,1,0,0,1,0,1,0,0]\n",
    "\n",
    "path = dtw.warping_path(\n",
    "    s1, s2\n",
    ")\n",
    "print(path)\n",
    "dtwvis.plot_warping(s1,s2, path, filename=\"trial1.png\")\n",
    "print(dtw.distance(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sequences_v1(detected, generated):\n",
    "    from dtaidistance import dtw\n",
    "    from dtaidistance import dtw_visualisation as dtwvis\n",
    "\n",
    "    path = dtw.warping_path(\n",
    "        detected, generated\n",
    "    )\n",
    "    print(path)\n",
    "    dtwvis.plot_warping(detected, generated, path, filename=\"veh_trial.png\")\n",
    "    print(dtw.distance(detected, generated))\n",
    "    # the lower the better distance    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to account for the timescales for both sequences being skewed, I need to write my own algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My thought process for making the algorithm (Aug 23 2023)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "DTW is good at quantifying how far off the pattern is (solely in terms of when 0 changes to 1 and vice versa) but not how many 0s and 1s there are in a row. To solve this, I can create my own algorithm that quantifies how far off the pattern is (in terms of numbers in a row).\n",
    "\n",
    "\n",
    "There are 2 things I will need to account for:\n",
    "1. FPS is off: the camera reads in images at half second intervals, however for Nyquest's theorm, the flashes of the headlights have to be at 1s intervals. This means if the headlights flash `0,1,0`, the camera will read `0,0,1,1,0,0`. There may be problems with this, like if the camera reads in faster than 1/2s, so it may read in `0,0,1,1,1,0,0`. My algorithm will quantify these discrepensies.\n",
    "2. If a bird flies through the camera frame, for example, the camera feed of `0,1,1,0,0,1,0,1` may read `0,1,1,0,0,0,0,1`. My algorithm will have to count the number of times this happens.\n",
    "\n",
    "\n",
    "To do this, I think I can use DTW to find the distances of the paths, which should give which point on path 1 corrosponds to a point on path 2. I can make my own function to compare how many points corrospond to the paths at each segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sequences_v2(detected, generated, caminterval, flashinterval, skipfirst=False, save='warp.png'):\n",
    "    path = dtw.warping_path(\n",
    "        detected, generated\n",
    "    )\n",
    "    distance = dtw.distance(\n",
    "        detected, generated\n",
    "    )\n",
    "    dtwvis.plot_warping(detected, generated, path, filename=save)\n",
    "    \n",
    "    \n",
    "    detectedlen = {}\n",
    "    generatedlen = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    lastg = -1\n",
    "    for ind, dg in enumerate(path):\n",
    "        d, ge = dg\n",
    "        \n",
    "        if (ind > 0 and generated[path[ind-1][1]] == generated[path[ind][1]]):\n",
    "            g = lastg\n",
    "            \n",
    "        else:\n",
    "            g = ge\n",
    "            lastg = g\n",
    "            \n",
    "        if (ind > 0 and path[ind-1][1] != path[ind][1] and generated[path[ind-1][1]] == generated[path[ind][1]]):\n",
    "            generatedlen[g]+=1\n",
    "        else:\n",
    "            generatedlen[g] = 1\n",
    "            \n",
    "        if (g not in detectedlen): detectedlen[g] = 0\n",
    "        detectedlen[g]+=1\n",
    "    \n",
    "    weirdcounter = 0\n",
    "    for g in detectedlen:\n",
    "        if skipfirst: \n",
    "            if g == 0: continue\n",
    "        weirdcounter += int(abs(detectedlen[g] - ( generatedlen[g] * (flashinterval / caminterval) )) / (flashinterval/caminterval))\n",
    "        \n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Path:\", path)\n",
    "    print(\"Dist:\", distance)\n",
    "    for d,g in path:\n",
    "        print(d,g)    \n",
    "    print(\"DetectedLen: \", detectedlen)\n",
    "    print(\"GeneratedLen:\", generatedlen)\n",
    "    print(weirdcounter, \"/\", len(generated))\n",
    "    \n",
    "    # return weirdcounter/len(generated), distance**2 / len(generated)\n",
    "    return weirdcounter, distance**2\n",
    "        \n",
    "camerr, birderr = compare_sequences_v2(\n",
    "    detected= [1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1],\n",
    "    generated=[1, 0, 1, 0, 1],\n",
    "    caminterval=1,\n",
    "    flashinterval=1,\n",
    "    save='figs/timescale_diff.png'\n",
    ")\n",
    "\n",
    "print(camerr, birderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alg works pretty well\n",
    "\n",
    "When the vehicle is displaying the right pattern, it shows anywhere from 0-3 distance (depending on how many birds fly through), and has a timescale error of 0.\n",
    "\n",
    "When the vehicle is displaying the wrong pattern, it shows 5+ distance, and has a timescale error avging around 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses functions from other parts of the code, so they may have to be run first before this can be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_livefeed_rfdevice_wheadlights(model):\n",
    "    \n",
    "    # model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    foldername = 'runs/detect/predict12/'\n",
    "    filename = foldername + 'image0.jpg'\n",
    "    latencies = []\n",
    "    count = 0\n",
    "    headlight_pattern = []\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        startbig = time.time()\n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        results = model(\n",
    "            frame, \n",
    "            save=True, \n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "\n",
    "        # crop the image to just the vehicle\n",
    "        cropped_image = crop_image(\n",
    "            ogimage = frame, cropboxes=results[0].boxes.xyxy.numpy()\n",
    "        )\n",
    "        \n",
    "        hl_onoff, threshimg = detect_onoff_headlights_v1(\n",
    "            img=cropped_image\n",
    "        )\n",
    "        headlight_pattern.append(hl_onoff)\n",
    "\n",
    "        \n",
    "        # cv2.imshow('boxed frame', cv2.imread(filename))\n",
    "        # cv2.imshow('cropped frame', cropped_image)\n",
    "        cv2.imshow('thresholded frame', threshimg)\n",
    "        \n",
    "        \n",
    "        endbig = time.time()\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "        cv2.imwrite(foldername + 'video_frames/frame_%s.jpg' % current_time, frame)\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    print(headlight_pattern)\n",
    "    \n",
    "    # compare_sequences_v1(\n",
    "    #     detected=headlight_pattern,\n",
    "    #     generated= [0,1,1,1,0,0,1,0,1,0,0]\n",
    "    # )\n",
    "    \n",
    "    camerr, birderr = compare_sequences_v2(\n",
    "        detected=headlight_pattern,\n",
    "        # generated= [0,1,1,1,0,0,1,0,1,0,0],\n",
    "        generated= [random.randint(0,1) for _ in range(10)],\n",
    "        caminterval=avg(latencies),\n",
    "        flashinterval=1,\n",
    "        skipfirst=True\n",
    "    )\n",
    "    \n",
    "    print(camerr, birderr)\n",
    "    \n",
    "cv2_livefeed_rfdevice_wheadlights(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
