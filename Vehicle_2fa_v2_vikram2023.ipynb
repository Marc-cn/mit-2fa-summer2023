{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection & Confirmation System\n",
    "### Version 2\n",
    "Summer 2023 \n",
    "\n",
    "Created by Vikram Anantha \\\n",
    "Continued from Ben Dwyer's code \\\n",
    "July 27 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "(Same as V1) \\\n",
    "This code is meant to be implemented into Road-Side Systems (RSSs), like traffic cameras, such that it can communicate with vehicles, especially Autonomous Vehicles (AVs) to verify their identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "(Same as V1) \\\n",
    "One problem that might arise when AVs communicate with RSSs is that a hacker with malicious intent can join the same channel and communicate with the RSSs as if they were the vehicle. To combat this, the RSS can command the vehicle to confirm its identity by performing a specific task. Examples of this include:\n",
    " + Displaying a specific pattern on a screen, similar to a QR code\n",
    " + Flashing headlights in a specific pattern\n",
    " + Making a sound in a specific pattern\n",
    "\n",
    "Once the vehicle performs the task, the RSS can confirm this has been done visually, thus confirming the identiy of the vehicle. This visual confirmation, a form of Two Factor Authentication (2FA) is the premise of this code.\n",
    "\n",
    "\n",
    "### Overview of Version 2\n",
    "\n",
    "Here is an overview of the main simple code:\n",
    "\n",
    "`while loop forever:`\n",
    " - `read in the frame from the ZED camera`\n",
    " - `detect the vehicle in the frame`\n",
    " - `process the frame:`\n",
    "    - `save a cropped version of the frame of just the vehicle`\n",
    "    - `greyscale the cropped image`\n",
    "    - `convert the image to a binary image (any pixel above a brightness is white, any below is black); I call it \"thresholding\" the image`\n",
    "    - `find the Mean Image Brightness (MIB)`\n",
    "    - `high MIBs means brighter image, so the headlight is on, lower means dimmer means headlights off`\n",
    " - `add the on/off state to a list`\n",
    "\n",
    "\n",
    "### Changes from V1\n",
    "\n",
    "These were the shortcomings from Version 1:\n",
    " 1. When using live feed, there is about a second of delay per frame to detect if a vehicle is present, and to recognize there is no bounding box. It is much more time to recognize the array in the bounding box, which needs to be done for each vehicle.\n",
    " 2. The code runs a for loop to go through each vehicle and detect the pattern, meaning it verifies each vehicle one by one. Because each vehicle takes a long time to be verified, when multiple vehicles are present, the system will take a _very_ long time.\n",
    " 3. When using a webcam, or a dedicated camera for the computer (as in not a smartphone camera, which uses HDR to make screens appear normal with everything else), the Arduino display is super bright, and cannot be recognized. However, this doesn't really matter as much, since in the real environment, having a screen on the windshield won't be implemented. Instead, it would most likely leverage the headlights, having them flash a pattern across time, rather than display a pattern across space.\n",
    "\n",
    "Version 2 solves these problems in these ways:\n",
    " 1. In this Version, the code only detects if there is a vehicle present, so only using 1 model instead of 2. This should cut the latency by about half. In addition, this code has the model on device, meaning it can utilize the extensive hardware to run the model faster. This cuts the latency by half again.\n",
    " 2. This part isn't really fixed, since it still has to go through every vehicle to detect their pattern verification\n",
    " 3. This version does not need a camera with HDR, in fact it actually leverages the HDR-less camera. When the headlights are shining, the camera overexposes them, meaning they will be much brighter than everything else. This makes it much easier to threshold the image and calculate the average brightness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and Reports\n",
    "In this file, I have the code that I wrote (or borrowed), however I also inclde some mid-way reports, either what I wrote at the end of the day to get my thoughts in order, or what I sent to Dajiang at the end of the day. I kept this in to show my thought process and all the steps taken to make the final algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Version 2 report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meeting with Dajiang Suo [Jul 24 2023]\n",
    "\n",
    "\n",
    "ZED obj detection should be faster\n",
    " - https://www.stereolabs.com/docs/object-detection/\n",
    " - Detect vehicle + detect headlights + detect headlight pattern\n",
    " - ^^ simultaneously\n",
    " - Api provided by zed camera to do categorization\n",
    " - Look through https://www.stereolabs.com/docs/object-detection/custom-od/\n",
    " - To get obj detection and custom detection\n",
    "\n",
    "\n",
    "Action-State Joint Learning-Based Vehicle Taillight\n",
    " - https://arxiv.org/pdf/1906.03683.pdf\n",
    " - Recognition in Diverse Actual Traffic Scenes\n",
    "\n",
    "\n",
    "Document all these steps rigorouly\n",
    "\n",
    "\n",
    "Another Paper\n",
    " - https://github.com/TechToker/CarLightSignalsDetection\n",
    " - ^^ sliding window of frames?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo (Jul 27 2023):\n",
    " + implement Zed camera based object detection\n",
    " + run a model to use headlights as the pattern recognition\n",
    "    + first get that to run on a given video\n",
    "    + then write arduino code to make the display act like headlights\n",
    "    + then get it to work in real time\n",
    "    + then see if the pattern recognition is needed with ML or just if statements\n",
    " + try to get multiple vehicles to work fast (idk how yet)\n",
    "    + zed api?\n",
    " + Priority 1: documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "BENS_RF_APIKEY = \"2Vs9PCO5LGDCSkI0huRq\"\n",
    "BENS_RF_WORKSPACE = \"meng-thesis-5fidi\"\n",
    "BENS_RF_PROJECT = \"rc-vehicle-detection_v2\"\n",
    "ONOFF_THRES = 8\n",
    "HL_THRESHOLD = 200\n",
    "\n",
    "avg = lambda x: sum(x)/len(x)\n",
    "def export_video(input_val, output_val, fps=25):\n",
    "    # stich images together into video\n",
    "    os.system(f\"ffmpeg -y -framerate {fps} -pattern_type glob -i '{input_val}' -c:v libx264 -r 30 -pix_fmt yuv420p {output_val}\")\n",
    "def save_frames(image, dir):\n",
    "    current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "    cv2.imwrite(dir % current_time, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zed Camera Code: Object Detection Birds Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is borrowed from the ZED Camera API, which has this and many more codes here\n",
    "# https://github.com/stereolabs/zed-sdk/tree/master/tutorials/tutorial%206%20-%20object%20detection/python\n",
    "\n",
    "\n",
    "########################################################################\n",
    "#\n",
    "# Copyright (c) 2022, STEREOLABS.\n",
    "#\n",
    "# All rights reserved.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
    "# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
    "# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n",
    "# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
    "# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
    "# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
    "# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
    "# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
    "# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
    "# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\"\"\"\n",
    "    This sample demonstrates how to capture 3D point cloud and detected objects\n",
    "    with the ZED SDK and display the result in an OpenGL window.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "# import helpercode.ogl_viewer.viewer as gl\n",
    "import helpercode.cv_viewer.tracking_viewer as cv_viewer\n",
    "from helpercode.batch_system_handler import *\n",
    "# because these files are in some random folder, I copied the important ones into the directory `helpercode`\n",
    "\n",
    "\n",
    "##\n",
    "# Variable to enable/disable the batch option in Object Detection module\n",
    "# Batching system allows to reconstruct trajectories from the object detection module by adding Re-Identification / Appareance matching.\n",
    "# For example, if an object is not seen during some time, it can be re-ID to a previous ID if the matching score is high enough\n",
    "# Use with caution if image retention is activated (See batch_system_handler.py) :\n",
    "#   --> Images will only appear if an object is detected since the batching system is based on OD detection.\n",
    "USE_BATCHING = False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "if True:\n",
    "    print(\"Running object detection ... Press 'Esc' to quit\")\n",
    "    zed = sl.Camera()\n",
    "    \n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters()\n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP  \n",
    "    init_params.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "    init_params.depth_maximum_distance = 20\n",
    "    is_playback = False                             # Defines if an SVO is used\n",
    "        \n",
    "    # If applicable, use the SVO given as parameter\n",
    "    # Otherwise use ZED live stream\n",
    "    if len(sys.argv) == 2:\n",
    "        filepath = sys.argv[1]\n",
    "        print(\"Using SVO file: {0}\".format(filepath))\n",
    "        init_params.svo_real_time_mode = True\n",
    "        init_params.set_from_svo_file(filepath)\n",
    "        is_playback = True\n",
    "\n",
    "    status = zed.open(init_params)\n",
    "    if status != sl.ERROR_CODE.SUCCESS:\n",
    "        print(repr(status))\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # Enable positional tracking module\n",
    "    positional_tracking_parameters = sl.PositionalTrackingParameters()\n",
    "    # If the camera is static in space, enabling this setting below provides better depth quality and faster computation\n",
    "    positional_tracking_parameters.set_as_static = True\n",
    "    zed.enable_positional_tracking(positional_tracking_parameters)\n",
    "\n",
    "    # Enable object detection module\n",
    "    batch_parameters = sl.BatchParameters()\n",
    "    if USE_BATCHING:\n",
    "        batch_parameters.enable = True\n",
    "        batch_parameters.latency = 2.0\n",
    "        batch_handler = BatchSystemHandler(batch_parameters.latency*2)\n",
    "    else:\n",
    "        batch_parameters.enable = False\n",
    "    obj_param = sl.ObjectDetectionParameters(batch_trajectories_parameters=batch_parameters)\n",
    "        \n",
    "    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.MULTI_CLASS_BOX_FAST\n",
    "    # Defines if the object detection will track objects across images flow.\n",
    "    obj_param.enable_tracking = True\n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    camera_infos = zed.get_camera_information()\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    point_cloud_res = sl.Resolution(min(camera_infos.camera_configuration.resolution.width, 720), min(camera_infos.camera_configuration.resolution.height, 404)) \n",
    "    point_cloud_render = sl.Mat()\n",
    "    # viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)\n",
    "    \n",
    "    # Configure object detection runtime parameters\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "    detection_confidence = 60\n",
    "    obj_runtime_param.detection_confidence_threshold = detection_confidence\n",
    "    # To select a set of specific object classes\n",
    "    obj_runtime_param.object_class_filter = [sl.OBJECT_CLASS.PERSON]\n",
    "    # To set a specific threshold\n",
    "    obj_runtime_param.object_class_detection_confidence_threshold = {sl.OBJECT_CLASS.PERSON: detection_confidence} \n",
    "\n",
    "    # Runtime parameters\n",
    "    runtime_params = sl.RuntimeParameters()\n",
    "    runtime_params.confidence_threshold = 50\n",
    "\n",
    "    # Create objects that will store SDK outputs\n",
    "    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "    objects = sl.Objects()\n",
    "    image_left = sl.Mat()\n",
    "\n",
    "    # Utilities for 2D display\n",
    "    display_resolution = sl.Resolution(min(camera_infos.camera_configuration.resolution.width, 1280), min(camera_infos.camera_configuration.resolution.height, 720))\n",
    "    image_scale = [display_resolution.width / camera_infos.camera_configuration.resolution.width\n",
    "                 , display_resolution.height / camera_infos.camera_configuration.resolution.height]\n",
    "    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239,255], np.uint8)\n",
    "\n",
    "    # Utilities for tracks view\n",
    "    camera_config = zed.get_camera_information().camera_configuration\n",
    "    tracks_resolution = sl.Resolution(400, display_resolution.height)\n",
    "    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance)\n",
    "    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)\n",
    "    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)\n",
    "\n",
    "    # Will store the 2D image and tracklet views \n",
    "    global_image = np.full((display_resolution.height, display_resolution.width+tracks_resolution.width, 4), [245, 239, 239,255], np.uint8)\n",
    "\n",
    "    # Camera pose\n",
    "    cam_w_pose = sl.Pose()\n",
    "    cam_c_pose = sl.Pose()\n",
    "\n",
    "    quit_app = False\n",
    "\n",
    "    # while(viewer.is_available() and (quit_app == False)):\n",
    "    while((quit_app == False)):\n",
    "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
    "            # Retrieve objects\n",
    "            returned_state = zed.retrieve_objects(objects, obj_runtime_param)\n",
    "            \n",
    "            if (returned_state == sl.ERROR_CODE.SUCCESS and objects.is_new):\n",
    "                # Retrieve point cloud\n",
    "                zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA,sl.MEM.CPU, point_cloud_res)\n",
    "                point_cloud.copy_to(point_cloud_render)\n",
    "                # Retrieve image\n",
    "                zed.retrieve_image(image_left, sl.VIEW.LEFT, sl.MEM.CPU, display_resolution)\n",
    "                image_render_left = image_left.get_data()\n",
    "                # Get camera pose\n",
    "                zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)\n",
    "\n",
    "                update_render_view = True\n",
    "                # update_3d_view = True\n",
    "                update_tracking_view = True\n",
    "\n",
    "                if USE_BATCHING:\n",
    "                    zed.get_position(cam_c_pose, sl.REFERENCE_FRAME.CAMERA)\n",
    "                    objects_batch = []\n",
    "                    zed.get_objects_batch(objects_batch)\n",
    "                    batch_handler.push(cam_c_pose,cam_w_pose,image_left,point_cloud,objects_batch)\n",
    "                    cam_c_pose, cam_w_pose, image_left, point_cloud_render, objects = batch_handler.pop(cam_c_pose,cam_w_pose,image_left,point_cloud,objects)\n",
    "                    \n",
    "                    image_render_left = image_left.get_data()\n",
    "                    \n",
    "                    update_tracking_view = objects.is_new\n",
    "\n",
    "                    if WITH_IMAGE_RETENTION:\n",
    "                        update_render_view = objects.is_new\n",
    "                        # update_3d_view = objects.is_new\n",
    "                    else:\n",
    "                        update_render_view = True\n",
    "                        # update_3d_view = True\n",
    "\n",
    "                # 3D rendering\n",
    "                # if update_3d_view:\n",
    "                    # viewer.updateData(point_cloud_render, objects)\n",
    "\n",
    "                # 2D rendering\n",
    "                if update_render_view:\n",
    "                    np.copyto(image_left_ocv,image_render_left)\n",
    "                    cv_viewer.render_2D(image_left_ocv,image_scale,objects, obj_param.enable_tracking)\n",
    "                    global_image = cv2.hconcat([image_left_ocv,image_track_ocv])\n",
    "\n",
    "                # Tracking view\n",
    "                if update_tracking_view:\n",
    "                    track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)\n",
    "                    \n",
    "            cv2.imshow(\"ZED | 2D View and Birds View\",global_image)\n",
    "            cv2.waitKey(10)\n",
    "\n",
    "        if (is_playback and (zed.get_svo_position() == zed.get_svo_number_of_frames()-1)):\n",
    "            print(\"End of SVO\")\n",
    "            quit_app = True\n",
    "\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    # viewer.exit()\n",
    "    image_left.free(sl.MEM.CPU)\n",
    "    point_cloud.free(sl.MEM.CPU)\n",
    "    point_cloud_render.free(sl.MEM.CPU)\n",
    "\n",
    "    if USE_BATCHING:\n",
    "        batch_handler.clear()\n",
    "\n",
    "    # Disable modules and close camera\n",
    "    zed.disable_object_detection()\n",
    "    zed.disable_positional_tracking()\n",
    "\n",
    "    zed.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About RoboFlow and Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Roboflow](https://roboflow.com/) is a platform which houses a vast variety of models, mainly for computer vision. It is often used because it is easy to train the models, and prediction happens over the cloud. For computer vision, Roboflow uses [YOLOv8](https://blog.roboflow.com/whats-new-in-yolov8/) (You Only Look Once), the fastest and most accurate architecture.\n",
    "\n",
    "Roboflow has a very intuitive process to upload training images and annotate them. For models that detect (rather than just classify), the model not only must be given the image, but also the coordinates on the image of where the object in question is, and what the object is called. Roboflow allows you to create the training data fairly easily, and can then be downloaded to be trained on device.\n",
    "\n",
    "In Version 1, the training data and the models was stored on RoboFlow, meaning anytime the code required a prediction from the model, it would make a call through the internet. This is ideal for most cases, (i.e. if the user is using a Windows laptop from 1997), however the RSS is meant to be run on an Edge computer, meaning it should have the capabilities to run the model on device. In this version, the model is trained and run on device, making it much faster.\n",
    "\n",
    "To do this, the dataset created in RoboFlow is downloaded onto the device, and a YoloV8 model is trained with this dataset.\n",
    "\n",
    "When the dataset is downloaded, it is stored in a folder (for example, `RC-Vehicle-Detection_v2-1`), which has some stuff\n",
    " - `test/`\n",
    "    - `images/`\n",
    "        - folder of the unedited images that were uploaded\n",
    "    - `labels/`\n",
    "        - txt files with the coords of where the object is and its classification\n",
    " - `train/`\n",
    "    - same subfolders as `test`\n",
    " - `valid/`\n",
    "    - same subfolders as `test`\n",
    " - `data.yaml`\n",
    "    - the main datafile, has info about where the images are, what the clssifications are, etc. This file is the only file inputted when training the model \n",
    " - `README`s\n",
    "\n",
    "The model is then trained with this data, and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BENS_RF_APIKEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# To Download the Dataset from Ben's RoboFlow project\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# !pip install roboflow\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# ^^ Don't run this line if it's already installed\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m rf \u001b[39m=\u001b[39m Roboflow(api_key\u001b[39m=\u001b[39mBENS_RF_APIKEY)\n\u001b[1;32m      7\u001b[0m project \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mworkspace(BENS_RF_WORKSPACE)\u001b[39m.\u001b[39mproject(BENS_RF_PROJECT)\n\u001b[1;32m      8\u001b[0m dataset \u001b[39m=\u001b[39m project\u001b[39m.\u001b[39mversion(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdownload(\u001b[39m\"\u001b[39m\u001b[39myolov8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BENS_RF_APIKEY' is not defined"
     ]
    }
   ],
   "source": [
    "# To Download the Dataset from Ben's RoboFlow project\n",
    "\n",
    "# !pip install roboflow\n",
    "# ^^ Don't run this line if it's already installed\n",
    "\n",
    "rf = Roboflow(api_key=BENS_RF_APIKEY)\n",
    "project = rf.workspace(BENS_RF_WORKSPACE).project(BENS_RF_PROJECT)\n",
    "dataset = project.version(1).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a custom detector\n",
    "\n",
    "# based on this tutorial: https://learnopencv.com/train-yolov8-on-custom-dataset/\n",
    "\n",
    "# Load the model.\n",
    "model = YOLO('yolov8m.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8m_rc_v2'\n",
    ")\n",
    "\n",
    "# For nano model, epochs 100, imgsize 640, it took 769 mins to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8s.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8s_rc_v3'\n",
    ")\n",
    "\n",
    "# If a model ever gets interrupted in training, you can continue it by doing\n",
    "\n",
    "# model = YOLO('runs/detect/yolov8m_rc_v2/weights/last.pt')\n",
    "# model.train(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.149 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.143 üöÄ Python-3.8.10 torch-2.0.0 CPU (ARMv8 Processor rev 1 (v8l))\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=runs/detect/yolov8m_rc_v2/weights/last.pt, data=/home/auto-id/Vikram Anantha Summer 2023/Codes/RC-Vehicle-Detection_v2-1/data.yaml, epochs=100, patience=50, batch=8, imgsz=1280, save=True, save_period=-1, cache=False, device=None, workers=0, project=None, name=yolov8m_rc_v2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/yolov8m_rc_v2\n",
      "WARNING ‚ö†Ô∏è ClearML installed but not initialized correctly, not logging this run. It seems ClearML is not configured on this machine!\n",
      "To get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\n",
      "Setup instructions can be found here: https://clear.ml/docs\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25856899 parameters, 25856883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 475/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/auto-id/Vikram Anantha Summer 2023/Codes/RC-Vehicle-Detection_v2-1/train/labels.cache... 720 images, 3 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 720/720 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/auto-id/Vikram Anantha Summer 2023/Codes/RC-Vehicle-Detection_v2-1/valid/labels.cache... 68 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/yolov8m_rc_v2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Resuming training from runs/detect/yolov8m_rc_v2/weights/last.pt from epoch 71 to 100 total epochs\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/yolov8m_rc_v2\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     71/100         0G      0.816     0.4185      1.171         17       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:37:45<00:00, 65.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:43<00:00, 20.78s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.705\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     72/100         0G     0.8149     0.4222      1.155         12       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:33:52<00:00, 62.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:42<00:00, 20.48s/it]\n",
      "                   all         68         68      0.998          1      0.995      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     73/100         0G      0.799     0.4164      1.151         13       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:34:00<00:00, 62.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:43<00:00, 20.75s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.744\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     74/100         0G     0.8029     0.4091      1.155         22       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:35:21<00:00, 63.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:44<00:00, 20.93s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.736\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     75/100         0G     0.7962     0.4102      1.138         14       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:36:27<00:00, 64.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:47<00:00, 21.57s/it]\n",
      "                   all         68         68      0.998          1      0.995      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     76/100         0G     0.7565     0.3983      1.125         13       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:37:06<00:00, 64.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:49<00:00, 21.89s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.714\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     77/100         0G      0.773     0.4121      1.113         13       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:38:18<00:00, 65.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:46<00:00, 21.33s/it]\n",
      "                   all         68         68      0.998          1      0.995      0.699\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     78/100         0G     0.7512     0.3977      1.115         18       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:39:12<00:00, 66.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:48<00:00, 21.70s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     79/100         0G     0.7496      0.397      1.118         11       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:39:58<00:00, 66.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:52<00:00, 22.46s/it]\n",
      "                   all         68         68      0.998          1      0.995      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     80/100         0G      0.729     0.3827      1.094         14       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:39:34<00:00, 66.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:50<00:00, 22.20s/it]\n",
      "                   all         68         68      0.998          1      0.995      0.706\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     81/100         0G     0.7595      0.394      1.123          9       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:38:06<00:00, 65.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:49<00:00, 21.80s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.723\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     82/100         0G     0.7428     0.3945      1.113         17       1280: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [1:37:06<00:00, 64.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:47<00:00, 21.41s/it]\n",
      "                   all         68         68      0.999          1      0.995      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     83/100         0G     0.7497      0.405      1.111         16       1280:  41%|‚ñà‚ñà‚ñà‚ñà      | 37/90 [40:38<58:13, 65.91s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Resuming interrupted model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Weekend of Aug 3 it took 6800 mins\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39mruns/detect/yolov8m_rc_v2/weights/last.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model\u001b[39m.\u001b[39;49mtrain(resume\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/model.py:376\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    377\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/trainer.py:192\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/engine/trainer.py:332\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp):\n\u001b[1;32m    331\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 332\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[1;32m    333\u001b[0m     \u001b[39mif\u001b[39;00m RANK \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    334\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:44\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mForward pass of the model on a single scale.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mWrapper for `_forward_once` method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:214\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_criterion()\n\u001b[0;32m--> 214\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch[\u001b[39m'\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39mif\u001b[39;00m preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m preds\n\u001b[1;32m    215\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 82\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:180\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    181\u001b[0m     y\u001b[39m.\u001b[39mextend(m(y[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm)\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ultralytics/nn/modules/conv.py:38\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     37\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Resuming interrupted model\n",
    "# Weekend of Aug 3 it took 6800 mins\n",
    "# Aug 7 night      it took 1232 mins\n",
    "\n",
    "model = YOLO('runs/detect/yolov8m_rc_v2/weights/last.pt')\n",
    "\n",
    "model.train(resume=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default YOLOv8 model is the nano model, which is said to be the smallest and is therefore the fastest. However, there are 2 other models that we can use: YOLOv8s (small) and YOLOv8m (medium). These models are bigger, so in theory should be more accurate but might take longer.\n",
    "\n",
    "Here is how each model performed:\n",
    "\n",
    "\n",
    "Training\n",
    "| Model Size | Epochs | Image Size | Training Time | Code Name |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Nano | 100 | 640 | 769 mins (12.8 hrs) | Nano 100x640 |\n",
    "| Medium | 71 | 1280 | 6800 mins (4.7 days) | Med 71x1280 |\n",
    "| Medium | 100 | 1280 | 6800+1232+? mins | Med 100x1280 |\n",
    "\n",
    "Testing\n",
    "| Model Size | Image Source | Time per Image | Accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| Nano 100x640 | Photo from Ben's phone   | 0.1s | ~85% |\n",
    "| Nano 100x640 | Video from my phone   | 0.3s | ~85% |\n",
    "| Nano 100x640 | Frame from ZED Camera | 0.5s | ~80% |\n",
    "| Med 71x1280 | Photo from Ben's phone   | 1s | ~85% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')\n",
    "# metrics = model.val()\n",
    "# metrics.box.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying it on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(glob.glob('RC-Vehicle-Detection_v2-1/test/images/*.jpg'),\n",
    "                save=True,\n",
    "                save_txt=True,\n",
    "                save_conf=True,\n",
    "                save_crop=True,\n",
    "                )\n",
    "# Nano: Takes 3.6 seconds to predict on 35 images, so about 0.1s per image.\n",
    "# Medium: 56s to predict 35 images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making my own score function for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realdata_path = 'RC-Vehicle-Detection_v2-1/test/labels/*.txt'\n",
    "fakedata_path = 'runs/detect/predict2/labels/*.txt'\n",
    "\n",
    "realdata_files = glob.glob(realdata_path)\n",
    "fakedata_files = glob.glob(fakedata_path)\n",
    "if (len(realdata_files) != len(fakedata_files)):\n",
    "    raise \"WrongFilePathException\"\n",
    "confs = []\n",
    "bounds_diffs = []\n",
    "for i in range(len(realdata_files)):\n",
    "    with open(realdata_files[i], 'r') as rfile:\n",
    "        robj, rxl, rxr, ryl, ryr = map(float, rfile.readline().split(\" \"))\n",
    "    with open(fakedata_files[i], 'r') as ffile:\n",
    "        fobj, fxl, fxr, fyl, fyr, fconf = map(float, ffile.readline().split(\" \"))\n",
    "        \n",
    "    confs.append(fconf)\n",
    "    bounds_diffs.append(abs(rxl-fxl)*100)\n",
    "    bounds_diffs.append(abs(rxr-fxr)*100)\n",
    "    bounds_diffs.append(abs(ryl-fyl)*100)\n",
    "    bounds_diffs.append(abs(ryr-fyr)*100)\n",
    "    \n",
    "print(\"Average Confidence:             \", avg(confs))\n",
    "print(\"Average Bounding Box Difference:\", avg(bounds_diffs), \"% of the image\")\n",
    "\n",
    "# Average Confidence:              0.8520260857142857\n",
    "# Average Bounding Box Difference: 0.6273896666666672% of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying the model on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Videos/drive_05/drive_05_vikram2023.mp4\",\n",
    "      save=True,\n",
    "      save_txt=True,\n",
    "      save_conf=True,\n",
    "      save_crop=True\n",
    "      )\n",
    "\n",
    "# 15.5s video, with save,save_txt,save_conf,save_crop:\n",
    "# 160 seconds to predict\n",
    "# 1/3 seconds to predict per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CV2 to detect RCC with on device model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_livefeed_rfdevice(foldername = 'runs/detect/predict10/'):\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    \n",
    "    filename = foldername + 'image0.jpg'\n",
    "    # Where it saves the frame\n",
    "    \n",
    "    latencies = [] # For understanding later\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read() # getting the frame\n",
    "        startbig = time.time() # to calculate the latency\n",
    "        \n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        model(\n",
    "            frame, \n",
    "            save=True, \n",
    "            verbose=False\n",
    "        )\n",
    "        # The model can save the image, but does so in some random place\n",
    "        \n",
    "        # display frame\n",
    "        boxedframe = cv2.imread(filename)\n",
    "        endbig = time.time()\n",
    "        cv2.imshow('frame', boxedframe)\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        \n",
    "        \n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        \n",
    "        save_frames(\n",
    "            image=frame,\n",
    "            dir = foldername + 'video_frames/frame_%s.jpg'\n",
    "        )\n",
    "        # To save the frames if you want to make a video\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    \n",
    "    # Turn the frames into a video\n",
    "    export_video(\n",
    "        input_val=foldername + 'video_frames/frame_*.jpg',\n",
    "        fps=4,\n",
    "        output_val=foldername + 'in_the_sun.mp4'\n",
    "    )\n",
    "    \n",
    "cv2_livefeed_rfdevice(\n",
    "    foldername = 'runs/detect/predict10/' \n",
    ")\n",
    "# ~0.52 seconds per loop\n",
    "# ~0.42 seconds of latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using ZED camera tracking code with custom detection to recognize the vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses ZED Camera's code\n",
    "# The benefit of this is that it uses the 2 cameras to create a 3D map\n",
    "# This means it can detect the distance of the object, and it can track it across the space\n",
    "# However its a bit hard to modify this code\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import cv2\n",
    "import pyzed.sl as sl\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from threading import Lock, Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "import glob\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "avg = lambda x: sum(x)/len(x)\n",
    "\n",
    "# import helpercode.custom.ogl_viewer.viewer as gl\n",
    "import helpercode.cv_viewer.tracking_viewer as cv_viewer\n",
    "# from helpercode.batch_system_handler import *\n",
    "\n",
    "lock = Lock()\n",
    "run_signal = False\n",
    "exit_signal = False\n",
    "\n",
    "\n",
    "def xywh2abcd(xywh, im_shape):\n",
    "    output = np.zeros((4, 2))\n",
    "\n",
    "    # Center / Width / Height -> BBox corners coordinates\n",
    "    x_min = (xywh[0] - 0.5*xywh[2]) #* im_shape[1]\n",
    "    x_max = (xywh[0] + 0.5*xywh[2]) #* im_shape[1]\n",
    "    y_min = (xywh[1] - 0.5*xywh[3]) #* im_shape[0]\n",
    "    y_max = (xywh[1] + 0.5*xywh[3]) #* im_shape[0]\n",
    "\n",
    "    # A ------ B\n",
    "    # | Object |\n",
    "    # D ------ C\n",
    "\n",
    "    output[0][0] = x_min\n",
    "    output[0][1] = y_min\n",
    "\n",
    "    output[1][0] = x_max\n",
    "    output[1][1] = y_min\n",
    "\n",
    "    output[2][0] = x_min\n",
    "    output[2][1] = y_max\n",
    "\n",
    "    output[3][0] = x_max\n",
    "    output[3][1] = y_max\n",
    "    return output\n",
    "\n",
    "def detections_to_custom_box(detections, im0):\n",
    "    output = []\n",
    "    for i, det in enumerate(detections):\n",
    "        xywh = det.xywh[0]\n",
    "\n",
    "        # Creating ingestable objects for the ZED SDK\n",
    "        obj = sl.CustomBoxObjectData()\n",
    "        obj.bounding_box_2d = xywh2abcd(xywh, im0.shape)\n",
    "        obj.label = det.cls\n",
    "        obj.probability = det.conf\n",
    "        obj.is_grounded = False\n",
    "        output.append(obj)\n",
    "    return output\n",
    "\n",
    "\n",
    "def torch_thread(weights, img_size, conf_thres=0.2, iou_thres=0.45):\n",
    "    global image_net, exit_signal, run_signal, detections\n",
    "\n",
    "    print(\"Intializing Network...\")\n",
    "\n",
    "    model = YOLO(weights)\n",
    "\n",
    "    while not exit_signal:\n",
    "        if run_signal:\n",
    "            lock.acquire()\n",
    "\n",
    "            img = cv2.cvtColor(image_net, cv2.COLOR_BGRA2RGB)\n",
    "            # https://docs.ultralytics.com/modes/predict/#video-suffixes\n",
    "            det = model.predict(img, save=False, imgsz=img_size, conf=conf_thres, iou=iou_thres)[0].cpu().numpy().boxes\n",
    "\n",
    "            # ZED CustomBox format (with inverse letterboxing tf applied)\n",
    "            detections = detections_to_custom_box(det, image_net)\n",
    "            lock.release()\n",
    "            run_signal = False\n",
    "        sleep(0.01)\n",
    "\n",
    "\n",
    "def main():\n",
    "    global image_net, exit_signal, run_signal, detections\n",
    "    weights = \"runs/detect/yolov8n_rc_v13/weights/best.pt\"\n",
    "    img_size = 416\n",
    "    conf_thres=0.4\n",
    "    svo = None\n",
    "    capture_thread = Thread(target=torch_thread, kwargs={\n",
    "        'weights': weights, \n",
    "        'img_size': img_size, \n",
    "        \"conf_thres\": conf_thres\n",
    "    })\n",
    "    capture_thread.start()\n",
    "    \n",
    "    latencies = []\n",
    "\n",
    "    print(\"Initializing Camera...\")\n",
    "\n",
    "    zed = sl.Camera()\n",
    "\n",
    "    input_type = sl.InputType()\n",
    "    if svo is not None:\n",
    "        input_type.set_from_svo_file(svo)\n",
    "\n",
    "    # Create a InitParameters object and set configuration parameters\n",
    "    init_params = sl.InitParameters(input_t=input_type, svo_real_time_mode=True)\n",
    "    init_params.coordinate_units = sl.UNIT.METER\n",
    "    init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # QUALITY\n",
    "    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP\n",
    "    init_params.depth_maximum_distance = 50\n",
    "\n",
    "    runtime_params = sl.RuntimeParameters()\n",
    "    status = zed.open(init_params)\n",
    "\n",
    "    if status != sl.ERROR_CODE.SUCCESS:\n",
    "        print(repr(status))\n",
    "        exit()\n",
    "\n",
    "    image_left_tmp = sl.Mat()\n",
    "\n",
    "    print(\"Initialized Camera\")\n",
    "\n",
    "    positional_tracking_parameters = sl.PositionalTrackingParameters()\n",
    "    # If the camera is static, uncomment the following line to have better performances and boxes sticked to the ground.\n",
    "    # positional_tracking_parameters.set_as_static = True\n",
    "    zed.enable_positional_tracking(positional_tracking_parameters)\n",
    "\n",
    "    obj_param = sl.ObjectDetectionParameters()\n",
    "    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.CUSTOM_BOX_OBJECTS\n",
    "    obj_param.enable_tracking = True\n",
    "    zed.enable_object_detection(obj_param)\n",
    "\n",
    "    objects = sl.Objects()\n",
    "    obj_runtime_param = sl.ObjectDetectionRuntimeParameters()\n",
    "\n",
    "    # Display\n",
    "    camera_infos = zed.get_camera_information()\n",
    "    camera_res = camera_infos.camera_configuration.resolution\n",
    "    # Create OpenGL viewer\n",
    "    # viewer = gl.GLViewer()\n",
    "    point_cloud_res = sl.Resolution(min(camera_res.width, 720), min(camera_res.height, 404))\n",
    "    point_cloud_render = sl.Mat()\n",
    "    # viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)\n",
    "    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, sl.MEM.CPU)\n",
    "    image_left = sl.Mat()\n",
    "    # Utilities for 2D display\n",
    "    display_resolution = sl.Resolution(min(camera_res.width, 1280), min(camera_res.height, 720))\n",
    "    image_scale = [display_resolution.width / camera_res.width, display_resolution.height / camera_res.height]\n",
    "    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4), [245, 239, 239, 255], np.uint8)\n",
    "\n",
    "    # Utilities for tracks view\n",
    "    camera_config = camera_infos.camera_configuration\n",
    "    tracks_resolution = sl.Resolution(400, display_resolution.height)\n",
    "    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps, init_params.depth_maximum_distance)\n",
    "    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)\n",
    "    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)\n",
    "    # Camera pose\n",
    "    cam_w_pose = sl.Pose()\n",
    "\n",
    "    # while viewer.is_available() and not exit_signal:\n",
    "    while not exit_signal:\n",
    "        if zed.grab(runtime_params) == sl.ERROR_CODE.SUCCESS:\n",
    "            # -- Get the image\n",
    "            \n",
    "            lock.acquire()\n",
    "            zed.retrieve_image(image_left_tmp, sl.VIEW.LEFT)\n",
    "            \n",
    "            image_net = image_left_tmp.get_data()\n",
    "            lock.release()\n",
    "            run_signal = True\n",
    "            startbig = time.time()\n",
    "\n",
    "            # -- Detection running on the other thread\n",
    "            while run_signal:\n",
    "                sleep(0.001)\n",
    "\n",
    "            # Wait for detections\n",
    "            lock.acquire()\n",
    "            # -- Ingest detections\n",
    "            zed.ingest_custom_box_objects(detections)\n",
    "            lock.release()\n",
    "            zed.retrieve_objects(objects, obj_runtime_param)\n",
    "\n",
    "            # -- Display\n",
    "            # Retrieve display data\n",
    "            zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA, sl.MEM.CPU, point_cloud_res)\n",
    "            point_cloud.copy_to(point_cloud_render)\n",
    "            zed.retrieve_image(image_left, sl.VIEW.LEFT, sl.MEM.CPU, display_resolution)\n",
    "            zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)\n",
    "\n",
    "            # 3D rendering\n",
    "            # viewer.updateData(point_cloud_render, objects)\n",
    "            # 2D rendering\n",
    "            np.copyto(image_left_ocv, image_left.get_data())\n",
    "            cv_viewer.render_2D(image_left_ocv, image_scale, objects, obj_param.enable_tracking)\n",
    "            global_image = cv2.hconcat([image_left_ocv, image_track_ocv])\n",
    "            # Tracking view\n",
    "            track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)\n",
    "            endbig = time.time()\n",
    "            cv2.imshow(\"ZED | 2D View and Birds View\", global_image)\n",
    "            key = cv2.waitKey(10)\n",
    "            if key == 27:\n",
    "                exit_signal = True\n",
    "                \n",
    "            \n",
    "            print(round(endbig-startbig, 2), \"seconds\")\n",
    "            latencies.append(endbig-startbig)\n",
    "        else:\n",
    "            exit_signal = True\n",
    "\n",
    "    # viewer.exit()\n",
    "    exit_signal = True\n",
    "    zed.close()\n",
    "    \n",
    "    print(round(avg(latencies), 3), \"seconds per loop\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    main()\n",
    "    \n",
    "# 0.605 seconds per loop\n",
    "# 0.605 seconds of latency\n",
    "\n",
    "\n",
    "# in hallway, max distance = 4.3 meters\n",
    "# 0.68 seconds per loop\n",
    "\n",
    "# in outside, 5.6 max partly cloudy\n",
    "# 80% outside partly cloudy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report after Jul 31 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the YOLOv8 model and have it predicting on device- the performance is pretty good. When I run it on a simple image, it takes about 0.1 seconds to predict. When I run it on a video, it takes about 0.3 seconds per frame. When I run it on live feed with CV2, it takes about 0.43 seconds of processing. However, when I run the custom detector on the ZED Camera code (which processes both cameras, and tracks the object), it takes about 0.65 seconds to process. Is this latency a good amount, or is it too much?\n",
    "\n",
    "I also tried it outside. When the car is coming towards the camera, it has about an 80% accuracy, but can identify the car pretty well, in any scenario. The car can go about 5.6 meters away before the camera stops detecting it. The algorithm was pretty good about recognizing the car in the sunlight, however when I tested it today it was about 5pm and partly cloudy so it might not be the best example- I'll check again tomorrow in more direct sunlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report After Aug 2 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got a video yesterday of how the object detection performs in sunny weather when the model is on device using YoloV8 nano model, and it isn't the worst, however it isn't the best. I found that it works best  when the lighting is consistent (everything in the frame is either sunny or shady); when there is a variety, (half shady half sunny) the camera can't decide whether to make the sunny parts too sunny or the shady parts too shady, so it does some combination of both, which doesn't make the frame that good (since these kinds of webcams don't have High Dynamic Range which most of our phone cameras do). [The video (with the confidence levels) is here](https://photos.app.goo.gl/2ZW4CrC4H61wA8sZ6)\n",
    "\n",
    "I think that this accuracy can be fixed simply by making the training data more diverse (since it was largely  just the car in a hallway, a dorm room, and outside but not that sunny), and making sure that the camera taking the photos is also diverse, or is at least the zed camera\n",
    "\n",
    "Something that I thought about while I was on my run this morning is that right now, the code runs like this:\n",
    "while true:\n",
    "   read frame from zed cam\n",
    "   do image recognition on the frame [~0.5 seconds to do]\n",
    "   display the frame\n",
    "\n",
    "This means that the frames are being read about once every half second. The way I had envisioned the blinking would be that within a span of 2 seconds, it would have blinked a considerable number of times in different durations to be able to convey a unique code. My concern with this is that since the camera is reading in each frame infrequently, it wouldn't register most of the blinks.\n",
    "\n",
    "I thought of a couple of fixes for this:\n",
    " - We could have each blink last at least half a second, to ensure the edge computer can read each time the display turns off and on. The only issue with this is that the message might take several seconds to convey, which a car wouldn't have if it is moving on the road\n",
    " - We could also try parallel processing, so that the edge computer would read each frame in separately from the processing, meaning the camera will capture every time the car headlights blink. The problem with this is that the computer has to process each of those frames, which would take about 0.5s per frame, so by the time the identity is confirmed the car is long gone\n",
    " - This could also be a problem with the model itself. I noticed that the default model Roboflow comes with has very little latency, so it isn't impossible to reach very fast prediction times, however I assume that they achieved this with a large, diverse, body of training data, and possibly fine tuning the hyperparameters of the model. It is possible to get the training data, but would take a lot of manual work. \n",
    "\n",
    "I have been trying to train the models with different parameters, but I've only been able to train it over night. I'll do a performance review of each of the models to see which one is the fastest and most accurate.\n",
    "\n",
    "I was also thinking, since I now can edit the code on Ben's display, we could try to display a QR code which links to a url hosted on a server (that could submit a POST request), which the edge computer is connected to. I figured this might be good since there should be some optimized models out there to scan QR codes, so it should help with latency, however this still requires the board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Detecting the Headlights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [this paper from Innopolis University in Russia](https://github.com/TechToker/Vehicle-rear-lights-analyser/blob/master/FinalReport.pdf), researchers intended detect if the car in front of them were braking or not. \n",
    "\n",
    "The researchers use YOLOv4 to detect whether a car is present for each frame, and then cropp the frame to just the car.\n",
    "\n",
    "To detect the presense of brakelights, they convert each image into YCrCb space, then removes all pixels that aren't pure red. They utilize the fact that the taillights are symmetrical in cars to weed away noise from other cars. They then apply a bounding box over the taillights.\n",
    "\n",
    "To determine if the car is braking, it determines the brightness in the bounding boxes. It's important to note that it has to see what the car looks like when braking vs not braking to know which is brighter, and thus be able to distinguish when it's braking.\n",
    "\n",
    "Our algorithm can utilize soemthing similar, but will have to identify the headlights a different way since headlights aren't red. Potentially, a ML model could be used to determine the headlights' position. Determining when its on vs off would be the same as the Innopolis paper.\n",
    "\n",
    "\n",
    "\n",
    "[This article](https://ieeexplore.ieee.org/abstract/document/6761567) talks about how the researchers detected headlights to track vehicles. They first converted the image to a binary image (meaning grayscaled, then converted everything above a certain threshold to 255 and below that threshold to 0), and detected the blobs. The then detected the blobs of white areas, and considered each blob a candidate for one of the pair of headlights. Finally, they used the property that headlights are symmetrical to pair 2 of the blobs and determine if they are symmetrical (using some equations).\n",
    "From this, they applied it to counting and tracking vehicles in night time / low light. It remains to be seen whether this technique would work in the daytime (since converting the image to binary might not work as well / produce more blobs in higher light), but a method like this could work on the prototype car (since it has the red display board)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting headlights method 1: Using a dataset from RoboFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report From Aug 1 2023 (kind of just a flood of random thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps that need to be done now.\n",
    "\n",
    "I need to find a good way to detect headlights.\n",
    "There is a headlights ML algorithm that I can train it on\n",
    "That needs to be done over night\n",
    "\n",
    "I also would need to make the headlights work on Ben's display\n",
    "I need Ben's arduino code to make that happen\n",
    "\n",
    "I also need to find a way to decode the headlights\n",
    "I need the display to make that work?\n",
    "\n",
    "Overnight I should train the algorithms\n",
    "Then tmr I should test to see if it works in the day\n",
    "Then find the intensity values\n",
    "\n",
    "Also take a picture of actual car headluights, play around with those in different formats (Binary, YCrCb, etc), also needs to be done at home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "# Downloading a headlights detection dataset\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"2Vs9PCO5LGDCSkI0huRq\")\n",
    "project = rf.workspace(\"nours-g8unb\").project(\"vehicles_night\")\n",
    "dataset = project.version(3).download(\"yolov8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training a custom detector\n",
    "\n",
    "# based on this tutorial: https://learnopencv.com/train-yolov8-on-custom-dataset/\n",
    "\n",
    "\n",
    " \n",
    "# Load the model.\n",
    "model = YOLO('yolov8n.pt')\n",
    " \n",
    "# # Training.\n",
    "results = model.train(\n",
    "   data=f'{dataset.location}/data.yaml',\n",
    "   imgsz=1280,\n",
    "   epochs=100,\n",
    "   batch=8,\n",
    "   name='yolov8n_headlights_v1'\n",
    ")\n",
    "\n",
    "# Took 1258 mins for imgsz=1280, epochs 100, nano model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reopen the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_headlights_v1/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(glob.glob('Videos/headlight_test/*.jpg'),\n",
    "                save=True,\n",
    "                # save_txt=True,\n",
    "                save_conf=True,\n",
    "                # save_crop=True,\n",
    "                )\n",
    "# Takes 5.9 seconds to predict on 6 images, so about 1s per image.\n",
    "\n",
    "# Accuracy really bad\n",
    "# Scrap this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Don't detect headlights, detect flashes of light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Aug 7 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Detect headlights, one approach is to detect the vehicle like normal, and simply detect when the vehicle's lights flash by detecting when the vehicle region becomes bright. This way, you don't have to recognize where a vehicle's headlights are, you can instead act like a human and only notice when the lights flash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple ways to detect a blinking light are the following\n",
    " - Take the average brightness of the cropped image, compare them frame by frame, and see when the biggest different is, and that is when it blinks ([explained here](https://stackoverflow.com/questions/38030170/best-way-to-detect-a-light-blinking-from-video-footage-via-python))\n",
    "    - You could do this on the normal cropped image, a greyscaled version, or a thresholded version\n",
    "    - This has problems in that it might be influenced by other factors, like shade, or the vehicle moving\n",
    "    - For my testing, it does work, however I set the threshold to 200. When the headlights blinked, the mean image value was 11, and otherwise it was 0.5\n",
    " - Take the difference of 2 consecutive full frames ([explained here](https://stackoverflow.com/questions/8877228/opencv-detect-blinking-lights)) to detect when the light blinks\n",
    "    - This should only be done with thresholded images (otherwise it causes seizures)\n",
    "    - This won't work because the vehicle moves so it will capture the difference between the vehicle's last positions, not the headlights, [as shown here](https://photos.app.goo.gl/CMKZ16492xYHcSuj6)\n",
    " - Take the difference of 2 consecutive _cropped_ frames to detect when the light blinks\n",
    "    - This won't work because the cropped frames have very slightly different dimensions, and it would be hard to align them as it would get the vehicle change, not the headlights\n",
    "\n",
    "I did the first option because it's the only one that can actually work. I did this by detecting the vehicle, cropping the vehicle to just that, converting the cropped image to only black or white (I call it thresholding the image, [here is a video where I try a bunch of different thresholds](https://photos.app.goo.gl/YyxE9YHTYNSLU2sCA)), and finding the average light value of the cropped image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open and run the RC detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/detect/yolov8n_rc_v13/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(['Videos/PXL_20230807_143124752.MP.jpg', 'Videos/PXL_20230807_143126993.MP.jpg'],\n",
    "                save=True,\n",
    "                # save_txt=True,\n",
    "                save_conf=True,\n",
    "                save_crop=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking frames, detecting the RC, Thresholding them to detect blinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/462, mean: 219.6951171875\n",
      " 2/462, mean: 204.07083333333333\n",
      " 3/462, mean: 180.45677083333334\n",
      " 4/462, mean: 178.7586154513889\n",
      " 5/462, mean: 161.6962673611111\n",
      " 6/462, mean: 149.2085720486111\n",
      " 7/462, mean: 125.78610387731482\n",
      " 8/462, mean: 79.22142650462963\n",
      " 9/462, mean: 114.9827907986111\n",
      " 10/462, mean: 131.90654658564816\n",
      " 11/462, mean: 117.90983072916667\n",
      " 12/462, mean: 130.315625\n",
      " 13/462, mean: 122.4687427662037\n",
      " 14/462, mean: 115.05559172453704\n",
      " 15/462, mean: 113.75673466435185\n",
      " 16/462, mean: 113.71258680555556\n",
      " 17/462, mean: 117.84908130787036\n",
      " 18/462, mean: 118.99704861111111\n",
      " 19/462, mean: 120.36489438657408\n",
      " 20/462, mean: 120.2925853587963\n",
      " 21/462, mean: 116.90353009259259\n",
      " 22/462, mean: 115.37643229166666\n",
      " 23/462, mean: 125.9232204861111\n",
      " 24/462, mean: 120.98640769675926\n",
      " 25/462, mean: 119.6740234375\n",
      " 26/462, mean: 113.93394097222222\n",
      " 27/462, mean: 113.22216435185184\n",
      " 28/462, mean: 110.79341724537036\n",
      " 29/462, mean: 106.70672743055556\n",
      " 30/462, mean: 108.84562355324074\n",
      " 31/462, mean: 115.48391203703704\n",
      " 32/462, mean: 115.6882957175926\n",
      " 33/462, mean: 115.4459129050926\n",
      " 34/462, mean: 115.58278356481482\n",
      " 35/462, mean: 115.59471209490741\n",
      " 36/462, mean: 115.66726707175926\n",
      " 37/462, mean: 117.904296875\n",
      " 38/462, mean: 116.66188512731482\n",
      " 39/462, mean: 118.30211950231481\n",
      " 40/462, mean: 119.87545572916666\n",
      " 41/462, mean: 121.14442997685185\n",
      " 42/462, mean: 119.10366753472222\n",
      " 43/462, mean: 116.2611111111111\n",
      " 44/462, mean: 114.76463396990741\n",
      " 45/462, mean: 114.71126302083333\n",
      " 46/462, mean: 114.5165943287037\n",
      " 47/462, mean: 112.90255353009259\n",
      " 48/462, mean: 112.18462818287037\n",
      " 49/462, mean: 118.60943287037037\n",
      " 50/462, mean: 125.52970196759259\n",
      " 51/462, mean: 122.30653935185185\n",
      " 52/462, mean: 112.72866753472222\n",
      " 53/462, mean: 116.00802951388889\n",
      " 54/462, mean: 120.8560546875\n",
      " 55/462, mean: 125.53585069444445\n",
      " 56/462, mean: 126.82991174768519\n",
      " 57/462, mean: 126.81859809027777\n",
      " 58/462, mean: 126.84073350694445\n",
      " 59/462, mean: 126.85032552083334\n",
      " 60/462, mean: 127.84088541666667\n",
      " 61/462, mean: 127.31344762731482\n",
      " 62/462, mean: 126.88734085648149\n",
      " 63/462, mean: 125.69534866898148\n",
      " 64/462, mean: 126.95067274305555\n",
      " 65/462, mean: 127.40678530092593\n",
      " 66/462, mean: 128.3974681712963\n",
      " 67/462, mean: 128.1431568287037\n",
      " 68/462, mean: 128.4878544560185\n",
      " 69/462, mean: 128.45305266203704\n",
      " 70/462, mean: 126.70128038194444\n",
      " 71/462, mean: 127.0517578125\n",
      " 72/462, mean: 126.5736328125\n",
      " 73/462, mean: 126.70656828703704\n",
      " 74/462, mean: 125.99921875\n",
      " 75/462, mean: 123.17744502314815\n",
      " 76/462, mean: 118.99704861111111\n",
      " 77/462, mean: 111.03825954861111\n",
      " 78/462, mean: 96.03093894675926\n",
      " 79/462, mean: 129.04603587962964\n",
      " 80/462, mean: 129.52219328703703\n",
      " 81/462, mean: 133.96341869212964\n",
      " 82/462, mean: 129.57433449074074\n",
      " 83/462, mean: 128.6939597800926\n",
      " 84/462, mean: 125.7900390625\n",
      " 85/462, mean: 126.40208333333334\n",
      " 86/462, mean: 127.75640190972223\n",
      " 87/462, mean: 131.2070674189815\n",
      " 88/462, mean: 129.5552734375\n",
      " 89/462, mean: 122.8658275462963\n",
      " 90/462, mean: 119.13478009259259\n",
      " 91/462, mean: 123.29193431712963\n",
      " 92/462, mean: 115.18569878472222\n",
      " 93/462, mean: 118.11630497685185\n",
      " 94/462, mean: 107.90708188657408\n",
      " 95/462, mean: 113.32743055555555\n",
      " 96/462, mean: 110.13304398148148\n",
      " 97/462, mean: 111.5548755787037\n",
      " 98/462, mean: 104.2568287037037\n",
      " 99/462, mean: 102.0839916087963\n",
      " 100/462, mean: 102.98367332175926\n",
      " 101/462, mean: 103.40891927083334\n",
      " 102/462, mean: 103.05536747685186\n",
      " 103/462, mean: 97.52962962962962\n",
      " 104/462, mean: 98.0398509837963\n",
      " 105/462, mean: 97.15935329861111\n",
      " 106/462, mean: 97.83620515046296\n",
      " 107/462, mean: 97.85268373842592\n",
      " 108/462, mean: 97.75823929398148\n",
      " 109/462, mean: 97.8585865162037\n",
      " 110/462, mean: 97.2349826388889\n",
      " 111/462, mean: 97.16869936342593\n",
      " 112/462, mean: 97.21665943287037\n",
      " 113/462, mean: 97.40444155092592\n",
      " 114/462, mean: 100.11652199074074\n",
      " 115/462, mean: 101.40185185185184\n",
      " 116/462, mean: 100.30873119212963\n",
      " 117/462, mean: 102.27718460648148\n",
      " 118/462, mean: 102.69369936342592\n",
      " 119/462, mean: 106.2818504050926\n",
      " 120/462, mean: 108.27391493055555\n",
      " 121/462, mean: 107.98357204861111\n",
      " 122/462, mean: 107.95934606481481\n",
      " 123/462, mean: 102.26205873842592\n",
      " 124/462, mean: 99.24881365740741\n",
      " 125/462, mean: 95.48333333333333\n",
      " 126/462, mean: 104.59095052083333\n",
      " 127/462, mean: 102.94284577546296\n",
      " 128/462, mean: 102.81999421296297\n",
      " 129/462, mean: 103.21314380787037\n",
      " 130/462, mean: 111.91260850694445\n",
      " 131/462, mean: 102.84422019675925\n",
      " 132/462, mean: 104.3052806712963\n",
      " 133/462, mean: 103.7843605324074\n",
      " 134/462, mean: 101.98401331018519\n",
      " 135/462, mean: 101.6599754050926\n",
      " 136/462, mean: 101.89338107638889\n",
      " 137/462, mean: 102.18101851851851\n",
      " 138/462, mean: 102.26144386574074\n",
      " 139/462, mean: 103.31324508101852\n",
      " 140/462, mean: 103.04675925925926\n",
      " 141/462, mean: 102.76650028935185\n",
      " 142/462, mean: 102.6165943287037\n",
      " 143/462, mean: 102.02176649305555\n",
      " 144/462, mean: 101.2705150462963\n",
      " 145/462, mean: 99.49144241898148\n",
      " 146/462, mean: 101.79327980324074\n",
      " 147/462, mean: 102.47320601851852\n",
      " 148/462, mean: 102.47738715277778\n",
      " 149/462, mean: 102.28763744212964\n",
      " 150/462, mean: 102.2669777199074\n",
      " 151/462, mean: 102.20475260416667\n",
      " 152/462, mean: 99.5904369212963\n",
      " 153/462, mean: 99.62978877314815\n",
      " 154/462, mean: 101.1234375\n",
      " 155/462, mean: 101.62541956018518\n",
      " 156/462, mean: 102.13232060185184\n",
      " 157/462, mean: 101.95954137731482\n",
      " 158/462, mean: 102.63799189814814\n",
      " 159/462, mean: 102.75985966435185\n",
      " 160/462, mean: 102.4644748263889\n",
      " 161/462, mean: 103.06077835648148\n",
      " 162/462, mean: 102.58043981481481\n",
      " 163/462, mean: 102.63147424768519\n",
      " 164/462, mean: 102.5388744212963\n",
      " 165/462, mean: 102.47000868055555\n",
      " 166/462, mean: 102.40458622685185\n",
      " 167/462, mean: 102.49681712962963\n",
      " 168/462, mean: 102.33362991898149\n",
      " 169/462, mean: 102.32649739583333\n",
      " 170/462, mean: 102.23598813657408\n",
      " 171/462, mean: 102.31911892361111\n",
      " 172/462, mean: 102.43803530092593\n",
      " 173/462, mean: 102.48870081018518\n",
      " 174/462, mean: 102.31788917824075\n",
      " 175/462, mean: 102.11682581018519\n",
      " 176/462, mean: 102.2072120949074\n",
      " 177/462, mean: 101.59381510416667\n",
      " 178/462, mean: 101.65985243055556\n",
      " 179/462, mean: 102.25935329861112\n",
      " 180/462, mean: 100.77664930555555\n",
      " 181/462, mean: 99.3427662037037\n",
      " 182/462, mean: 98.5980324074074\n",
      " 183/462, mean: 102.03972077546297\n",
      " 184/462, mean: 102.30325520833334\n",
      " 185/462, mean: 100.7724681712963\n",
      " 186/462, mean: 98.22234519675926\n",
      " 187/462, mean: 92.45926649305555\n",
      " 188/462, mean: 61.18954716435185\n",
      " 189/462, mean: 113.57510127314815\n",
      " 190/462, mean: 130.00203993055555\n",
      " 191/462, mean: 90.7497974537037\n",
      " 192/462, mean: 85.5797019675926\n",
      " 193/462, mean: 100.28425925925926\n",
      " 194/462, mean: 102.2314380787037\n",
      " 195/462, mean: 82.98420138888889\n",
      " 196/462, mean: 80.44834346064815\n",
      " 197/462, mean: 78.99453848379629\n",
      " 198/462, mean: 76.93508391203704\n",
      " 199/462, mean: 76.4677806712963\n",
      " 200/462, mean: 69.61047453703704\n",
      " 201/462, mean: 77.2361255787037\n",
      " 202/462, mean: 77.87866753472223\n",
      " 203/462, mean: 76.49508101851852\n",
      " 204/462, mean: 76.82686631944445\n",
      " 205/462, mean: 76.08545283564816\n",
      " 206/462, mean: 75.74567418981482\n",
      " 207/462, mean: 75.44389467592593\n",
      " 208/462, mean: 75.37810329861111\n",
      " 209/462, mean: 75.37478298611111\n",
      " 210/462, mean: 75.69796006944445\n",
      " 211/462, mean: 76.64449508101852\n",
      " 212/462, mean: 76.87667100694445\n",
      " 213/462, mean: 76.95439091435185\n",
      " 214/462, mean: 77.50593171296296\n",
      " 215/462, mean: 77.37336516203703\n",
      " 216/462, mean: 76.30028935185184\n",
      " 217/462, mean: 77.5326171875\n",
      " 218/462, mean: 78.04099392361111\n",
      " 219/462, mean: 77.4615379050926\n",
      " 220/462, mean: 77.50187355324074\n",
      " 221/462, mean: 77.49117476851852\n",
      " 222/462, mean: 76.04733072916666\n",
      " 223/462, mean: 76.36780237268519\n",
      " 224/462, mean: 76.37099971064815\n",
      " 225/462, mean: 76.21555989583334\n",
      " 226/462, mean: 76.40432581018518\n",
      " 227/462, mean: 76.68446180555556\n",
      " 228/462, mean: 75.75698784722222\n",
      " 229/462, mean: 74.78671875\n",
      " 230/462, mean: 74.99958767361112\n",
      " 231/462, mean: 79.44278067129629\n",
      " 232/462, mean: 79.9029513888889\n",
      " 233/462, mean: 79.3060329861111\n",
      " 234/462, mean: 80.0869212962963\n",
      " 235/462, mean: 80.05740740740741\n",
      " 236/462, mean: 80.24236111111111\n",
      " 237/462, mean: 80.88822337962964\n",
      " 238/462, mean: 81.04415509259259\n",
      " 239/462, mean: 80.77717737268519\n",
      " 240/462, mean: 82.02487702546296\n",
      " 241/462, mean: 83.68232783564815\n",
      " 242/462, mean: 83.92630931712964\n",
      " 243/462, mean: 85.56297743055555\n",
      " 244/462, mean: 85.15912905092593\n",
      " 245/462, mean: 84.36225405092593\n",
      " 246/462, mean: 84.23177806712962\n",
      " 247/462, mean: 83.54127604166666\n",
      " 248/462, mean: 81.9283420138889\n",
      " 249/462, mean: 82.20737123842592\n",
      " 250/462, mean: 81.92366898148148\n",
      " 251/462, mean: 82.46586371527778\n",
      " 252/462, mean: 81.57503616898148\n",
      " 253/462, mean: 80.6257957175926\n",
      " 254/462, mean: 80.40358072916666\n",
      " 255/462, mean: 82.33821614583333\n",
      " 256/462, mean: 81.8775535300926\n",
      " 257/462, mean: 81.50555555555556\n",
      " 258/462, mean: 81.10539641203704\n",
      " 259/462, mean: 80.344921875\n",
      " 260/462, mean: 79.92447193287038\n",
      " 261/462, mean: 75.93075086805555\n",
      " 262/462, mean: 79.09267216435185\n",
      " 263/462, mean: 80.93507667824075\n",
      " 264/462, mean: 84.94773582175925\n",
      " 265/462, mean: 85.68509114583334\n",
      " 266/462, mean: 87.05834780092593\n",
      " 267/462, mean: 86.9841941550926\n",
      " 268/462, mean: 81.7185474537037\n",
      " 269/462, mean: 82.68598813657407\n",
      " 270/462, mean: 82.99047309027777\n",
      " 271/462, mean: 83.22904369212964\n",
      " 272/462, mean: 78.45381944444445\n",
      " 273/462, mean: 81.42648292824074\n",
      " 274/462, mean: 77.93818721064815\n",
      " 275/462, mean: 81.51158130787037\n",
      " 276/462, mean: 82.10604021990741\n",
      " 277/462, mean: 81.9021484375\n",
      " 278/462, mean: 82.88250144675926\n",
      " 279/462, mean: 82.10001446759259\n",
      " 280/462, mean: 83.66449652777777\n",
      " 281/462, mean: 82.49144241898148\n",
      " 282/462, mean: 83.42789351851852\n",
      " 283/462, mean: 83.88289930555555\n",
      " 284/462, mean: 84.53318865740741\n",
      " 285/462, mean: 83.88757233796296\n",
      " 286/462, mean: 83.21932870370371\n",
      " 287/462, mean: 83.0552806712963\n",
      " 288/462, mean: 83.00375434027778\n",
      " 289/462, mean: 83.10631510416667\n",
      " 290/462, mean: 83.22412471064816\n",
      " 291/462, mean: 82.46979890046296\n",
      " 292/462, mean: 82.66987847222222\n",
      " 293/462, mean: 82.58650173611112\n",
      " 294/462, mean: 82.38420862268518\n",
      " 295/462, mean: 82.07394386574074\n",
      " 296/462, mean: 81.85480324074074\n",
      " 297/462, mean: 81.88480902777778\n",
      " 298/462, mean: 81.5103515625\n",
      " 299/462, mean: 81.45206163194445\n",
      " 300/462, mean: 81.3884837962963\n",
      " 301/462, mean: 81.55966435185185\n",
      " 302/462, mean: 81.29280960648148\n",
      " 303/462, mean: 80.71286168981482\n",
      " 304/462, mean: 81.83746383101852\n",
      " 305/462, mean: 82.04233940972222\n",
      " 306/462, mean: 82.74489293981482\n",
      " 307/462, mean: 81.53457754629629\n",
      " 308/462, mean: 81.67710503472222\n",
      " 309/462, mean: 81.04132667824074\n",
      " 310/462, mean: 81.08153935185184\n",
      " 311/462, mean: 81.07895688657408\n",
      " 312/462, mean: 80.96827980324075\n",
      " 313/462, mean: 81.01095196759259\n",
      " 314/462, mean: 80.61263744212962\n",
      " 315/462, mean: 80.92843605324074\n",
      " 316/462, mean: 81.03271846064816\n",
      " 317/462, mean: 81.6972728587963\n",
      " 318/462, mean: 82.48922887731482\n",
      " 319/462, mean: 82.3095630787037\n",
      " 320/462, mean: 82.74698350694445\n",
      " 321/462, mean: 81.10441261574074\n",
      " 322/462, mean: 82.43057002314815\n",
      " 323/462, mean: 81.11093026620371\n",
      " 324/462, mean: 84.7752025462963\n",
      " 325/462, mean: 85.03209635416667\n",
      " 326/462, mean: 83.8900318287037\n",
      " 327/462, mean: 83.52455150462963\n",
      " 328/462, mean: 83.21674623842593\n",
      " 329/462, mean: 82.93931568287037\n",
      " 330/462, mean: 82.31030092592593\n",
      " 331/462, mean: 80.81161024305555\n",
      " 332/462, mean: 83.4736400462963\n",
      " 333/462, mean: 79.38510561342592\n",
      " 334/462, mean: 82.34227430555555\n",
      " 335/462, mean: 81.7849537037037\n",
      " 336/462, mean: 80.97615017361112\n",
      " 337/462, mean: 80.88650173611111\n",
      " 338/462, mean: 80.66096643518519\n",
      " 339/462, mean: 80.46457609953704\n",
      " 340/462, mean: 80.25379774305556\n",
      " 341/462, mean: 81.42390046296296\n",
      " 342/462, mean: 80.2263744212963\n",
      " 343/462, mean: 79.35018084490741\n",
      " 344/462, mean: 79.47856626157407\n",
      " 345/462, mean: 79.44278067129629\n",
      " 346/462, mean: 79.46331741898148\n",
      " 347/462, mean: 79.71541521990741\n",
      " 348/462, mean: 80.8652271412037\n",
      " 349/462, mean: 80.52212818287038\n",
      " 350/462, mean: 82.4277416087963\n",
      " 351/462, mean: 81.0236183449074\n",
      " 352/462, mean: 81.77474681712962\n",
      " 353/462, mean: 86.71549479166667\n",
      " 354/462, mean: 86.47298900462962\n",
      " 355/462, mean: 87.20985243055556\n",
      " 356/462, mean: 88.27419704861111\n",
      " 357/462, mean: 88.32879774305556\n",
      " 358/462, mean: 90.48589409722223\n",
      " 359/462, mean: 88.85734230324074\n",
      " 360/462, mean: 89.34149305555556\n",
      " 361/462, mean: 88.89915364583334\n",
      " 362/462, mean: 89.28824508101852\n",
      " 363/462, mean: 88.29830005787036\n",
      " 364/462, mean: 87.97586082175926\n",
      " 365/462, mean: 87.84513888888888\n",
      " 366/462, mean: 87.43169849537037\n",
      " 367/462, mean: 87.73802806712963\n",
      " 368/462, mean: 87.47670717592592\n",
      " 369/462, mean: 86.83293547453704\n",
      " 370/462, mean: 87.14934895833333\n",
      " 371/462, mean: 86.36489438657408\n",
      " 372/462, mean: 86.75287905092593\n",
      " 373/462, mean: 86.23171296296296\n",
      " 374/462, mean: 85.55805844907407\n",
      " 375/462, mean: 86.53312355324074\n",
      " 376/462, mean: 85.91566840277778\n",
      " 377/462, mean: 90.01969762731481\n",
      " 378/462, mean: 91.38041087962964\n",
      " 379/462, mean: 92.58433159722222\n",
      " 380/462, mean: 93.67425491898148\n",
      " 381/462, mean: 82.35580150462962\n",
      " 382/462, mean: 113.06106770833334\n",
      " 383/462, mean: 112.40130931712963\n",
      " 384/462, mean: 107.13135850694445\n",
      " 385/462, mean: 79.22511574074075\n",
      " 386/462, mean: 145.40951967592594\n",
      " 387/462, mean: 181.2163845486111\n",
      " 388/462, mean: 143.656640625\n",
      " 389/462, mean: 127.43211805555555\n",
      " 390/462, mean: 125.6671875\n",
      " 391/462, mean: 121.20517939814815\n",
      " 392/462, mean: 121.45420283564815\n",
      " 393/462, mean: 122.06698495370371\n",
      " 394/462, mean: 122.79573206018519\n",
      " 395/462, mean: 123.03676215277778\n",
      " 396/462, mean: 122.3392505787037\n",
      " 397/462, mean: 122.42471788194445\n",
      " 398/462, mean: 120.38297164351852\n",
      " 399/462, mean: 122.21959635416667\n",
      " 400/462, mean: 120.77144820601852\n",
      " 401/462, mean: 122.24320746527778\n",
      " 402/462, mean: 122.76351273148148\n",
      " 403/462, mean: 126.5201388888889\n",
      " 404/462, mean: 128.6199291087963\n",
      " 405/462, mean: 128.06039496527777\n",
      " 406/462, mean: 127.7334056712963\n",
      " 407/462, mean: 128.3384403935185\n",
      " 408/462, mean: 128.08917100694444\n",
      " 409/462, mean: 128.1814019097222\n",
      " 410/462, mean: 128.40103443287038\n",
      " 411/462, mean: 128.66948784722223\n",
      " 412/462, mean: 128.68793402777777\n",
      " 413/462, mean: 128.64956597222223\n",
      " 414/462, mean: 128.6129195601852\n",
      " 415/462, mean: 127.918359375\n",
      " 416/462, mean: 120.40559895833333\n",
      " 417/462, mean: 114.23621238425926\n",
      " 418/462, mean: 114.61448206018518\n",
      " 419/462, mean: 119.50185908564815\n",
      " 420/462, mean: 124.34951533564815\n",
      " 421/462, mean: 126.61802662037037\n",
      " 422/462, mean: 126.84946469907408\n",
      " 423/462, mean: 127.56296296296296\n",
      " 424/462, mean: 127.80325520833334\n",
      " 425/462, mean: 127.70130931712963\n",
      " 426/462, mean: 127.53578559027778\n",
      " 427/462, mean: 127.5673900462963\n",
      " 428/462, mean: 127.55152633101852\n",
      " 429/462, mean: 127.51180555555555\n",
      " 430/462, mean: 127.3751808449074\n",
      " 431/462, mean: 127.46323061342592\n",
      " 432/462, mean: 127.11336805555555\n",
      " 433/462, mean: 127.12086950231482\n",
      " 434/462, mean: 127.07241753472222\n",
      " 435/462, mean: 127.07647569444444\n",
      " 436/462, mean: 126.88303674768518\n",
      " 437/462, mean: 126.8880787037037\n",
      " 438/462, mean: 126.98153935185185\n",
      " 439/462, mean: 126.67213541666666\n",
      " 440/462, mean: 127.35931712962963\n",
      " 441/462, mean: 127.14607928240741\n",
      " 442/462, mean: 126.94157262731481\n",
      " 443/462, mean: 127.04339554398148\n",
      " 444/462, mean: 126.79695457175926\n",
      " 445/462, mean: 126.84503761574074\n",
      " 446/462, mean: 126.85229311342593\n",
      " 447/462, mean: 126.97957175925926\n",
      " 448/462, mean: 125.72055844907408\n",
      " 449/462, mean: 124.053515625\n",
      " 450/462, mean: 121.50068721064815\n",
      " 451/462, mean: 114.63046875\n",
      " 452/462, mean: 110.18985821759259\n",
      " 453/462, mean: 120.90204716435186\n",
      " 454/462, mean: 129.73678385416667\n",
      " 455/462, mean: 129.7255931712963\n",
      " 456/462, mean: 129.51616753472223\n",
      " 457/462, mean: 129.6465205439815\n",
      " 458/462, mean: 129.6359447337963\n",
      " 459/462, mean: 129.63582175925927\n",
      " 460/462, mean: 129.1923755787037\n",
      " 461/462, mean: 122.02455873842592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/aarch64-linux-gnu --incdir=/usr/include/aarch64-linux-gnu --arch=arm64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from 'Videos/aug72023vids/diff_frames*.jpg':\n",
      "  Duration: 00:00:28.81, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: mjpeg (Baseline), gray(bt470bg/unknown/unknown), 1920x1080 [SAR 1:1 DAR 16:9], 16 fps, 16 tbr, 16 tbn, 16 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0xaaaac3bfdf40] using SAR=1/1\n",
      "[libx264 @ 0xaaaac3bfdf40] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0xaaaac3bfdf40] profile High, level 4.0\n",
      "[libx264 @ 0xaaaac3bfdf40] 264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=18 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'Videos/aug72023vids/frame_differences.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], q=-1--1, 30 fps, 15360 tbn, 30 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  865 fps= 53 q=-1.0 Lsize=   43357kB time=00:00:28.73 bitrate=12361.2kbits/s dup=404 drop=0 speed=1.76x    \n",
      "video:43347kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.023399%\n",
      "[libx264 @ 0xaaaac3bfdf40] frame I:127   Avg QP:23.04  size:123037\n",
      "[libx264 @ 0xaaaac3bfdf40] frame P:276   Avg QP:28.84  size: 56045\n",
      "[libx264 @ 0xaaaac3bfdf40] frame B:462   Avg QP:30.33  size: 28771\n",
      "[libx264 @ 0xaaaac3bfdf40] consecutive B-frames: 26.7%  4.2%  6.2% 62.9%\n",
      "[libx264 @ 0xaaaac3bfdf40] mb I  I16..4:  9.9% 51.9% 38.2%\n",
      "[libx264 @ 0xaaaac3bfdf40] mb P  I16..4:  1.6%  7.3% 15.2%  P16..4:  6.7%  3.4%  2.1%  0.0%  0.0%    skip:63.7%\n",
      "[libx264 @ 0xaaaac3bfdf40] mb B  I16..4:  1.6%  2.6%  2.9%  B16..8: 22.4%  3.6%  1.5%  direct: 0.7%  skip:64.8%  L0:53.9% L1:45.4% BI: 0.7%\n",
      "[libx264 @ 0xaaaac3bfdf40] 8x8 transform intra:43.3% inter:4.2%\n",
      "[libx264 @ 0xaaaac3bfdf40] coded y,uvDC,uvAC intra: 30.1% 0.0% 0.0% inter: 4.2% 0.0% 0.0%\n",
      "[libx264 @ 0xaaaac3bfdf40] i16 v,h,dc,p: 56% 32% 11%  1%\n",
      "[libx264 @ 0xaaaac3bfdf40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 64%  8% 28%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0xaaaac3bfdf40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 43% 18% 26%  2%  2%  2%  3%  1%  3%\n",
      "[libx264 @ 0xaaaac3bfdf40] i8c dc,h,v,p: 100%  0%  0%  0%\n",
      "[libx264 @ 0xaaaac3bfdf40] Weighted P-Frames: Y:0.7% UV:0.0%\n",
      "[libx264 @ 0xaaaac3bfdf40] ref P L0: 54.0%  3.3% 25.3% 17.3%\n",
      "[libx264 @ 0xaaaac3bfdf40] ref B L0: 70.0% 19.7% 10.4%\n",
      "[libx264 @ 0xaaaac3bfdf40] ref B L1: 95.8%  4.2%\n",
      "[libx264 @ 0xaaaac3bfdf40] kb/s:12315.29\n"
     ]
    }
   ],
   "source": [
    "blinkinglights = sorted(glob.glob(\n",
    "    # \"runs/detect/predict10/crops/RC-Car/*.jpg\"\n",
    "    \"runs/detect/predict5/video_frames/*.jpg\"\n",
    "))\n",
    "\n",
    "for framefileind in range(1, len(blinkinglights)):\n",
    "    framefile = blinkinglights[framefileind]\n",
    "    lastframefile = blinkinglights[framefileind-1]\n",
    "    threshold = 100\n",
    "    # frame = cv2.imread(framefile)\n",
    "    # frame = cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY)\n",
    "    _, frame = cv2.threshold(cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "    # lastframe = cv2.imread(lastframefile)\n",
    "    # lastframe = cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY)\n",
    "    _, lastframe = cv2.threshold(cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "    framediff = frame-lastframe\n",
    "    \n",
    "    cv2.imwrite('Videos/aug72023vids/diff_frames%04d.jpg' % framefileind, framediff)\n",
    "    cv2.imshow('diff frame', framediff)\n",
    "    # cv2.imwrite('Videos/aug72023vids/cropped_vehicle_blinking_normal%04d.jpg' % framefileind, frame)\n",
    "    # cv2.imshow('frame', frame)\n",
    "    print(f' {framefileind}/{len(blinkinglights)}, mean: {frame.mean()}')\n",
    "    # frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # threshold = 70\n",
    "\n",
    "    # # Apply threshold to the masked grayscale image\n",
    "    # _, thresh = cv2.threshold(frame_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # cv2.imshow('frame', thresh)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "export_video(\n",
    "    input_val='Videos/aug72023vids/diff_frames*.jpg',\n",
    "    fps=16,\n",
    "    output_val='Videos/aug72023vids/frame_differences.mp4'\n",
    ")\n",
    "    \n",
    "# for threshold in range(1, 256):\n",
    "#     framefile = blinkinglights[0]\n",
    "#     # lastframefile = blinkinglights[framefileind-1]\n",
    "    \n",
    "#     # frame = cv2.imread(framefile)\n",
    "#     # frame = cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY)\n",
    "#     _, frame = cv2.threshold(cv2.cvtColor(cv2.imread(framefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "#     # lastframe = cv2.imread(lastframefile)\n",
    "#     # lastframe = cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY)\n",
    "#     # _, lastframe = cv2.threshold(cv2.cvtColor(cv2.imread(lastframefile), cv2.COLOR_BGR2GRAY), threshold, 255, cv2.THRESH_BINARY)\n",
    "#     # framediff = frame-lastframe\n",
    "    \n",
    "#     # cv2.imshow('diff frame', framediff)\n",
    "#     frame = cv2.putText(frame, \"Threshold: %s\" % threshold, (5,15), cv2.FONT_HERSHEY_PLAIN, 0.75, (100,100,100), 1)\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     cv2.imwrite('Videos/aug72023vids/threshold%03d.jpg' % threshold, frame)\n",
    "#     # print(f' {framefileind}/{len(blinkinglights)}')\n",
    "#     # frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     # threshold = 70\n",
    "\n",
    "#     # # Apply threshold to the masked grayscale image\n",
    "#     # _, thresh = cv2.threshold(frame_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#     # cv2.imshow('frame', thresh)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# export_video(\n",
    "#     input_val='Videos/aug72023vids/threshold*.jpg',\n",
    "#     fps=20,\n",
    "#     output_val='Videos/aug72023vids/different_thresholds.mp4'\n",
    "# )\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Code To detect flashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n",
      "Results saved to \u001b[1mruns/detect/predict13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "Average Latency:  0.392 seconds\n"
     ]
    }
   ],
   "source": [
    "def crop_image(ogimage, cropboxes):\n",
    "    if len(cropboxes) == 0: np.array([])\n",
    "    cropbox = cropboxes[0] \n",
    "    tly, tlx, bry, brx = map(int, list(cropbox))\n",
    "    newimg = ogimage[tlx:brx, tly:bry]\n",
    "    return newimg    \n",
    "    \n",
    "def threshold_image(img, threshold=200):\n",
    "    _, newimg = cv2.threshold(\n",
    "        cv2.cvtColor(\n",
    "            img, \n",
    "            cv2.COLOR_BGR2GRAY\n",
    "        ), \n",
    "        threshold, 255, cv2.THRESH_BINARY\n",
    "    )\n",
    "    \n",
    "    return newimg\n",
    "    \n",
    "    \n",
    "\n",
    "def cv2_livefeed_rfdevice_wheadlights():\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    \n",
    "    foldername = 'runs/detect/predict12/'\n",
    "    filename = foldername + 'image0.jpg'\n",
    "    latencies = []\n",
    "    count = 0\n",
    "    headlight_pattern = []\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        \n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "        startbig = time.time()\n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        results = model(\n",
    "            frame, save=True, verbose=False\n",
    "        )\n",
    "\n",
    "\n",
    "        # crop the image to just the vehicle\n",
    "        cropped_image = crop_image(\n",
    "            ogimage = frame, cropboxes=results[0].boxes.xyxy.numpy()\n",
    "        )\n",
    "        \n",
    "        # greyscale and threshold the image\n",
    "        threshimg = threshold_image(\n",
    "            img=cropped_image, threshold=HL_THRESHOLD\n",
    "        )\n",
    "        headlight_pattern.append(int(threshimg.mean() > ONOFF_THRES))\n",
    "        \n",
    "        # cv2.imshow('boxed frame', cv2.imread(filename))\n",
    "        # cv2.imshow('cropped frame', cropped_image)\n",
    "        cv2.imshow('thresholded frame', threshimg)\n",
    "        \n",
    "        \n",
    "        endbig = time.time()\n",
    "        \n",
    "        # print(f\"{round(endbig-startbig, 2)} seconds overall\")\n",
    "        latencies.append(endbig-startbig)\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n",
    "        cv2.imwrite(foldername + 'video_frames/frame_%s.jpg' % current_time, frame)\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(headlight_pattern)\n",
    "    \n",
    "    print(\"Average Latency: \", round(avg(latencies), 3), \"seconds\")\n",
    "    \n",
    "cv2_livefeed_rfdevice_wheadlights()\n",
    "\n",
    "# Average latency: 0.392s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
