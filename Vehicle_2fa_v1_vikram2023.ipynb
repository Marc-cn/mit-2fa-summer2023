{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723177e7",
   "metadata": {},
   "source": [
    "# Vehicle Detection & Confirmation System\n",
    "## Version 1\n",
    "Created by Vikram Anantha \\\n",
    "Continued from Ben Dwyer's code \\\n",
    "Summer 2023 \n",
    "\n",
    "## Summary\n",
    "\n",
    "This code is meant to be implemented into Road-Side Systems (RSSs), like traffic cameras, such that it can communicate with vehicles, especially Autonomous Vehicles (AVs).\n",
    "\n",
    "### Background\n",
    "One problem that might arise when AVs communicate with RSSs is that a hacker with malicious intent can join the same channel and communicate with the RSSs as if they were the vehicle. To combat this, the RSS can command the vehicle to confirm its identity by performing a specific task. Examples of this include:\n",
    " + Displaying a specific pattern on a screen, like a QR code\n",
    " + Flashing headlights in a specific pattern\n",
    " + Making a sound in a specific pattern\n",
    "\n",
    "Once the vehicle performs the task, the RSS can confirm this has been done visually, thus confirming the identiy of the vehicle. This visual confirmation, a form of Two Factor Authentication (2FA) is the premise of this code.\n",
    "\n",
    "\n",
    "### Overview of this code\n",
    "\n",
    "Ben's initial code did the following: \\\n",
    "Given a [video](https://photos.app.goo.gl/KZE2xpdJrcP1h6hf6) of a [Remote Controlled Car with Arduino Display (RCC+AD)](https://photos.app.goo.gl/JfqdrNorKNP7Z9vz5) coming towards a camera\n",
    "(the camera acted as a traffic camera), it should:\n",
    "1. Slice the video into its frames\n",
    "2. For each frame, detect the vehicle and the pattern shown, draw boxes and label if its confirmed (this takes a long time)\n",
    "3. Put all the frames together into a video\n",
    "4. save the video\n",
    "\n",
    "My code took his initial code, however instead of using a sample video, it uses live webcam feed. \\\n",
    "Everything else was the same.\n",
    "\n",
    "### Shortcomings with this code\n",
    "\n",
    "Although this Version does work, it doesn't work that well.\n",
    " + When using live feed, there is about a second of delay per frame to detect if a vehicle is present, and to recognize there is no bounding box. It is much more time to recognize the array in the bounding box, which needs to be done for each vehicle.\n",
    " + The code runs a for loop to go through each vehicle and detect the pattern, meaning it verifies each vehicle one by one. Because each vehicle takes a long time to be verified, when multiple vehicles are present, the system will take a _very_ long time.\n",
    " + When using a webcam, or a dedicated camera for the computer (as in not a smartphone camera, which uses HDR to make screens appear normal with everything else), the Arduino display is super bright, and cannot be recognized.\n",
    " + The last point also doesn't really matter as much, since in the real environment, having a screen on the windshield won't be implemented. Instead, it would most likely leverage the headlights, having them flash a pattern across time, rather than display a pattern across space.\n",
    "\n",
    "Version 2 should address all of these shortcomings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ea8ac",
   "metadata": {},
   "source": [
    "## FFMPEG & Roboflow Install\n",
    "This block only needs to be run once on a machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92579fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/AWConant/jikanpy.git\n",
      "  Cloning git://github.com/AWConant/jikanpy.git to /tmp/pip-req-build-vxf13lf9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q git://github.com/AWConant/jikanpy.git /tmp/pip-req-build-vxf13lf9\n"
     ]
    }
   ],
   "source": [
    "# Basically don't touch this part- it is kind of delicate and should basically work\n",
    "# If the os.system lines don't work, then run those lines yourself\n",
    "# This cell only needs to be run once on your machine. After that, everything has already been installed\n",
    "\n",
    "# This cell also isn't entirely important if you only want to run the live feed functions\n",
    "from IPython.display import clear_output\n",
    "import os, urllib.request\n",
    "import getpass\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
    "# if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
    "#     hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
    "#     urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
    "\n",
    "# # from tmg import (\n",
    "# #     loadingAn,\n",
    "# #     textAn,\n",
    "# # )\n",
    "\n",
    "# # loadingAn(name=\"lds\")\n",
    "# # textAn(\"Installing Dependencies...\", ty='twg')\n",
    "os.system('pip install git+git://github.com/AWConant/jikanpy.git')\n",
    "os.system('sudo add-apt-repository -y ppa:jonathonf/ffmpeg-4')\n",
    "os.system('sudo apt-get update')\n",
    "os.system('sudo apt install mediainfo')\n",
    "os.system('sudo apt-get install ffmpeg')\n",
    "# password = getpass.getpass()\n",
    "# pw_commands = [\n",
    "#    \"sudo -S add-apt-repository -y ppa:jonathonf/ffmpeg-4\",\n",
    "#     \"sudo -S apt-get update\",\n",
    "#     \"sudo -S apt install mediainfo\",\n",
    "#     \"sudo -S apt-get install ffmpeg\"\n",
    "# ]\n",
    "# for command in pw_commands:\n",
    "#     os.popen(command, 'w').write(password+'\\n')\n",
    "clear_output()\n",
    "print('Installation finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Roboflow (this is actually important)\n",
    "!pip3 install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412975e",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d960e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports have been imported\n"
     ]
    }
   ],
   "source": [
    "# Imports everything that should be needed\n",
    "\n",
    "from roboflow import Roboflow\n",
    "import json\n",
    "from time import sleep\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "import base64\n",
    "import requests\n",
    "from os.path import exists\n",
    "import os, sys, re, glob\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ecapture import ecapture as ec\n",
    "\n",
    "font_dir = \"Extras/Product Sans Bold.ttf\" # this is for the images created and displayed\n",
    "\n",
    "print(\"Imports have been imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a8d98",
   "metadata": {},
   "source": [
    "### Roboflow\n",
    "[Roboflow](https://roboflow.com/) is a platform which houses a vast variety of models, mainly for computer vision. It is often used because it is easy to train the models, and prediction happens over the cloud. For computer vision, Roboflow uses [YOLOv8](https://blog.roboflow.com/whats-new-in-yolov8/) (You Only Look Once), the fastest and most accurate architecture.\n",
    "\n",
    "Ben has 3 models used in this code: one for detecting the vehicle, one for detecting the bounding box, and one for detecting whether each box is on or off in the bounding box. To access Roboflow, you need to provide the API key and the project name for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba69293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Models retreived\n"
     ]
    }
   ],
   "source": [
    "# Retreive the roboflow models \n",
    "\n",
    "rf = Roboflow(api_key=\"vAsGYa1MuHAuaPrRdiar\")\n",
    "\n",
    "project_car = rf.workspace().project(\"rc-vehicle-detection_v2\")\n",
    "model_car = project_car.version(1).model\n",
    "\n",
    "### Bounding box for the LED array\n",
    "project_led = rf.workspace().project(\"led-signal-detection_v2\")\n",
    "model_led = project_led.version(2).model\n",
    "\n",
    "### Determines if an LED is on or off\n",
    "on_off_project = rf.workspace().project(\"led-signal-detection_v3\")\n",
    "on_off_model = on_off_project.version(4).model\n",
    "\n",
    "print(\"Models retreived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20d142",
   "metadata": {},
   "source": [
    "# Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63339cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video files pathfiles variables\n",
    "\n",
    "# This block isn't actually needed for the live feed, but is kept for legacy\n",
    "drive = '05'\n",
    "video_dir = 'Videos/drive_%s/' % drive\n",
    "input_video = video_dir + 'drive_%s_vikram2023.mp4' % drive\n",
    "input_video_frames_dir = video_dir + \"frames/\"\n",
    "output_video_frames_dir = video_dir + \"frames_wboxes/\"\n",
    "output_video = video_dir + 'drive_%s_wboxes_vikram2023.mp4' % drive\n",
    "\n",
    "# Here is a bunch of formats that are defined in one place, for convenience\n",
    "extention = \".png\"\n",
    "frames_format = \"frame_%04d\" + extention\n",
    "twofa_crop_dir = video_dir + \"on_off_crop/\"\n",
    "twofa_frames_dir = input_video_frames_dir\n",
    "aoi_crop_dir = video_dir + 'aoi_crop/'\n",
    "aoi_crop_format = 'aoi_crop%s.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bab8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of helper functions, related to labeling the image\n",
    "\n",
    "## Draws boxes on the image\n",
    "def draw_boxes(box, x0, y0, img, class_name, color=None, weight=5):\n",
    "    # OPTIONAL - color map, change the key-values for each color to make the\n",
    "    # class output labels specific to your dataset\n",
    "    color_map = {\n",
    "        \"RC-Car\":\"red\"\n",
    "    }\n",
    "\n",
    "    # get position coordinates\n",
    "    bbox = ImageDraw.Draw(img) \n",
    "    if (color == None): outline_color = color_map[class_name]\n",
    "    else: outline_color = color\n",
    "    bbox.rectangle(box, outline = outline_color, width=weight)\n",
    "    bbox.text((x0, y0), class_name, fill='black', anchor='mm')\n",
    "\n",
    "    return img\n",
    "\n",
    "## Puts a label on the image\n",
    "def label_img(box_pos = (0, 0), img=None, text_label=\"\", box_color=(255, 255, 255), weight=5, font_size=15):\n",
    "    \n",
    "    label = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define the position and size of the red box\n",
    "    box_size = (500, 100)\n",
    "\n",
    "    # Draw the red box\n",
    "    label.rectangle((box_pos, (box_pos[0]+box_size[0], box_pos[1]+box_size[1])), fill=box_color, width=weight)\n",
    "\n",
    "    # Define the position and font of the text\n",
    "    text_pos = (box_pos[0]+25, box_pos[1]+40)\n",
    "    text_font = ImageFont.truetype(font_dir, font_size)\n",
    "    # Draw the white text\n",
    "    text_color = (255, 255, 255)\n",
    "    label.text(text_pos, text_label, fill=text_color, font=text_font)\n",
    "\n",
    "    return img\n",
    "\n",
    "## Saves the labeled and boxed image\n",
    "def save_with_bbox_renders(img):\n",
    "    file_name = os.path.basename(img.filename)\n",
    "    img.save(output_video_frames_dir + file_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More helper functions, related to the object detection\n",
    "\n",
    "## Prepares the image before it is put through the model\n",
    "def prep_img(file_path):\n",
    "    image = cv2.imread(file_path)\n",
    "\n",
    "    # Get the image dimensions\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Define the maximum width and height of the resized image\n",
    "    max_size = 1000\n",
    "\n",
    "    # Calculate the aspect ratio of the image\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    # Calculate the new dimensions for the resized image\n",
    "    # all the training data was square images\n",
    "    if width > height:\n",
    "        new_width = max_size\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = max_size\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    # Resize the image using bilinear interpolation\n",
    "    img = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return img\n",
    "\n",
    "## Gets the predictions, and presents the vars to crop / label\n",
    "def make_predictions(model, img):\n",
    "    predictions = model.predict(img).json()['predictions']\n",
    "    img_copy = img.copy()\n",
    "\n",
    "    print(\"4> \", predictions)\n",
    "    prediction = predictions[0]\n",
    "    x = int(prediction['x'])\n",
    "    y = int(prediction['y'])\n",
    "    height = int(prediction['height'])\n",
    "    width = int(prediction['width'])\n",
    "    cls = prediction['class']\n",
    "    \n",
    "    cropped_img = img[y-height//2:y+height//2, x-width//2:x+width//2]\n",
    "    \n",
    "    return x, y, height, width, cropped_img\n",
    "\n",
    "def render_boxed_image(image, verbose=False):\n",
    "    # INFERENCE\n",
    "    predictions = model_car.predict(image).json()['predictions']\n",
    "    newly_rendered_image = Image.open(image)\n",
    "\n",
    "    # RENDER \n",
    "    # for each detection, create a crop and convert into CLIP encoding\n",
    "    if verbose: print(predictions)\n",
    "    for prediction in predictions:\n",
    "        # rip bounding box coordinates from current detection\n",
    "        # note: infer returns center points of box as (x,y) and width, height\n",
    "        # ----- but pillow crop requires the top left and bottom right points to crop\n",
    "        x0 = prediction['x'] - prediction['width'] / 2\n",
    "        x1 = prediction['x'] + prediction['width'] / 2\n",
    "        y0 = prediction['y'] - prediction['height'] / 2\n",
    "        y1 = prediction['y'] + prediction['height'] / 2\n",
    "        box = (x0, y0, x1, y1)\n",
    "\n",
    "        newly_rendered_image = draw_boxes(box, x0, y0, newly_rendered_image, prediction['class'])\n",
    "    return newly_rendered_image\n",
    "\n",
    "## Take the bounding box, and crop the image up into just the on/off squares\n",
    "def crop_thirds(image, row, col, sz):\n",
    "    # Get the dimensions of the input image\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Determine the size of the square to crop\n",
    "    size = min(height, width)\n",
    "    \n",
    "    # Crop the input image to a square\n",
    "    x = (width - size) // 2\n",
    "    y = (height - size) // 2\n",
    "    square = image[y:y+size, x:x+size]\n",
    "    \n",
    "    # Determine the dimensions of each third of the square\n",
    "    third_size = size // sz\n",
    "    x = third_size * (col - 1)\n",
    "    y = third_size * (row - 1)\n",
    "    \n",
    "    # Crop the specified section of the square\n",
    "    crop = square[y:y+third_size, x:x+third_size]\n",
    "    \n",
    "    cv2.imwrite(twofa_crop_dir + f'{row}{col}.jpg', crop)\n",
    "\n",
    "\n",
    "## Decode the pattern\n",
    "def find_code(directory, model):\n",
    "    ans = []\n",
    "\n",
    "    path = directory + '*.jpg'\n",
    "    # Use the glob function to get a list of file paths\n",
    "    file_paths = glob.glob(path)\n",
    "\n",
    "    # Use the sorted function to sort the file paths in ascending order\n",
    "    sorted_file_paths = sorted(file_paths)\n",
    "\n",
    "    # Iterate through the sorted file paths\n",
    "    for path in sorted_file_paths:\n",
    "        if not os.path.isdir(path):\n",
    "            on_off = model.predict(path).json()['predictions']\n",
    "\n",
    "            val = on_off[0]['predictions'][0]['class']\n",
    "            if val == \"On\":\n",
    "                a = 1\n",
    "            else:\n",
    "                a = 0\n",
    "            ans.append(a)\n",
    "    return ans\n",
    "\n",
    "def decode_signal(sz, directory, expected, cropped_img):\n",
    "    \n",
    "    # Decoding Signal\n",
    "    for col in range(1, sz+1):\n",
    "        for row in range(1, sz+1):\n",
    "            crop_thirds(cropped_img, row, col, sz)\n",
    "\n",
    "    ans = find_code(directory, on_off_model)\n",
    "    \n",
    "    if ans == expected:\n",
    "#         print(\"Vehicle verified. \")\n",
    "        return ans\n",
    "    else:\n",
    "#         print(\"Unable to verify vehicle.\")\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a53ece",
   "metadata": {},
   "source": [
    "# 2FA Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318da12b",
   "metadata": {},
   "source": [
    "### How does 2FA work\n",
    "\n",
    "For each frame:\n",
    " 1. Detect if there is a vehicle (the model technically detected RCC+ADs):\n",
    "    1. Simply do `roboflow_model.predict()` to detect if a vehicle is there and get the coords\n",
    "    2. Draw the box around the vehicle, label the image\n",
    "    3. Crop the frame such that it's only the vehicle ([like this](https://photos.app.goo.gl/R4HdeR6yvDfXgNsc9))\n",
    " 2. Detect if there is an Arduino Display which shows a bounding box\n",
    "    1. Use a roboflow model\n",
    "    2. If so, crop the image again so it only shows the bounding box\n",
    "    3. If not, just say so\n",
    " 3. Detect the pattern inside the bounding box\n",
    "    1. Slice the image (which at this point is just a 3x3 grid of on/off squares) into the 9 small squares\n",
    "    2. For each square, use roboflow to predict whether the box is off or on\n",
    " 4. If the pattern is correct, then the identity has been confirmed\n",
    " 5. Else, then it hasn't been confimed _yet_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code that does all the 2 factor Authentication\n",
    "def twofa(image, model_car, model_led, expected, sz, verified):   \n",
    "    print()\n",
    "    img_to_crop = cv2.imread(image)\n",
    "    newly_rendered_image = Image.open(image)\n",
    "\n",
    "    # Vehicle Predictions\n",
    "    # to be replaced with a faster obj detection model?\n",
    "    \n",
    "    rc_predictions = model_car.predict(image).json()['predictions']\n",
    "    \n",
    "    # print(rc_predictions)\n",
    "\n",
    "    # RENDER \n",
    "    for prediction in rc_predictions:\n",
    "        x0 = prediction['x'] - prediction['width'] / 2\n",
    "        x1 = prediction['x'] + prediction['width'] / 2\n",
    "        y0 = prediction['y'] - prediction['height'] / 2\n",
    "        y1 = prediction['y'] + prediction['height'] / 2\n",
    "        box = (x0, y0, x1, y1)\n",
    "\n",
    "        start = time.time()\n",
    "        if verified == False:\n",
    "            newly_rendered_image = draw_boxes(box, x0, y0, newly_rendered_image, prediction['class'], (255, 0, 0), weight=7)\n",
    "        else: \n",
    "            newly_rendered_image = draw_boxes(box, x0, y0, newly_rendered_image, prediction['class'], (0, 255, 0), weight=7)\n",
    "        print(f\"{round(time.time()-start, 2)} seconds to draw boxes\")\n",
    "\n",
    "        if verified == False:\n",
    "            start = time.time()\n",
    "            AOI_img = img_to_crop.copy()\n",
    "            height, width = AOI_img.shape[:2]\n",
    "            x0 = int(max(0, x0))\n",
    "            x1 = int(min(width, x1))\n",
    "            y0 = int(max(0, y0))\n",
    "            y1 = int(min(height, y1))\n",
    "            AOI_img = AOI_img[y0:y1, x0:x1]\n",
    "            # file_path = aoi_crop_dir + (aoi_crop_format % i)\n",
    "            file_path = 'Videos/aoi_frame_crop.jpg'\n",
    "            cv2.imwrite(file_path, AOI_img)\n",
    "            print(f\"{round(time.time()-start, 2)} seconds to save AOI_img\")\n",
    "            try:     \n",
    "                # LED Array Predictions\n",
    "                start = time.time()\n",
    "                img = prep_img(file_path)\n",
    "                print(f\"{round(time.time()-start, 2)} seconds to prep img\")\n",
    "                start = time.time()\n",
    "                x, y, height, width, cropped_img = make_predictions(model_led, img)\n",
    "                print(f\"{round(time.time()-start, 2)} seconds to prep img\")\n",
    "                # print(x, y)\n",
    "                print(\"Attempting to verify vehicle: Array Found\")\n",
    "\n",
    "                directory = twofa_crop_dir\n",
    "\n",
    "                ans = decode_signal(sz, directory, expected, cropped_img)\n",
    "\n",
    "                print(ans)\n",
    "\n",
    "                if ans == expected:\n",
    "                    verified = True\n",
    "                    newly_rendered_image = label_img(box_pos = (x0,y0), img=newly_rendered_image, text_label=\"Vehicle ID Confirmed\", box_color=(0, 255, 0), font_size=30)\n",
    "                else:\n",
    "                    newly_rendered_image = label_img(box_pos = (x0,y0), img=newly_rendered_image, text_label=\"Vehicle Unidentified\", box_color=(255, 0, 0), font_size=30)\n",
    "\n",
    "            except IndexError:\n",
    "                print(\"Attempting to verify vehicle: No array found.\")\n",
    "        else:\n",
    "            print(\"Vehicle has been verified.\")\n",
    "            \n",
    "    return newly_rendered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does 2fa on a video file (or the frames of the video file)\n",
    "# This function isn't actually used, but is there for legacy\n",
    "def run_on_globbed(globbed_files, model_car, model_led, expected, sz):\n",
    "\n",
    "    for i in range(len(globbed_files)):\n",
    "        print(i+1,\"/\", len(globbed_files))\n",
    "        newly_rendered_image = twofa(globbed_files[i], model_car, model_led, expected, sz, False)\n",
    "\n",
    "        save_with_bbox_renders(newly_rendered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code that is run for live feed\n",
    "\n",
    "def use_livefeed_twofa():\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(-1) # use the camera\n",
    "    vid.set(cv2.CAP_PROP_FRAME_WIDTH, 3840) \n",
    "    vid.set(cv2.CAP_PROP_FRAME_HEIGHT, 2160)\n",
    "    # 4k dimensions\n",
    "    filename = 'Videos/frame_cv.png'\n",
    "    \n",
    "    verified = False\n",
    "    expected = [\n",
    "        1, 0, 0,\n",
    "        0, 0, 0,\n",
    "        0, 1, 1\n",
    "    ] # sample patter\n",
    "    sz = 3 # how many rows&cols\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        startbig = time.time()\n",
    "        for _ in range(4): \n",
    "            vid.grab()\n",
    "        # for some reason cv2 takes the next 5 frames and buffers them\n",
    "        # this means that whats displayed is 5 frames behind\n",
    "        # but when each frame takes a second to process, that's a 5 second delay\n",
    "        # this makes sure its only looking at the current frame\n",
    "        \n",
    "        \n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        \n",
    "        # do vehicle detection\n",
    "        cv2.imwrite(filename, frame) # roboflow model needs a file thats aleady saved on the computer\n",
    "        opencv_boxed_image = twofa(filename, model_car, model_led, expected, sz, verified)\n",
    "        opencv_boxed_image = np.array(opencv_boxed_image)[:, :, ::-1].copy() # turns the PIL image to a CV2-ready image\n",
    "        \n",
    "        # display frame\n",
    "        cv2.imshow('frame', opencv_boxed_image)\n",
    "        \n",
    "        print(f\"{round(time.time()-startbig, 2)} seconds overall\")\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a62f7b",
   "metadata": {},
   "source": [
    "## Actual Main Code that should be run for V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CODE\n",
    "\n",
    "use_livefeed_twofa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For if you want to test 2FA on an image\n",
    "\n",
    "globbed_files = ['Videos/the_car_3.jpg']\n",
    "expected = [\n",
    "    1, 0, 0,\n",
    "    0, 0, 0,\n",
    "    0, 1, 1\n",
    "]\n",
    "run_on_globbed(globbed_files, model_car, model_led, expected, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2281d2",
   "metadata": {},
   "source": [
    "## Legacy functions (that I don't want to delete)\n",
    "I'm a very sentimental person \\\n",
    "These functions might also be useful in the future idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video helper functions\n",
    "# These functions aren't actually used in the live feed, but are there for legacy\n",
    "\n",
    "def import_video(input_vid_val=input_video, input_frames_val=input_video_frames_dir+frames_format, fps=25):\n",
    "    # break video down into images - UPDATE THE PATH TO THE FILE!\n",
    "    os.environ['inputFile'] = input_vid_val\n",
    "\n",
    "    # fps value: the number of frames to sample per second from the video\n",
    "    # !ffmpeg  -hide_banner -loglevel error -i \"$inputFile\" -vf fps=25 \"$inputFile_out%04d.png\" \n",
    "    os.system(f'ffmpeg -hide_banner -loglevel error -i \"$inputFile\" -vf fps={fps} {input_frames_val}')\n",
    "    print(\"Frames have been gotten!\")\n",
    "\n",
    "def export_video_with_boxes(input_val = input_video_frames_dir + frames_format, output_val = output_video, fps=25):\n",
    "    # stich images together into video\n",
    "    os.system(f\"ffmpeg -r {fps} -s 1920x1080 -i {input_val} -vcodec libx264 -crf {fps}  -pix_fmt yuv420p {output_val}\")\n",
    "    # !ffmpeg -r 25 -s 1920x1080 -i '/Users/bendwyer/Documents/Academic/2022-2023/Thesis/Vehicle Detection/%04d.png' -vcodec libx264 -crf 25  -pix_fmt yuv420p '/Users/bendwyer/Documents/Academic/2022-2023/Thesis/Vehicle Detection/test.mp4'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3eae4d",
   "metadata": {},
   "source": [
    "# Detect vehicle with sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecd784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2840ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference on each image from the split up video\n",
    "\n",
    "# glob files based on location and file format\n",
    "globbed_files = sorted(glob.glob(input_video_frames_dir + '*' + extention))\n",
    "# print(globbed_files)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for image in globbed_files:\n",
    "  newly_rendered_image = render_boxed_image(image)\n",
    "  # WRITE\n",
    "  save_with_bbox_renders(newly_rendered_image, verbose=True)\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "print(f\"It took {(end-start)/60} minutes to detect {len(globbed_files)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884facd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_video_with_boxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb34bf1",
   "metadata": {},
   "source": [
    "# Live view object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de9a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code only detects the vehicle, doesn't do any 2 factor authentication\n",
    "\n",
    "# This code isn't actually used, but is there for legacy\n",
    "\n",
    "def use_livefeed_objdet():\n",
    "  \n",
    "    # define a video capture object\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    filename = 'Videos/frame_cv.png'\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        # Capture the video frame by frame\n",
    "        ret, frame = vid.read()\n",
    "        \n",
    "        # Zed cameras have two cameras so only use one of them\n",
    "        frame = np.hsplit(frame, 2)[0]\n",
    "        \n",
    "        # do vehicle detection\n",
    "        cv2.imwrite(filename, frame)\n",
    "        opencv_boxed_image = np.array(render_boxed_image(filename))\n",
    "        opencv_boxed_image = opencv_boxed_image[:, :, ::-1].copy() \n",
    "        \n",
    "        # display frame\n",
    "        cv2.imshow('frame', opencv_boxed_image)\n",
    "\n",
    "        # the 'q' button is set as the\n",
    "        # quitting button you may use any\n",
    "        # desired button of your choice\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # After the loop release the cap object\n",
    "    vid.release()\n",
    "    # Destroy all the windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12799e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_livefeed_objdet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c22e0c",
   "metadata": {},
   "source": [
    "## Ignore beyond this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IGNORE THIS ###\n",
    "\n",
    "ROBOFLOW_API_KEY = \"vAsGYa1MuHAuaPrRdiar\"\n",
    "ROBOFLOW_MODEL = \"rc-vehicle-detection_v2\" # eg xx-xxxx--#\n",
    "ROBOFLOW_SIZE = 416\n",
    "\n",
    "import cv2\n",
    "import base64\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "upload_url = \"\".join([\n",
    "    \"https://detect.roboflow.com/\",\n",
    "    ROBOFLOW_MODEL,\n",
    "    \"?access_token=\",\n",
    "    ROBOFLOW_API_KEY,\n",
    "    \"&format=image\",\n",
    "    \"&stroke=5\"\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IGNORE THIS ###\n",
    "\n",
    "def infer(video):\n",
    "    # Get the current image from the webcam\n",
    "    ret, img = video.read()\n",
    "\n",
    "    # Resize (while maintaining the aspect ratio) to improve speed and save bandwidth\n",
    "    height, width, channels = img.shape\n",
    "    scale = ROBOFLOW_SIZE / max(height, width)\n",
    "    img = cv2.resize(img, (round(scale * width), round(scale * height)))\n",
    "\n",
    "    # Encode image to base64 string\n",
    "    retval, buffer = cv2.imencode('.jpg', img)\n",
    "    img_str = base64.b64encode(buffer)\n",
    "\n",
    "    # Get prediction from Roboflow Infer API\n",
    "    resp = requests.post(upload_url, data=img_str, headers={\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }, stream=True).raw\n",
    "\n",
    "    # Parse result image\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    print(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IGNORE THIS ###\n",
    "\n",
    "video = cv2.VideoCapture(-1)\n",
    "\n",
    "while 1:\n",
    "    # On \"q\" keypress, exit\n",
    "    if(cv2.waitKey(1) == ord('q')):\n",
    "        break\n",
    "\n",
    "    # Synchronously get a prediction from the Roboflow Infer API\n",
    "    image = infer(video)\n",
    "    # And display the inference results\n",
    "    try:\n",
    "        cv2.imshow('image', image)\n",
    "    except Exception as e:\n",
    "        print(\"oops\")\n",
    "    \n",
    "# Release resources when finished\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
